---

## PHASE 1C: Gradient Descent - Backtracking Line Search

**Date**: 2025-09-06  
**Objective**: Test backtracking line search vs constant lr=0.1 baseline

### Experiments Conducted:
1. `03a_setup_gd_backtracking_ols_lr_10_c1_0001_rho_08.py` - c1=0.01, œÅ=0.8
2. `03b_setup_gd_backtracking_ols_lr_10_c1_001_rho_05.py` - c1=0.01, œÅ=0.5
3. `03c_setup_gd_backtracking_ols_lr_10_c1_01_rho_05.py` - c1=0.1, œÅ=0.5

### Results Summary:

| Setup | c1 | œÅ | Convergence | Time (s) | Iterations | Final Loss | Gradient Norm | Status |
|-------|----|----|-------------|----------|------------|------------|---------------|---------|
| **03 (baseline)** | - | - | ‚úÖ Yes | 112.7 | 20,100 | 0.011925 | 9.93e-06 | Reference |
| **03a** | 0.01 | 0.8 | ‚ùå No | 389.6 | 10,000 | 0.011925 | 3.85e-05 | Failed |
| **03b** | 0.01 | 0.5 | ‚úÖ Yes | 526.7 | **6,000** | 0.011925 | 9.39e-06 | üèÜ **Winner** |
| **03c** | 0.1 | 0.5 | ‚úÖ Yes | 718.5 | 7,400 | 0.011925 | 9.49e-06 | Success |

### Key Findings:
- **œÅ=0.5 is critical**: All œÅ=0.5 setups converged, œÅ=0.8 failed
- **Optimal parameters**: c1=0.01, œÅ=0.5 (setup 03b)
- **Dramatic speedup**: 3.4x faster than constant lr (6,000 vs 20,100 iterations)
- **Aggressive backtracking wins**: œÅ=0.5 > œÅ=0.8 for convergence

### Decision:
**Backtracking line search (c1=0.01, œÅ=0.5)** significantly outperforms constant learning rate.

---

## NEXT PHASE: 1D - Learning Rate Decay Schedules

**New baseline to beat**: 03b with 6,000 iterations
**Planned experiments**:
- Linear decay, sqrt decay, exponential decay
- Compare against backtracking winner performance