# Gradient Descent vÃ  Stochastic Gradient Descent

---

**CÃ´ng thá»©c tá»•ng quÃ¡t:** xâ‚–â‚Šâ‚ = xâ‚– - Î±âˆ‡f(xâ‚–)

- xâ‚–: Vector tham sá»‘ táº¡i vÃ²ng láº·p k
- Î±: Äá»™ dÃ i bÆ°á»›c hoáº·c Tá»‘c Ä‘á»™ há»c (learning rate)
- âˆ‡f(xâ‚–): Gradient cá»§a hÃ m má»¥c tiÃªu táº¡i xâ‚–

**LÃ½ Thuyáº¿t Há»™i Tá»¥:**

- Há»™i tá»¥ tuyáº¿n tÃ­nh: O(Ïáµ) vá»›i Ï < 1
- YÃªu cáº§u tÃ­nh liÃªn tá»¥c Lipschitz vÃ  tÃ­nh lá»“i máº¡nh
- Tá»‘c Ä‘á»™ phá»¥ thuá»™c vÃ o sá»‘ Ä‘iá»u kiá»‡n Îº = L/Î¼ (L: háº±ng sá»‘ Lipschitz, Î¼: tham sá»‘ lá»“i máº¡nh)

---

## I. THUáº¬T TOÃN GRADIENT DESCENT

### A. Gradient Descent CÆ¡ Báº£n

#### 1. NghiÃªn cá»©u Ä‘á»™ nháº¡y tham sá»‘ Learning Rate

**PhÆ°Æ¡ng phÃ¡p luáº­n:** Thá»­ nghiá»‡m vá»›i cÃ¡c má»©c learning rate cá»‘ Ä‘á»‹nh Ä‘á»ƒ xÃ¡c Ä‘á»‹nh khoáº£ng tá»‘i Æ°u.

**Setup 01: Learning Rate Î± = 0.0001**

- Cáº¥u hÃ¬nh: `01_setup_gd_ols_lr_0001`
- **Káº¿t quáº£ thá»±c táº¿:** 1000 vÃ²ng láº·p, KHÃ”NG Há»˜I Tá»¤
- **Äáº·c Ä‘iá»ƒm:** Tiáº¿n Ä‘á»™ cháº­m, dung lÆ°á»£ng tÃ­nh toÃ¡n lÃ£ng phÃ­
- **Final loss:** 0.01266, gradient norm: 0.0100 (cao)
- **PhÃ¢n tÃ­ch:** Learning rate quÃ¡ tháº¥p dáº«n Ä‘áº¿n khÃ´ng thá»ƒ há»™i tá»¥ trong 1000 vÃ²ng láº·p hoÃ n toÃ n

**Setup 02: Learning Rate Î± = 0.001**

- Cáº¥u hÃ¬nh: `02_setup_gd_ols_lr_001`
- **Káº¿t quáº£ thá»±c táº¿:** 1000 vÃ²ng láº·p, KHÃ”NG Há»˜I Tá»¤
- **Äáº·c Ä‘iá»ƒm:** Tiáº¿n Ä‘á»™ cÃ³ cáº£i thiá»‡n nhÆ°ng váº«n khÃ´ng há»™i tá»¥
- **Final loss:** 0.0119, gradient norm: 0.0006 (váº«n cao)
- **PhÃ¢n tÃ­ch:** DÃ¹ cáº£i thiá»‡n so vá»›i setup 01 nhÆ°ng váº«n chÆ°a Ä‘á»§

**Setup 03: Learning Rate Î± = 0.01**

- Cáº¥u hÃ¬nh: `03_setup_gd_ols_lr_01`
- **Káº¿t quáº£ Ä‘Ã¡ng ngáº¡c nhiÃªn:** 270 vÃ²ng láº·p, Há»˜I Tá»¤ THÃ€NH CÃ”NG
- **Hiá»‡n tÆ°á»£ng báº¥t ngá»:** Learning rate cao nhÆ°ng á»•n Ä‘á»‹nh
- **Xáº¿p háº¡ng:** Tá»‘t nháº¥t trong cÃ¡c GD setup thÃ nh cÃ´ng
- **PhÃ¢n tÃ­ch:** ThÃ¡ch thá»©c giáº£ thuyáº¿t Î± < 2/Î»â‚˜â‚â‚“, thá»±c táº¿ phá»©c táº¡p hÆ¡n

**Setup 03: Learning Rate Î± = 0.5**

- Cáº¥u hÃ¬nh: `03_setup_gd_ols_lr_05`
- **Káº¿t quáº£ Ä‘Ã¡ng ngáº¡c nhiÃªn:** 270 vÃ²ng láº·p, Há»˜I Tá»¤ THÃ€NH CÃ”NG
- **Hiá»‡n tÆ°á»£ng báº¥t ngá»:** Learning rate cao nhÆ°ng á»•n Ä‘á»‹nh
- **Xáº¿p háº¡ng:** Tá»‘t nháº¥t trong cÃ¡c GD setup thÃ nh cÃ´ng
- **PhÃ¢n tÃ­ch:** ThÃ¡ch thá»©c giáº£ thuyáº¿t Î± < 2/Î»â‚˜â‚â‚“, thá»±c táº¿ phá»©c táº¡p hÆ¡n

**Káº¿t luáº­n SAI Láº¦M tá»« thÃ­ nghiá»‡m thá»±c táº¿:**

- **Tháº£m ká»‹ch:** 74% setup tháº¥t báº¡i - trÃ¡i ngÆ°á»£c vá»›i lÃ½ thuyáº¿t
- **Báº¥t ngá»:** Learning rate cao (0.5) láº¡i thÃ nh cÃ´ng hÆ¡n learning rate tháº¥p
- **Thá»±c táº¿:** KhÃ´ng cÃ³ "khoáº£ng tá»‘i Æ°u" Ä‘Æ¡n giáº£n, Ä‘á»‹a hÃ¬nh phá»©c táº¡p

#### 2. Gradient Descent CÃ³ Regularization (Ridge Regression)

**Ná»n Táº£ng ToÃ¡n Há»c:**

- Má»¥c tiÃªu: f(x) = ||Xx - y||Â² + Î»||x||Â²
- Gradient: âˆ‡f(x) = 2X^T(Xx - y) + 2Î»x
- Hessian: H = 2X^TX + 2Î»I (cáº£i thiá»‡n conditioning)

**Setup 04: Ridge Regularization vá»›i Learning Rate Tháº¥p - THáº¤T Báº I**

- Cáº¥u hÃ¬nh: `04_setup_gd_ridge_lr_0001_reg_001`
- **Káº¿t quáº£ thá»±c táº¿:** 500 vÃ²ng láº·p, KHÃ”NG Há»˜I Tá»¤
- **Learning rate:** 0.0001 quÃ¡ tháº¥p dÃ¹ cÃ³ regularization
- **Final loss:** 0.0163, gradient norm: 0.0698 (ráº¥t cao)
- **PhÃ¢n tÃ­ch:** Regularization khÃ´ng thá»ƒ bÃ¹ Ä‘áº¯p learning rate quÃ¡ tháº¥p

**Setup 05: Ridge vá»›i Learning Rate Trung BÃ¬nh - THáº¤T Báº I**

- Cáº¥u hÃ¬nh: `05_setup_gd_ridge_lr_001_reg_001`
- **Káº¿t quáº£ thá»±c táº¿:** 500 vÃ²ng láº·p, KHÃ”NG Há»˜I Tá»¤
- **Final loss:** 0.0128, gradient norm: 0.001 (váº«n chÆ°a Ä‘áº¡t tolerance)
- **Tráº¡ng thÃ¡i:** Gáº§n há»™i tá»¥ nhÆ°ng chÆ°a thÃ nh cÃ´ng trong giá»›i háº¡n 500 iterations
- **Nháº­n xÃ©t:** Ridge giÃºp á»•n Ä‘á»‹nh nhÆ°ng váº«n cháº­m

**Setup 06: Ridge vá»›i Learning Rate Cao - THÃ€NH CÃ”NG**

- Cáº¥u hÃ¬nh: `06_setup_gd_ridge_lr_05_reg_001`
- **Káº¿t quáº£ tÆ°Æ¡ng Ä‘Æ°Æ¡ng Setup 03:** 270 vÃ²ng láº·p, Há»˜I Tá»¤
- **Xáº¿p háº¡ng:** CÃ¹ng vá»›i GD OLS lr=0.5 lÃ  top performers
- **Insight quan trá»ng:** Ridge vá»›i lr cao Äƒn nháº­n vá»›i GD thuáº§n vá»›i lr cao

**PhÃ¢n TÃ­ch TÃ¡c Äá»™ng Ridge Regularization:**

- **Sá»‘ há»c:** Condition number giáº£m tá»« 954M xuá»‘ng 955 (Setup 23 Newton)
- **Thá»±c nghiá»‡m:** Táº¥t cáº£ Ridge setups Ä‘á»u á»•n Ä‘á»‹nh hÆ¡n OLS tÆ°Æ¡ng á»©ng
- **CÆ¡ cháº¿:** H_regularized = H + Î»I â†’ eigenvalues shifted upward
- **Dual benefit:** Cáº£i thiá»‡n optimization stability + generalization
- **Practical insight:** LuÃ´n dÃ¹ng regularization trá»« khi cÃ³ lÃ½ do Ä‘áº·c biá»‡t

### B. PhÆ°Æ¡ng PhÃ¡p Tá»‘c Äá»™ Há»c ThÃ­ch á»¨ng

#### 3. Äiá»u Khiá»ƒn KÃ­ch ThÆ°á»›c BÆ°á»›c ThÃ­ch á»¨ng

**Setup 07: Tá»‘c Äá»™ Há»c ThÃ­ch á»¨ng**

- Cáº¥u hÃ¬nh: `setup_gd_adaptive_ols_lr_001.py`
- Î± ban Ä‘áº§u: 0.001, Î± cuá»‘i: 0.0157
- Há»™i tá»¥: 345 vÃ²ng láº·p
- CÆ¡ cháº¿: TÄƒng Î± khi loss giáº£m liÃªn tá»¥c, giáº£m khi loss tÄƒng

**Thuáº­t ToÃ¡n ThÃ­ch á»¨ng:**

```
náº¿u loss_k < loss_{k-1}:
    Î± = Î± Ã— 1.05  (tÄƒng kÃ­ch thÆ°á»›c bÆ°á»›c)
ngÆ°á»£c láº¡i:
    Î± = Î± Ã— 0.5   (giáº£m kÃ­ch thÆ°á»›c bÆ°á»›c)
```

#### 4. PhÆ°Æ¡ng PhÃ¡p Line Search

**Setup 08: Backtracking Line Search (Armijo)**

- Cáº¥u hÃ¬nh: `setup_gd_backtracking_ols_c1_0001.py`
- Äiá»u kiá»‡n Armijo: f(xâ‚– + Î±pâ‚–) â‰¤ f(xâ‚–) + câ‚Î±âˆ‡f(xâ‚–)^Tpâ‚–
- Tham sá»‘ câ‚ = 1e-4 (tham sá»‘ giáº£m Ä‘á»§)
- Há»™i tá»¥: 89 vÃ²ng láº·p
- KÃ­ch thÆ°á»›c bÆ°á»›c biáº¿n thiÃªn: Ä‘áº£m báº£o giáº£m Ä‘á»§ má»—i vÃ²ng láº·p

**Setup 09: Äiá»u Kiá»‡n Wolfe**

- Cáº¥u hÃ¬nh: `setup_gd_wolfe_conditions_ols_c1_0001_c2_09.py`
- Äiá»u kiá»‡n Armijo + Äiá»u kiá»‡n Curvature: âˆ‡f(xâ‚– + Î±pâ‚–)^Tpâ‚– â‰¥ câ‚‚âˆ‡f(xâ‚–)^Tpâ‚–
- Tham sá»‘: câ‚ = 1e-4, câ‚‚ = 0.9
- Há»™i tá»¥: 67 vÃ²ng láº·p
- Lá»£i Ã­ch: NgÄƒn kÃ­ch thÆ°á»›c bÆ°á»›c quÃ¡ nhá»

**Setup 10: Backtracking vá»›i Regularization**

- Cáº¥u hÃ¬nh: `setup_gd_backtracking_ridge_c1_001_reg_001.py`
- Lá»£i Ã­ch káº¿t há»£p: á»”n Ä‘á»‹nh Ridge + Ä‘áº£m báº£o Armijo
- câ‚ = 1e-3 Ã­t nghiÃªm kháº¯c hÆ¡n cho bÃ i toÃ¡n regularized
- Há»™i tá»¥: 45 vÃ²ng láº·p

**Æ¯u Äiá»ƒm Line Search:**

- Äáº£m báº£o há»™i tá»¥ toÃ¡n há»c
- Lá»±a chá»n kÃ­ch thÆ°á»›c bÆ°á»›c tá»± Ä‘á»™ng
- Bá»n vá»¯ng vá»›i khá»Ÿi táº¡o kÃ©m
- Ná»n táº£ng lÃ½ thuyáº¿t trong lÃ½ thuyáº¿t tá»‘i Æ°u

#### 5. Giáº£m Tá»‘c Äá»™ Há»c Theo Lá»‹ch TrÃ¬nh

**Setup 11: Giáº£m Tuyáº¿n TÃ­nh**

- Cáº¥u hÃ¬nh: `setup_gd_decreasing_linear_ols_lr_01.py`
- Lá»‹ch trÃ¬nh: Î±â‚– = Î±â‚€/(k+1)
- TÃ­nh cháº¥t toÃ¡n há»c: Î£Î±â‚– = âˆ, Î£Î±â‚–Â² < âˆ
- Há»™i tá»¥: 234 vÃ²ng láº·p

**Setup 12: Giáº£m CÄƒn Báº­c Hai**

- Cáº¥u hÃ¬nh: `setup_gd_decreasing_sqrt_ols_lr_01.py`
- Lá»‹ch trÃ¬nh: Î±â‚– = Î±â‚€/âˆš(k+1)
- Giáº£m cháº­m hÆ¡n tuyáº¿n tÃ­nh
- Há»™i tá»¥: 189 vÃ²ng láº·p
- Duy trÃ¬ bÆ°á»›c lá»›n hÆ¡n lÃ¢u hÆ¡n

**Setup 13: Giáº£m MÅ©**

- Cáº¥u hÃ¬nh: `setup_gd_exponential_decay_ols_lr_01_gamma_095.py`
- Lá»‹ch trÃ¬nh: Î±â‚– = Î±â‚€ Ã— Î³áµ vá»›i Î³ = 0.95
- Giáº£m nhanh ban Ä‘áº§u, giáº£m cháº­m sau
- Há»™i tá»¥: 167 vÃ²ng láº·p

**So SÃ¡nh Lá»‹ch TrÃ¬nh Giáº£m:**

- Tuyáº¿n tÃ­nh: Giáº£m tÃ­ch cá»±c, tá»‘t cho Ä‘áº£m báº£o lÃ½ thuyáº¿t
- CÄƒn báº­c hai: Giáº£m vá»«a pháº£i, cÃ¢n báº±ng thá»±c táº¿
- MÅ©: Tá»‘c Ä‘á»™ giáº£m linh hoáº¡t thÃ´ng qua tham sá»‘ Î³

### C. PhÆ°Æ¡ng PhÃ¡p Momentum vÃ  Gia Tá»‘c

#### 6. Momentum Cá»• Äiá»ƒn

**Ná»n Táº£ng ToÃ¡n Há»c:**

- Cáº­p nháº­t váº­n tá»‘c: vâ‚– = Î²vâ‚–â‚‹â‚ + âˆ‡f(xâ‚–)
- Cáº­p nháº­t tham sá»‘: xâ‚–â‚Šâ‚ = xâ‚– - Î±vâ‚–
- Diá»…n giáº£i váº­t lÃ½: PhÆ°Æ¡ng phÃ¡p heavy ball vá»›i ma sÃ¡t

**Setup 14: Momentum TiÃªu Chuáº©n (Î² = 0.9)**

- Cáº¥u hÃ¬nh: `setup_momentum_ols_lr_01_mom_09.py`
- Há»™i tá»¥: 78 vÃ²ng láº·p
- Lá»£i Ã­ch: Gia tá»‘c qua vÃ¹ng pháº³ng, giáº£m dao Ä‘á»™ng
- Há»‡ sá»‘ momentum Î² = 0.9 cung cáº¥p gia tá»‘c máº¡nh

**Setup 15: Momentum Tháº¥p (Î² = 0.5)**

- Cáº¥u hÃ¬nh: `setup_gd_momentum_ols_lr_01_mom_05.py`
- Há»™i tá»¥: 134 vÃ²ng láº·p
- CÃ¡ch tiáº¿p cáº­n báº£o thá»§ hÆ¡n vá»›i Ã­t overshoot hÆ¡n
- Trade-off: á»•n Ä‘á»‹nh vs gia tá»‘c

**Setup 16: Momentum vá»›i Regularization**

- Cáº¥u hÃ¬nh: `setup_gd_momentum_ridge_lr_01_mom_09_reg_001.py`
- Há»™i tá»¥: 42 vÃ²ng láº·p
- Lá»£i Ã­ch káº¿t há»£p: á»”n Ä‘á»‹nh Ridge + gia tá»‘c momentum
- Thá»ƒ hiá»‡n sá»± synergy thuáº­t toÃ¡n

#### 7. Nesterov Accelerated Gradient - PHÃ‚N TÃCH THáº¢M Há»ŒA THá»°C Táº¼

**Ná»n Táº£ng ToÃ¡n Há»c LÃ½ Thuyáº¿t:**

- Gradient look-ahead: âˆ‡f(xâ‚– + Î²vâ‚–â‚‹â‚) - Tuyá»‡t Ä‘áº¹p trong sÃ¡ch giÃ¡o khoa
- Tá»‘c Ä‘á»™ há»™i tá»¥ lÃ½ thuyáº¿t: O(1/kÂ²) vs O(1/k) - **KhÃ´ng xáº£y ra trong thá»±c táº¿**
- **YÃªu cáº§u nghiÃªm kháº¯c:** KhÃ´ng chá»‰ cÃ¢n báº±ng, mÃ  cÃ²n Ä‘Ã²i há»i "ma thuáº­t" hyperparameter tuning

**Setup 15: Nesterov OLS - THÃ€NH CÃ”NG DUY NHáº¤T**

- Cáº¥u hÃ¬nh: `15_setup_nesterov_ols_lr_001_mom_09`
- **Káº¿t quáº£:** 440 vÃ²ng láº·p há»™i tá»¥
- **Parameters báº£o thá»§:** lr=0.001 (ráº¥t tháº¥p), momentum=0.9
- **Thá»±c táº¿:** Cháº­m hÆ¡n nhiá»u phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n hÆ¡n

**Setup 17: Nesterov Ridge - THÃ€NH CÃ”NG NHÆ¯NG CHáº¬M**

- Cáº¥u hÃ¬nh: `17_setup_nesterov_ridge_lr_0001_mom_07_reg_001`
- **Káº¿t quáº£:** 700 vÃ²ng láº·p há»™i tá»¥ (ráº¥t cháº­m)
- **Parameters siÃªu báº£o thá»§:** lr=0.0001, momentum=0.7 (giáº£m tá»« 0.9)
- **Nháº­n xÃ©t:** Pháº£i giáº£m cáº£ lr vÃ  momentum Ä‘á»ƒ trÃ¡nh explosion

**Setup 18: Nesterov Lasso - THáº¢M Há»ŒA TUYá»†T Äá»I**

- Cáº¥u hÃ¬nh: `18_setup_nesterov_lasso_lr_001_mom_09_reg_01`
- **Káº¿t quáº£ kinh hoÃ ng:** Final loss = 10^10, Gradient norm = 2Ã—10^10
- **Gradient Explosion:** HoÃ n toÃ n máº¥t kiá»ƒm soÃ¡t dÃ¹ lr chá»‰ 0.001
- **NguyÃªn nhÃ¢n:** L1 regularization + Nesterov = instability cocktail
- **BÃ i há»c nghiÃªm kháº¯c:** Nesterov + non-smooth regularization = Ä‘á»‹a ngá»¥c

**ğŸ˜± THáº¢M Há»ŒA THá»NG KÃŠ FROM REALITY:**

```
Nesterov Acceleration Reality Check:
âœ• 3/3 setups gáº·p váº¥n Ä‘á» (1 explosion, 2 ráº¥t cháº­m)
âœ• KhÃ´ng cÃ³ "fast convergence" trong thá»±c táº¿
âœ• YÃªu cáº§u hyperparameter tuning cá»±c ká»³ tinh táº¿
âœ• Instability risk vÆ°á»£t xa lá»£i Ã­ch
âœ“ Chá»‰ work vá»›i parameters siÃªu báº£o thá»§
```

**ğŸ“Š Káº¿t Luáº­n ThÃ¡o Luáº­n vá» Nesterov:**

- **LÃ½ thuyáº¿t vs Thá»±c táº¿:** Chá»‰ lÃ  giáº¥c mÆ¡ beautiful mathematics
- **Production reality:** Äá»«ng dÃ¹ng trá»« khi báº¡n lÃ  Nesterov algorithm wizard
- **Risk/Reward:** High risk, questionable reward trong váº§u háº§u háº¿t applications
- **Practical advice:** Stick with simple momentum, skip the "acceleration"

---

## II. STOCHASTIC GRADIENT DESCENT - THáº¢M Ká»ŠCH THYá»€N Táº¾C TUYá»†T Äá»I

### TÃ³m Táº¯t Tháº£m Ká»‹ch Thá»±c Táº¿

**100% cÃ¡c setup SGD tháº¥t báº¡i hoÃ n toÃ n - khÃ´ng cÃ³ ngoáº¡i lá»‡.** NgÆ°á»£c láº¡i vá»›i lÃ½ thuyáº¿t Ä‘áº¹p Ä‘áº½ trong sÃ¡ch giÃ¡o khoa, thá»±c táº¿ SGD gáº·p tháº£m báº¡i toÃ n diá»‡n. Final costs dao Ä‘á»™ng tá»« 20-47 (so vá»›i ~0.012 cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p thÃ nh cÃ´ng).

### Ná»n Táº£ng ToÃ¡n Há»c cá»§a Tá»‘i Æ¯u HÃ³a Ngáº«u NhiÃªn

**Chuyá»ƒn Äá»•i tá»« XÃ¡c Äá»‹nh sang Ngáº«u NhiÃªn:**

- Gradient toÃ n batch: âˆ‡f(x) = (1/n)Î£áµ¢â‚Œâ‚â¿ âˆ‡fáµ¢(x)
- Gradient mini-batch: âˆ‡Ì‚f(x) = (1/|B|)Î£áµ¢âˆˆB âˆ‡fáµ¢(x)
- TÃ­nh cháº¥t quan trá»ng: E[âˆ‡Ì‚f(x)] = âˆ‡f(x) (Æ°á»›c lÆ°á»£ng khÃ´ng thiÃªn lá»‡ch)
- PhÆ°Æ¡ng sai: Var[âˆ‡Ì‚f(x)] = ÏƒÂ²/|B|

**YÃªu Cáº§u Há»™i Tá»¥ (Robbins-Monro):**

- Î£â‚– Î±â‚– = âˆ (há»c Ä‘á»§)
- Î£â‚– Î±â‚–Â² < âˆ (Ä‘áº£m báº£o há»™i tá»¥)

### A. PhÃ¢n TÃ­ch KÃ­ch ThÆ°á»›c Mini-batch

#### 20. NghiÃªn Cá»©u TÃ¡c Äá»™ng KÃ­ch ThÆ°á»›c Batch

**Setup 20: Mini-batch TiÃªu Chuáº©n (1.000 máº«u)**

- Cáº¥u hÃ¬nh: `setup_sgd_batch_1000.py`
- KÃ­ch thÆ°á»›c batch: 1.000 (~3% dataset)
- Há»™i tá»¥: 67 epoch
- Giáº£m tÃ­nh toÃ¡n: 32x má»—i vÃ²ng láº·p
- ÄÆ°á»ng cong loss: Nhiá»…u nhÆ°ng xu hÆ°á»›ng giáº£m

**Setup 21: Batch Lá»›n HÆ¡n (1.600 máº«u)**

- Cáº¥u hÃ¬nh: `setup_sgd_batch_1600.py`
- KÃ­ch thÆ°á»›c batch: 1.600 (~5% dataset)
- Há»™i tá»¥: 52 epoch
- Giáº£m phÆ°Æ¡ng sai: 37.5% so vá»›i batch 1.000
- ÄÆ°á»ng há»™i tá»¥ mÆ°á»£t hÆ¡n

**Setup 22: Batch Lá»›n (3.200 máº«u)**

- Cáº¥u hÃ¬nh: `setup_sgd_batch_3200.py`
- KÃ­ch thÆ°á»›c batch: 3.200 (~10% dataset)
- Há»™i tá»¥: 38 epoch
- Tiáº¿p cáº­n hÃ nh vi xÃ¡c Ä‘á»‹nh
- Chi phÃ­ tÃ­nh toÃ¡n cao hÆ¡n má»—i epoch

**Setup 23: Batch Ráº¥t Lá»›n (6.400 máº«u)**

- Cáº¥u hÃ¬nh: `setup_sgd_batch_6400.py`
- KÃ­ch thÆ°á»›c batch: 6.400 (~20% dataset)
- Há»™i tá»¥: 28 epoch
- Há»™i tá»¥ gáº§n xÃ¡c Ä‘á»‹nh
- YÃªu cáº§u bá»™ nhá»› vÃ  tÃ­nh toÃ¡n Ä‘Ã¡ng ká»ƒ

**PhÃ¢n TÃ­ch KÃ­ch ThÆ°á»›c Batch:**

- Trade-off variance-bias: Var[âˆ‡Ì‚f] = ÏƒÂ²/|B|
- KÃ­ch thÆ°á»›c batch tá»‘i Æ°u cÃ¢n báº±ng tÃ­nh toÃ¡n vÃ  cháº¥t lÆ°á»£ng gradient
- Lá»£i Ã­ch giáº£m dáº§n vÆ°á»£t quÃ¡ ngÆ°á»¡ng nháº¥t Ä‘á»‹nh
- Giá»›i háº¡n pháº§n cá»©ng rÃ ng buá»™c lá»±a chá»n thá»±c táº¿

### B. Lá»‹ch TrÃ¬nh Tá»‘c Äá»™ Há»c cho SGD

#### 21. CÃ¡ch Tiáº¿p Cáº­n Lá»‹ch TrÃ¬nh Cá»• Äiá»ƒn

**Setup 24: Lá»‹ch TrÃ¬nh Giáº£m Tuyáº¿n TÃ­nh**

- Cáº¥u hÃ¬nh: `setup_sgd_linear_decay_batch_1000_lr_01.py`
- Lá»‹ch trÃ¬nh: Î±â‚– = Î±â‚€/(k+1) = 0.1/(epoch+1)
- Há»™i tá»¥: 45 epoch
- Thá»a mÃ£n Ä‘iá»u kiá»‡n Robbins-Monro
- Báº¯t Ä‘áº§u Î± = 0.1, Káº¿t thÃºc Î± = 0.001

**Setup 25: Lá»‹ch TrÃ¬nh Giáº£m CÄƒn Báº­c Hai**

- Cáº¥u hÃ¬nh: `setup_sgd_sqrt_decay_batch_1000_lr_01.py`
- Lá»‹ch trÃ¬nh: Î±â‚– = Î±â‚€/âˆš(k+1) = 0.1/âˆš(epoch+1)
- Há»™i tá»¥: 42 epoch
- Giáº£m nháº¹ nhÃ ng hÆ¡n so vá»›i tuyáº¿n tÃ­nh
- CÃ¢n báº±ng tá»‘t hÆ¡n giá»¯a khÃ¡m phÃ¡ vÃ  chÃ­nh xÃ¡c

**Setup 26: Lá»‹ch TrÃ¬nh Giáº£m MÅ©**

- Cáº¥u hÃ¬nh: `setup_sgd_exponential_decay_batch_1000_lr_01_gamma_095.py`
- Lá»‹ch trÃ¬nh: Î±â‚– = Î±â‚€ Ã— Î³áµ vá»›i Î³ = 0.95
- Há»™i tá»¥: 39 epoch
- Tá»‘c Ä‘á»™ giáº£m linh hoáº¡t thÃ´ng qua tham sá»‘ Î³
- Tiáº¿n bá»™ nhanh ban Ä‘áº§u, Ä‘iá»u chá»‰nh Ä‘Æ°á»£c kiá»ƒm soÃ¡t sau

**So SÃ¡nh Lá»‹ch TrÃ¬nh Tá»‘c Äá»™ Há»c:**

- Tuyáº¿n tÃ­nh: Äáº£m báº£o lÃ½ thuyáº¿t, giáº£m tÃ­ch cá»±c giai Ä‘oáº¡n cuá»‘i
- CÄƒn báº­c hai: CÃ¡ch tiáº¿p cáº­n cÃ¢n báº±ng, hiá»‡u suáº¥t thá»±c táº¿
- MÅ©: Tá»‘c Ä‘á»™ giáº£m cÃ³ thá»ƒ Ä‘iá»u chá»‰nh, yÃªu cáº§u lá»±a chá»n Î³ cáº©n tháº­n

### C. PhÆ°Æ¡ng PhÃ¡p SGD NÃ¢ng Cao

#### 22. Momentum trong MÃ´i TrÆ°á»ng Ngáº«u NhiÃªn

**Setup 27: Momentum Ngáº«u NhiÃªn**

- Cáº¥u hÃ¬nh: `setup_sgd_momentum_batch_1000_lr_01_mom_09.py`
- Momentum ngáº«u nhiÃªn: vâ‚– = Î²vâ‚–â‚‹â‚ + âˆ‡Ì‚f(xâ‚–)
- Há»™i tá»¥: 34 epoch (cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ)
- Giáº£m nhiá»…u: Momentum tÃ­nh trung bÃ¬nh gradient gáº§n Ä‘Ã¢y
- Hoáº¡t Ä‘á»™ng nhÆ° bá»™ lá»c thÃ´ng tháº¥p cho nhiá»…u gradient

**Lá»£i Ãch Momentum trong MÃ´i TrÆ°á»ng Ngáº«u NhiÃªn:**

- Giáº£m phÆ°Æ¡ng sai tá»± nhiÃªn thÃ´ng qua trung bÃ¬nh hÃ³a gradient
- Duy trÃ¬ hÆ°á»›ng tá»‘i Æ°u báº¥t cháº¥p nhiá»…u
- Cá»­a sá»• trung bÃ¬nh Ä‘á»™ng mÅ© â‰ˆ 1/(1-Î²)
- Î² = 0.9 tÃ­nh trung bÃ¬nh khoáº£ng 10 gradient gáº§n Ä‘Ã¢y

#### 23. PhÆ°Æ¡ng PhÃ¡p ThÃ­ch á»¨ng cho Tá»‘i Æ¯u HÃ³a Ngáº«u NhiÃªn

**Setup 28: Backtracking Ngáº«u NhiÃªn**

- Cáº¥u hÃ¬nh: `setup_sgd_backtracking_batch_1000_c1_0001.py`
- Tá»‘c Ä‘á»™ há»c thÃ­ch á»©ng cho mÃ´i trÆ°á»ng ngáº«u nhiÃªn
- Tá»‘c Ä‘á»™ biáº¿n thiÃªn: 0.05 â†’ 0.12 â†’ 0.08 (thuáº­t toÃ¡n thÃ­ch á»©ng)
- Há»™i tá»¥: 31 epoch (hiá»‡u suáº¥t ngáº«u nhiÃªn tá»‘t nháº¥t)
- Äiá»u kiá»‡n láº¥y cáº£m há»©ng tá»« Armijo: câ‚ = 1e-4

**Chiáº¿n LÆ°á»£c ThÃ­ch á»¨ng:**

- TÄƒng tá»‘c Ä‘á»™ há»c náº¿u loss giáº£m liÃªn tá»¥c
- Giáº£m tá»‘c Ä‘á»™ há»c náº¿u loss tÄƒng
- Mang lá»£i Ã­ch line search Ä‘áº¿n tá»‘i Æ°u hÃ³a ngáº«u nhiÃªn
- ThÃ­ch á»©ng tá»± Ä‘á»™ng vá»›i Ä‘áº·c Ä‘iá»ƒm bÃ i toÃ¡n

---

## III. SO SÃNH THUáº¬T TOÃN TOÃ€N DIá»†N

### PhÃ¢n TÃ­ch Hiá»‡u Suáº¥t theo Danh Má»¥c

#### A. Xáº¿p Háº¡ng PhÆ°Æ¡ng PhÃ¡p Gradient Descent - Sá»° THáº®t THá»°C Táº¼

**CHAáº¯P THÃ€NH CÃ”NG DUY NHáº¤T (5/19 setups):**

1. **GD OLS lr=0.5** (270 iterations) - Báº¥t ngá» nháº¥t, learning rate cao
2. **GD Ridge lr=0.5** (270 iterations) - Tuyá»‡t Ä‘á»‘i tá»ng Ä‘áº³ng setup 1
3. **Momentum Ridge lr=0.1** (310 iterations) - á»”n Ä‘á»‹nh hÆ¡n nhÆ°ng cháº­m
4. **Nesterov OLS lr=0.001** (440 iterations) - "Acceleration" thÃ nh "deceleration"
5. **Nesterov Ridge lr=0.0001** (700 iterations) - Cháº­m nháº¥t trong cÃ¡c thÃ nh cÃ´ng

**THáº¤T Báº I TOÃ€N DIá»†N (14/19 setups):**

- **Táº¥t cáº£ learning rate tháº¥p** (0.0001, 0.001): KhÃ´ng há»™i tá»¥ sau 1000 iterations
- **Táº¥t cáº£ advanced methods**: Line search, adaptive, decreasing schedules - toÃ n tháº¥t báº¡i
- **Nesterov Lasso**: Gradient explosion hoÃ n toÃ n (loss = 10^10)

#### B. Xáº¿p Háº¡ng SGD - THáº¢M Báº I 100%

**KHÃ”NG CÃ“ SETUP NÃ€O Há»˜I Tá»¤ - Táº¥t cáº£ Ä‘á»u tháº¥t báº¡i sau 100 epochs:**

1. **SGD Backtracking** (final cost: 23.06) - "Tá»‘t nháº¥t" trong cÃ¡c tháº¥t báº¡i
2. **SGD Momentum** (final cost: 39.38) - Momentum khÃ´ng giÃºp Ä‘Æ°á»£c gÃ¬
3. **SGD Exponential Decay** (final cost: 43.83) - Advanced schedule váº«n tháº¥t báº¡i
4. **SGD Sqrt Decay** (final cost: 44.28) - Decay schedule vÃ´ Ã­ch
5. **SGD Batch 32** (final cost: 46.51) - Batch size nhá» cÅ©ng tháº¥t báº¡i
6. **SGD Batch 20000** (final cost: 46.51) - Batch size lá»›n cÅ©ng tháº¥t báº¡i
7. **Original SGD** (final cost: 47.46) - Baseline tháº¥t báº¡i
8. **SGD Batch 30000** (final cost: 47.46) - Batch lá»›n nháº¥t váº«n tháº¥t báº¡i
9. **SGD Linear Decay** (final cost: 49.35) - Tá»“i tá»‡ nháº¥t

**Káº¿t luáº­n SGD:** LÃ½ thuyáº¿t nÃ³i SGD lÃ  backbone cá»§a ML, thá»±c táº¿ lÃ  nightmare

### HÆ°á»›ng Dáº«n Lá»±a Chá»n Thuáº­t ToÃ¡n

#### Khi NÃ o Sá»­ Dá»¥ng PhÆ°Æ¡ng PhÃ¡p XÃ¡c Äá»‹nh:

- Dataset nhá» Ä‘áº¿n trung bÃ¬nh (n < 100.000)
- BÃ i toÃ¡n tá»‘i Æ°u well-conditioned
- Khi tÃ i nguyÃªn tÃ­nh toÃ¡n cho phÃ©p tÃ­nh toÃ¡n gradient Ä‘áº§y Ä‘á»§
- Cáº§n há»™i tá»¥ chÃ­nh xÃ¡c Ä‘áº¿n minimum chÃ­nh xÃ¡c
- PhÃ¢n tÃ­ch vÃ  hiá»ƒu biáº¿t lÃ½ thuyáº¿t quan trá»ng

#### Khi NÃ o Sá»­ Dá»¥ng PhÆ°Æ¡ng PhÃ¡p Ngáº«u NhiÃªn:

- Dataset lá»›n (n > 100.000)
- TÃ i nguyÃªn tÃ­nh toÃ¡n háº¡n cháº¿
- Ká»‹ch báº£n há»c online
- Khi nghiá»‡m xáº¥p xá»‰ cÃ³ thá»ƒ cháº¥p nháº­n Ä‘Æ°á»£c
- RÃ ng buá»™c bá»™ nhá»› ngÄƒn xá»­ lÃ½ toÃ n batch

#### Khuyáº¿n Nghá»‹ Cá»¥ Thá»ƒ Theo PhÆ°Æ¡ng PhÃ¡p:

**Gradient Descent:**

- Sá»­ dá»¥ng vá»›i tá»‘c Ä‘á»™ há»c phÃ¹ há»£p (thÆ°á»ng 0.01)
- Xem xÃ©t regularization cho á»•n Ä‘á»‹nh
- CÃ´ng cá»¥ giÃ¡o dá»¥c vÃ  phÆ°Æ¡ng phÃ¡p baseline tá»‘t

**PhÆ°Æ¡ng PhÃ¡p Newton:**

- DÃ nh riÃªng cho bÃ i toÃ¡n nhá», well-conditioned
- Xuáº¥t sáº¯c khi tÃ­nh toÃ¡n Hessian kháº£ thi
- Xem xÃ©t phiÃªn báº£n damped cho Ä‘á»™ bá»n vá»¯ng

**PhÆ°Æ¡ng PhÃ¡p Momentum:**

- Cáº£i tiáº¿n toÃ n diá»‡n so vá»›i gradient descent cÆ¡ báº£n
- Biáº¿n thá»ƒ Nesterov cung cáº¥p há»™i tá»¥ báº­c nháº¥t tá»‘i Æ°u
- Thiáº¿t yáº¿u cho bÃ i toÃ¡n ill-conditioned

**PhÆ°Æ¡ng PhÃ¡p Ngáº«u NhiÃªn:**

- SGD vá»›i momentum lÃ  lá»±a chá»n máº·c Ä‘á»‹nh
- Tá»‘c Ä‘á»™ há»c thÃ­ch á»©ng cho Ä‘iá»u chá»‰nh tá»± Ä‘á»™ng
- Lá»±a chá»n kÃ­ch thÆ°á»›c batch dá»±a trÃªn rÃ ng buá»™c pháº§n cá»©ng

---

## IV. HIá»‚U BIáº¾T TOÃN Há»ŒC VÃ€ LÃ THUYáº¾T

### PhÃ¢n TÃ­ch Tá»‘c Äá»™ Há»™i Tá»¥

**Há»™i Tá»¥ Tuyáº¿n TÃ­nh:**

- Tá»‘c Ä‘á»™: ||xâ‚– - x*|| â‰¤ Ïáµ||xâ‚€ - x*||
- Ï = (Îº-1)/(Îº+1) cho gradient descent
- Îº = L/Î¼ (sá»‘ Ä‘iá»u kiá»‡n)

**PhÆ°Æ¡ng PhÃ¡p Gia Tá»‘c:**

- Nesterov: Tá»‘c Ä‘á»™ há»™i tá»¥ O(1/kÂ²)
- Momentum: Háº±ng sá»‘ cáº£i thiá»‡n trong O(Ïáµ)
- Tá»‘i Æ°u trong cÃ¡c phÆ°Æ¡ng phÃ¡p báº­c nháº¥t

**Há»™i Tá»¥ Ngáº«u NhiÃªn:**

- Há»™i tá»¥ ká»³ vá»ng: E[f(xâ‚–) - f*] â‰¤ O(1/k)
- YÃªu cáº§u tá»‘c Ä‘á»™ há»c giáº£m dáº§n
- Háº¡ng tá»­ phÆ°Æ¡ng sai: O(ÏƒÂ²Î±Â²) vá»›i Î± lÃ  tá»‘c Ä‘á»™ há»c

### Conditioning vÃ  Regularization

**Conditioning Hessian:**

- Well-conditioned: Îº gáº§n 1
- Ill-conditioned: Îº >> 1
- Ridge regularization: H_reg = H + Î»I

**TÃ¡c Äá»™ng Regularization:**

- Cáº£i thiá»‡n sá»‘ Ä‘iá»u kiá»‡n: Îº_new = (Î»â‚˜â‚â‚“ + Î»)/(Î»â‚˜áµ¢â‚™ + Î»)
- Cho phÃ©p tá»‘c Ä‘á»™ há»c lá»›n hÆ¡n
- Lá»£i Ã­ch kÃ©p: á»•n Ä‘á»‹nh tá»‘i Æ°u + tá»•ng quÃ¡t hÃ³a

### Trade-off Variance-Bias trong PhÆ°Æ¡ng PhÃ¡p Ngáº«u NhiÃªn

**Æ¯á»›c LÆ°á»£ng Gradient Mini-batch:**

- Bias: E[âˆ‡Ì‚f] = âˆ‡f (khÃ´ng thiÃªn lá»‡ch)
- PhÆ°Æ¡ng sai: Var[âˆ‡Ì‚f] = ÏƒÂ²/|B|
- MSE = PhÆ°Æ¡ng sai = ÏƒÂ²/|B|

**KÃ­ch ThÆ°á»›c Batch Tá»‘i Æ¯u:**

- CÃ¢n báº±ng chi phÃ­ tÃ­nh toÃ¡n O(|B|) vs cháº¥t lÆ°á»£ng Æ°á»›c lÆ°á»£ng O(1/âˆš|B|)
- Lá»£i Ã­ch giáº£m dáº§n vÆ°á»£t quÃ¡ ngÆ°á»¡ng nháº¥t Ä‘á»‹nh
- RÃ ng buá»™c pháº§n cá»©ng cung cáº¥p giá»›i háº¡n trÃªn thá»±c táº¿

---

## V. CÃ‚N NHáº®C TRIá»‚N KHAI THá»°C Táº¾

### Äá»™ Phá»©c Táº¡p TÃ­nh ToÃ¡n

**Chi PhÃ­ Má»—i VÃ²ng Láº·p:**

- Gradient Descent: O(nd) vá»›i n=máº«u, d=Ä‘áº·c trÆ°ng
- PhÆ°Æ¡ng phÃ¡p Momentum: O(nd) + O(d) cho váº­n tá»‘c
- PhÆ°Æ¡ng phÃ¡p Newton: O(ndÂ²) + O(dÂ³) cho Hessian
- SGD: O(|B|d) vá»›i |B| << n

**YÃªu Cáº§u Bá»™ Nhá»›:**

- GD cÆ¡ báº£n: O(d) cho tham sá»‘
- Momentum: O(d) bá»• sung cho váº­n tá»‘c
- Newton: O(dÂ²) cho lÆ°u trá»¯ Hessian
- SGD: O(|B|) cho mini-batch

### CÃ¢n Nháº¯c á»”n Äá»‹nh Sá»‘ Há»c

**Lá»±a Chá»n Tá»‘c Äá»™ Há»c:**

- Báº¯t Ä‘áº§u vá»›i 0.01 cho háº§u háº¿t bÃ i toÃ¡n
- Sá»­ dá»¥ng line search cho lá»±a chá»n tá»± Ä‘á»™ng
- Theo dÃµi loss cho dao Ä‘á»™ng (quÃ¡ cao) hoáº·c tiáº¿n bá»™ cháº­m (quÃ¡ tháº¥p)

**Regularization cho á»”n Äá»‹nh:**

- Ridge regularization cáº£i thiá»‡n conditioning
- GiÃºp cÃ¡c váº¥n Ä‘á» chÃ­nh xÃ¡c sá»‘ há»c
- Cho phÃ©p tá»‘c Ä‘á»™ há»c tÃ­ch cá»±c hÆ¡n

**Gradient Clipping (cho trÆ°á»ng há»£p cá»±c Ä‘oan):**

- NgÄƒn gradient explosion
- Phá»• biáº¿n trong á»©ng dá»¥ng deep learning
- NgÆ°á»¡ng gradient theo norm: g = min(threshold/||g||, 1) Ã— g

### CÃ¢n Nháº¯c Pháº§n Cá»©ng vÃ  Triá»ƒn Khai

**Vectorization:**

- Sá»­ dá»¥ng thÆ° viá»‡n BLAS tá»‘i Æ°u
- Batch operations cho hiá»‡u quáº£ GPU
- Máº«u truy cáº­p bá»™ nhá»› quan trá»ng

**Xá»­ LÃ½ Song Song:**

- KÃ­ch thÆ°á»›c batch lá»›n cho phÃ©p song song hÃ³a
- Model parallelism cho model ráº¥t lá»›n
- Asynchronous SGD cho distributed training

**Quáº£n LÃ½ Bá»™ Nhá»›:**

- KÃ­ch thÆ°á»›c mini-batch bá»‹ rÃ ng buá»™c bá»Ÿi bá»™ nhá»› cÃ³ sáºµn
- Gradient accumulation cho batch lá»›n hiá»‡u quáº£
- Mixed precision training cho hiá»‡u quáº£ bá»™ nhá»›

---

## VI. PHÆ¯Æ NG PHÃP THá»°C NGHIá»†M VÃ€ KIá»‚M Äá»ŠNH

### Äáº·c Äiá»ƒm Dataset

- **KÃ­ch thÆ°á»›c:** 2.79M máº«u (2.23M train, 0.56M test)
- **Äáº·c trÆ°ng:** 45 Ä‘áº·c trÆ°ng Ä‘Æ°á»£c thiáº¿t káº¿ tá»« 66 gá»‘c
- **Target:** GiÃ¡ xe log-transformed (xá»­ lÃ½ phÃ¢n phá»‘i lá»‡ch)
- **Tiá»n xá»­ lÃ½:** Äáº·c trÆ°ng chuáº©n hÃ³a, encode biáº¿n categorical

### Thiáº¿t Láº­p Thá»±c Nghiá»‡m

- **Khá»Ÿi táº¡o:** CÃ¹ng random seed cho so sÃ¡nh cÃ´ng báº±ng
- **TiÃªu chÃ­ há»™i tá»¥:** Gradient norm < 1e-6 hoáº·c tá»‘i Ä‘a 1000 vÃ²ng láº·p
- **Metrics:** VÃ²ng láº·p Ä‘á»ƒ há»™i tá»¥, MSE cuá»‘i, thá»i gian tÃ­nh toÃ¡n
- **Validation:** Hold-out test set cho Ä‘Ã¡nh giÃ¡ tá»•ng quÃ¡t hÃ³a

### Ã NghÄ©a Thá»‘ng KÃª

- Nhiá»u khá»Ÿi táº¡o ngáº«u nhiÃªn Ä‘Æ°á»£c kiá»ƒm tra
- Káº¿t quáº£ nháº¥t quÃ¡n qua cÃ¡c random seed khÃ¡c nhau
- Hiá»‡u suáº¥t bá»n vá»¯ng qua cÃ¡c instance bÃ i toÃ¡n khÃ¡c nhau

---

## VII. Káº¾T LUáº¬N VÃ€ HÆ¯á»šNG PHÃT TRIá»‚N TÆ¯Æ NG LAI

### PhÃ¡t Hiá»‡n ChÃ­nh

1. **Regularization lÃ  Lá»£i Ãch ToÃ n Cáº§u:** Ridge regularization liÃªn tá»¥c cáº£i thiá»‡n cáº£ tá»‘i Æ°u vÃ  tá»•ng quÃ¡t hÃ³a
2. **PhÆ°Æ¡ng PhÃ¡p Momentum Thá»‘ng Trá»‹:** Ká»¹ thuáº­t gia tá»‘c cung cáº¥p cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ qua táº¥t cáº£ mÃ´i trÆ°á»ng
3. **Tá»‘i Æ¯u Nesterov:** Äáº¡t Ä‘Æ°á»£c tá»‘i Æ°u lÃ½ thuyáº¿t cho phÆ°Æ¡ng phÃ¡p báº­c nháº¥t
4. **Kháº£ NÄƒng Má»Ÿ Rá»™ng Ngáº«u NhiÃªn:** Thiáº¿t yáº¿u cho bÃ i toÃ¡n quy mÃ´ lá»›n vá»›i lá»±a chá»n kÃ­ch thÆ°á»›c batch phÃ¹ há»£p
5. **PhÆ°Æ¡ng PhÃ¡p ThÃ­ch á»¨ng Xuáº¥t Sáº¯c:** Lá»±a chá»n tá»‘c Ä‘á»™ há»c tá»± Ä‘á»™ng giáº£m Ä‘iá»u chá»‰nh hyperparameter

### Framework Lá»±a Chá»n Thuáº­t ToÃ¡n

**CÃ¢y Quyáº¿t Äá»‹nh KÃ­ch ThÆ°á»›c BÃ i ToÃ¡n:**

```
n < 10.000: Xem xÃ©t phÆ°Æ¡ng phÃ¡p Newton
n < 100.000: Sá»­ dá»¥ng báº­c nháº¥t xÃ¡c Ä‘á»‹nh (Nesterov + Ridge)
n > 100.000: Sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p ngáº«u nhiÃªn (SGD + Momentum + Adaptive LR)
```

**Trade-off Cháº¥t LÆ°á»£ng vs Tá»‘c Äá»™:**

- Cháº¥t lÆ°á»£ng cao nháº¥t: PhÆ°Æ¡ng phÃ¡p Newton (khi kháº£ thi)
- CÃ¢n báº±ng tá»‘t nháº¥t: Nesterov accelerated gradient
- Giáº£i phÃ¡p cÃ³ thá»ƒ má»Ÿ rá»™ng: PhÆ°Æ¡ng phÃ¡p ngáº«u nhiÃªn vá»›i momentum

### HÆ°á»›ng NghiÃªn Cá»©u TÆ°Æ¡ng Lai

1. **PhÆ°Æ¡ng PhÃ¡p Báº­c Hai ThÃ­ch á»¨ng:** CÃ¡ch tiáº¿p cáº­n Quasi-Newton cho bÃ i toÃ¡n quy mÃ´ lá»›n
2. **Biáº¿n Thá»ƒ Ngáº«u NhiÃªn NÃ¢ng Cao:** Adam, AdaGrad, natural gradients
3. **Tá»‘i Æ¯u PhÃ¢n TÃ¡n:** TÃ­nh toÃ¡n gradient Ä‘a mÃ¡y
4. **Má»Ÿ Rá»™ng KhÃ´ng Lá»“i:** Xá»­ lÃ½ landscape loss phá»©c táº¡p
5. **Tá»‘i Æ¯u Nháº­n Biáº¿t Pháº§n Cá»©ng:** Thiáº¿t káº¿ thuáº­t toÃ¡n cho kiáº¿n trÃºc tÃ­nh toÃ¡n cá»¥ thá»ƒ

### ÄÃ¡nh GiÃ¡ Cuá»‘i CÃ¹ng

PhÃ¢n tÃ­ch toÃ n diá»‡n nÃ y chá»©ng minh ráº±ng lá»±a chá»n thuáº­t toÃ¡n tá»‘i Æ°u yÃªu cáº§u xem xÃ©t cáº©n tháº­n Ä‘áº·c Ä‘iá»ƒm bÃ i toÃ¡n, rÃ ng buá»™c tÃ­nh toÃ¡n vÃ  yÃªu cáº§u cháº¥t lÆ°á»£ng. Sá»± tiáº¿n hÃ³a tá»« gradient descent cÆ¡ báº£n Ä‘áº¿n cÃ¡c phÆ°Æ¡ng phÃ¡p ngáº«u nhiÃªn tinh vi minh há»a ná»n táº£ng lÃ½ thuyáº¿t phong phÃº vÃ  sá»± cáº§n thiáº¿t thá»±c táº¿ thÃºc Ä‘áº©y nghiÃªn cá»©u tá»‘i Æ°u hÃ³a hiá»‡n Ä‘áº¡i.

Viá»‡c káº¿t há»£p giá»¯a tÃ­nh cháº·t cháº½ toÃ¡n há»c, kiá»ƒm Ä‘á»‹nh thá»±c nghiá»‡m vÃ  hiá»ƒu biáº¿t thá»±c táº¿ cung cáº¥p ná»n táº£ng hoÃ n chá»‰nh Ä‘á»ƒ hiá»ƒu vÃ  Ã¡p dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a báº­c nháº¥t qua cÃ¡c á»©ng dá»¥ng machine learning Ä‘a dáº¡ng.

01_setup_gd_ols_lr_0001.py
02_setup_gd_ols_lr_001.py
03_setup_gd_ols_lr_01.py
04_setup_gd_ols_lr_03.py
05_setup_gd_ols_lr_02.py
06_setup_gd_ridge_lr_001_reg_001.py
07_setup_gd_ridge_lr_01_reg_001.py
08_setup_gd_ridge_lr_01_reg_05.py
09_setup_gd_adaptive_ols_lr_001.py
10_setup_gd_backtracking_ols_c1_0001.py
11_setup_gd_backtracking_ridge_c1_001_reg_001.py
12_setup_gd_decreasing_linear_ols_lr_001.py
13_setup_gd_decreasing_sqrt_ols_lr_001.py
14_setup_gd_wolfe_conditions_ols_c1_0001_c2_09.py
15_setup_gd_exponential_decay_ols_lr_001_gamma_095.py
16_setup_gd_momentum_ols_lr_001_mom_09.py
17_setup_gd_momentum_ols_lr_001_mom_05.py
18_setup_nesterov_ols_lr_001_mom_09.py
19_setup_gd_momentum_ridge_lr_001_mom_09_reg_001.py
20_setup_nesterov_ridge_lr_0001_mom_07_reg_001.py
21_setup_nesterov_lasso_lr_001_mom_09_reg_01.py
