# Gradient Descent Methods - Kiáº¿n Thá»©c ToÃ¡n Há»c & Setups

## ğŸ“š **LÃ½ Thuyáº¿t ToÃ¡n Há»c**

### **Äá»‹nh NghÄ©a CÆ¡ Báº£n**
Gradient Descent lÃ  thuáº­t toÃ¡n tá»‘i Æ°u iterative Ä‘á»ƒ tÃ¬m minimum cá»§a function f(x).

**CÃ´ng thá»©c cáº­p nháº­t:**
```
x_{k+1} = x_k - Î±_k âˆ‡f(x_k)
```

Trong Ä‘Ã³:
- `x_k`: Ä‘iá»ƒm hiá»‡n táº¡i
- `Î±_k`: learning rate (step size)
- `âˆ‡f(x_k)`: gradient táº¡i x_k
- `x_{k+1}`: Ä‘iá»ƒm tiáº¿p theo

### **Gradient cá»§a MSE (Mean Squared Error)**
Cho bÃ i toÃ¡n linear regression: `f(w) = (1/2n) ||Xw - y||Â²`

**Gradient:**
```
âˆ‡f(w) = (1/n) X^T (Xw - y)
```

**Hessian:**
```
âˆ‡Â²f(w) = (1/n) X^T X
```

### **Äiá»u Kiá»‡n Convergence**
1. **Lipschitz Continuity:** `||âˆ‡f(x) - âˆ‡f(y)|| â‰¤ L||x - y||`
2. **Strong Convexity:** `f(y) â‰¥ f(x) + âˆ‡f(x)^T(y-x) + (Î¼/2)||y-x||Â²`

**Convergence rate:** `O(1/k)` cho convex, `O(Ï^k)` cho strongly convex

## ğŸ¯ **CÃ¡c Setup vÃ  Ã NghÄ©a**

### **1. Standard Setup**
```python
learning_rate = 0.01      # Moderate
max_iterations = 1000     # Sufficient
tolerance = 1e-6          # Good precision
```

**ğŸ§  CÃ¡ch nhá»›:**
- "0.01 = 1% cá»§a gradient má»—i step"
- "NhÆ° Ä‘i bá»™ vá»«a pháº£i, khÃ´ng vá»™i khÃ´ng cháº­m"
- "1000 bÆ°á»›c Ä‘á»§ cho háº§u háº¿t bÃ i toÃ¡n"

**âš–ï¸ Trade-offs:**
- âœ… á»”n Ä‘á»‹nh, Ã­t dao Ä‘á»™ng
- âœ… Dá»… tune, predictable
- âŒ KhÃ´ng nhanh nháº¥t
- âŒ CÃ³ thá»ƒ cháº­m cho sá»‘ liá»‡u lá»›n

### **2. Fast Setup**
```python
learning_rate = 0.1       # High
max_iterations = 500      # Fewer needed
tolerance = 1e-5          # Slightly relaxed
```

**ğŸ§  CÃ¡ch nhá»›:**
- "0.1 = 10% cá»§a gradient, bÆ°á»›c to"
- "NhÆ° cháº¡y nhanh, cÃ³ thá»ƒ vÆ°á»£t Ä‘Ã­ch"
- "500 bÆ°á»›c vÃ¬ hy vá»ng convergence nhanh"

**âš–ï¸ Trade-offs:**
- âœ… Convergence ráº¥t nhanh
- âœ… Tá»‘t cho experimentation
- âŒ Risk overshooting
- âŒ CÃ³ thá»ƒ dao Ä‘á»™ng

**âš ï¸ Warning Signs:**
- Cost tÄƒng thay vÃ¬ giáº£m
- Oscillations around minimum
- NaN values

### **3. Precise Setup**
```python
learning_rate = 0.001     # Low
max_iterations = 2000     # More needed
tolerance = 1e-8          # Very strict
```

**ğŸ§  CÃ¡ch nhá»›:**
- "0.001 = 0.1% gradient, bÆ°á»›c nhá»"
- "NhÆ° walking meditation, tá»« tá»« chÃ­nh xÃ¡c"
- "2000 bÆ°á»›c vÃ¬ cáº§n thá»i gian"

**âš–ï¸ Trade-offs:**
- âœ… Highest precision
- âœ… Very stable
- âœ… Finds true minimum
- âŒ Slow training
- âŒ May be overkill

## ğŸ“ˆ **Convergence Analysis**

### **Linear Convergence Rate**
Cho strongly convex function:
```
f(x_k) - f* â‰¤ Ï^k (f(x_0) - f*)
```

Trong Ä‘Ã³ `Ï = (Îº-1)/(Îº+1)`, `Îº = L/Î¼` (condition number)

**Condition number impact:**
- `Îº = 1`: Perfect (Ï = 0)
- `Îº = 10`: Good (Ï â‰ˆ 0.82)
- `Îº = 100`: Poor (Ï â‰ˆ 0.98)

### **Optimal Learning Rate**
Cho quadratic function: `Î±* = 2/(L + Î¼)`

**Rule of thumb:**
- Start with `Î± = 1/L`
- If oscillating: decrease Î±
- If too slow: increase Î±

## ğŸ”§ **Setup Selection Guide**

### **Theo Má»¥c ÄÃ­ch**

| Má»¥c Ä‘Ã­ch | Setup | LÃ½ do |
|----------|-------|-------|
| ğŸ“ **Há»c táº­p** | Standard | Hiá»ƒu rÃµ behavior, Ã­t surprising |
| âš¡ **Prototyping** | Fast | Káº¿t quáº£ nhanh, iterate ideas |
| ğŸ­ **Production** | Precise | Accuracy quan trá»ng, cÃ³ thá»i gian |
| ğŸ§ª **Research** | Precise | Reproducible, high quality |

### **Theo Äáº·c Äiá»ƒm Dataset**

| Dataset | Khuyáº¿n nghá»‹ | LÃ½ do |
|---------|------------|-------|
| **Small (< 1K)** | Precise | CÃ³ thá»ƒ afford slow training |
| **Medium (1K-10K)** | Standard | Balance tá»‘t |
| **Large (> 10K)** | Fast hoáº·c SGD | Training time matters |

### **Theo Condition Number**

| Condition Îº | Learning Rate | Strategy |
|-------------|---------------|-----------|
| **Îº < 10** | 0.1 (Fast) | Well-conditioned, can go fast |
| **10 â‰¤ Îº < 100** | 0.01 (Standard) | Moderate conditioning |
| **Îº â‰¥ 100** | 0.001 (Precise) | Ill-conditioned, go slow |

## ğŸ§  **Memory Aids**

### **Learning Rate Intuition**
```
Î± = 0.001  â†’  "Cá»¥ giÃ  Ä‘i bá»™"     (cháº­m, cháº¯c cháº¯n)
Î± = 0.01   â†’  "NgÆ°á»i bÃ¬nh thÆ°á»ng" (vá»«a pháº£i)
Î± = 0.1    â†’  "Cháº¡y nhanh"       (nhanh, riskë„˜ì–´ì§)
Î± = 1.0    â†’  "Nháº£y xa"          (probably overshoot)
```

### **Convergence Patterns**
```
ğŸ“‰ Good:     Cost giáº£m smooth
ğŸ“Š OK:       Cost giáº£m vá»›i small oscillations  
ğŸŒŠ Warning:  Cost oscillates around minimum
ğŸ’¥ Bad:      Cost explodes or NaN
```

### **Debugging Checklist**
1. **Cost tÄƒng?** â†’ Giáº£m learning rate
2. **Too slow?** â†’ TÄƒng learning rate hoáº·c check convergence
3. **Plateaus?** â†’ Check tolerance hoáº·c local minimum
4. **NaN values?** â†’ Learning rate quÃ¡ lá»›n

## ğŸ“ **Advanced Concepts**

### **Momentum Variant**
```python
v = Î² * v + Î± * gradient
w = w - v
```
- `Î² = 0.9`: "Nhá»› 90% momentum cÅ©"
- GiÃºp vÆ°á»£t local minima
- Accelerate in consistent directions

### **Adaptive Learning Rate**
```python
Î±_t = Î±_0 / (1 + decay_rate * t)
```
- Start fast, slow down over time
- "NhÆ° brake khi gáº§n Ä‘Ã­ch"

### **Line Search**
Automatically find optimal step size:
```python
Î±_t = argmin_Î± f(x_t - Î± * âˆ‡f(x_t))
```

## ğŸ” **Troubleshooting Guide**

### **Common Issues**

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Overshooting** | Cost oscillates/increases | Decrease Î± |
| **Slow convergence** | Cost plateaus early | Increase Î± or check tolerance |
| **Poor conditioning** | Very slow, erratic | Use preconditioning |
| **Local minimum** | Stuck at suboptimal | Add momentum or restart |

### **Setup Debugging**
```python
# Quick test for learning rate
for Î± in [0.001, 0.01, 0.1]:
    run_few_iterations(Î±)
    if cost_decreased:
        print(f"Î± = {Î±} works")
```

## ğŸ“– **Further Reading**

### **Key Papers**
1. **Cauchy (1847)**: Original gradient descent
2. **Polyak (1964)**: Momentum methods
3. **Nesterov (1983)**: Accelerated gradient descent

### **Modern Variations**
- **AdaGrad**: Adaptive per-parameter learning rates
- **Adam**: Adaptive moments estimation  
- **RMSprop**: Root mean square propagation

---

*"Gradient Descent: Follow the steepest downhill path to find the valley bottom"* ğŸ”ï¸â¡ï¸ğŸï¸