# Proximal Gradient Descent - Ki·∫øn Th·ª©c To√°n H·ªçc & Setups

## üìö **L√Ω Thuy·∫øt To√°n H·ªçc**

### **ƒê·ªãnh Nghƒ©a C∆° B·∫£n**
Proximal Gradient Descent gi·∫£i b√†i to√°n optimization c√≥ d·∫°ng:
```
minimize f(x) + g(x)
```
Trong ƒë√≥:
- `f(x)`: smooth function (c√≥ gradient)
- `g(x)`: non-smooth function (th∆∞·ªùng l√† regularization)

### **Algorithm**
```
1. Forward step:  z_k = x_k - Œ± ‚àáf(x_k)
2. Proximal step: x_{k+1} = prox_{Œ±g}(z_k)
```

### **Proximal Operator**
```
prox_Œªh(v) = argmin_x { h(x) + (1/2Œª)||x - v||¬≤ }
```

**Intuition:** T√¨m ƒëi·ªÉm g·∫ßn v nh∆∞ng minimize h(x)

### **Cho L1 Regularization**
Problem: `minimize (1/2)||Xw - y||¬≤ + Œª||w||‚ÇÅ`

- `f(w) = (1/2)||Xw - y||¬≤` (smooth)
- `g(w) = Œª||w||‚ÇÅ` (non-smooth)

**Proximal operator c·ªßa L1:**
```
prox_Œª|¬∑|(v) = sign(v) ‚äô max(|v| - Œª, 0)
```

ƒê√¢y ch√≠nh l√† **soft thresholding function**!

### **Soft Thresholding**
```
soft_threshold(v, Œª) = {
  v - Œª,  if v > Œª
  0,      if |v| ‚â§ Œª  
  v + Œª,  if v < -Œª
}
```

**üß† Intuition:** "Shrink v·ªÅ 0, set to 0 n·∫øu qu√° nh·ªè"

## üéØ **Setup v√† √ù Nghƒ©a**

### **Standard Setup**
```python
learning_rate = 0.01     # Moderate
lambda_l1 = 0.01         # Light regularization
max_iterations = 1000    # Sufficient
tolerance = 1e-6         # Good precision
```

**üß† C√°ch nh·ªõ:**
- "0.01 learning rate = standard nh∆∞ GD"
- "0.01 lambda = 1% penalty tr√™n weights"
- "Balance gi·ªØa fit v√† sparsity"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ Good balance fit vs sparsity
- ‚úÖ Moderate feature selection
- ‚úÖ Stable convergence
- ‚ùå May not be sparse enough

**Expected sparsity:** ~20-40% weights = 0

### **Sparse Setup**
```python
learning_rate = 0.01     # Same
lambda_l1 = 0.1          # Higher regularization
max_iterations = 1500    # More iterations
tolerance = 1e-7         # Precise
```

**üß† C√°ch nh·ªõ:**
- "0.1 lambda = 10% penalty, khuy·∫øn kh√≠ch sparsity"
- "1500 iterations v√¨ c·∫ßn th·ªùi gian ƒë·ªÉ sparse"
- "Higher penalty ‚Üí more zeros"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ High sparsity (60-80% zeros)
- ‚úÖ Automatic feature selection
- ‚úÖ Interpretable models
- ‚ùå May sacrifice some accuracy
- ‚ùå Risk of underfitting

### **Dense Setup**
```python
learning_rate = 0.01     # Same
lambda_l1 = 0.001        # Light regularization
max_iterations = 800     # Fewer needed
tolerance = 1e-5         # Relaxed
```

**üß† C√°ch nh·ªõ:**
- "0.001 lambda = 0.1% penalty, g·∫ßn nh∆∞ no regularization"
- "G·∫ßn nh∆∞ standard regression"
- "√çt sparsity, focus on accuracy"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ High accuracy
- ‚úÖ Keeps most features
- ‚úÖ Fast convergence
- ‚ùå Little feature selection
- ‚ùå Less interpretable

## üßÆ **Mathematical Deep Dive**

### **Convergence Theory**
For `f` L-smooth v√† `g` convex:
```
f(x_k) + g(x_k) - [f(x*) + g(x*)] ‚â§ O(1/k)
```

**Key insight:** Same convergence rate as gradient descent!

### **Sparsity Analysis**
Soft thresholding t·∫°o sparsity:
```
If |‚àáf(w)[i]| ‚â§ Œª ‚üπ w[i] = 0
```

**Intuition:** Feature i b·ªã set v·ªÅ 0 n·∫øu gradient nh·ªè h∆°n penalty

### **Regularization Path**
Khi Œª tƒÉng:
```
Œª = 0     ‚Üí No sparsity (standard regression)
Œª = small ‚Üí Some sparsity
Œª = large ‚Üí High sparsity  
Œª = ‚àû     ‚Üí All weights = 0
```

## üìä **Lambda Selection Guide**

### **Rule of Thumb**
```python
# Start with
Œª_max = max(|X^T y|) / n  # Largest useful lambda
Œª_start = 0.1 * Œª_max     # Good starting point
```

### **Cross-Validation Strategy**
```python
lambdas = [0.001, 0.01, 0.1, 1.0]
for Œª in lambdas:
    cv_score = cross_validate(Œª)
    sparsity = count_zeros(Œª)
    print(f"Œª={Œª}: CV={cv_score:.3f}, Sparsity={sparsity}%")
```

### **Lambda Values & Effects**

| Œª | Sparsity | Accuracy | Use Case |
|---|----------|----------|----------|
| **0.001** | 0-10% | Highest | Dense features needed |
| **0.01** | 20-40% | High | Balanced |
| **0.1** | 60-80% | Medium | Feature selection |
| **1.0** | 90%+ | Lower | Extreme sparsity |

## üéØ **Khi N√†o D√πng Proximal GD**

### **Perfect Use Cases**
- ‚úÖ **High-dimensional data** (p >> n)
- ‚úÖ **Feature selection** c·∫ßn thi·∫øt
- ‚úÖ **Interpretable models** required
- ‚úÖ **Sparse solutions** preferred
- ‚úÖ **Overfitting** concerns

### **Avoid When**
- ‚ùå **All features important** 
- ‚ùå **Non-linear relationships** (use kernels)
- ‚ùå **Very small datasets** (may overregularize)
- ‚ùå **Need smooth solutions** (L2 better)

### **Comparison v·ªõi Other Methods**

| Method | Regularization | Sparsity | Smoothness |
|--------|----------------|----------|------------|
| **Ridge** | L2: Œª‚àëw¬≤‚Çç·µ¢‚Çé | No | Yes |
| **Lasso (Proximal GD)** | L1: Œª‚àë\|w·µ¢\| | Yes | No |
| **Elastic Net** | L1 + L2 | Moderate | Moderate |

## üß† **Memory Aids & Intuition**

### **Proximal GD nh∆∞ "Penalty Kicks"**
```
1. Forward step = Kick ball toward goal (gradient step)
2. Proximal step = Referee applies penalty rules (regularization)
   - Light penalty (small Œª) = Ball continues mostly unchanged
   - Heavy penalty (large Œª) = Ball gets stopped/redirected
```

### **Soft Thresholding Visualization**
```
Input:    -2  -1  -0.5  0  0.5  1  2
Œª = 0.3:  -1.7 -0.7  0   0   0  0.7 1.7
Œª = 1.0:  -1   0    0   0   0   0   1
```

### **Sparsity Intuition**
```
Œª = 0.001 ‚Üí "Gentle encouragement to be small"
Œª = 0.01  ‚Üí "Moderate pressure for sparsity"  
Œª = 0.1   ‚Üí "Strong push toward zeros"
Œª = 1.0   ‚Üí "Heavy penalty, force many zeros"
```

## üîß **Implementation Tips**

### **Efficient Soft Thresholding**
```python
def soft_threshold(x, lambda_):
    return np.sign(x) * np.maximum(np.abs(x) - lambda_, 0)

# Vectorized for efficiency
def proximal_l1(x, alpha, lambda_l1):
    threshold = alpha * lambda_l1
    return soft_threshold(x, threshold)
```

### **Convergence Monitoring**
```python
# Track multiple metrics
metrics = {
    'total_cost': [],
    'smooth_cost': [],  
    'l1_penalty': [],
    'sparsity_count': [],
    'active_features': []
}
```

### **Warm Start Strategy**
```python
# Start with less regularization, gradually increase
lambdas = [0.001, 0.01, 0.1]
weights = initialize_random()

for Œª in lambdas:
    weights = proximal_gd(X, y, Œª, init_weights=weights)
```

## üîç **Troubleshooting Guide**

### **Common Issues**

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Not sparse enough** | Few zeros | Increase Œª |
| **Too sparse** | Many important features = 0 | Decrease Œª |
| **Poor accuracy** | High test error | Decrease Œª or add L2 |
| **Slow convergence** | Many iterations | Check learning rate |
| **Oscillations** | Cost fluctuates | Decrease learning rate |

### **Debugging Sparsity**
```python
# Check sparsity evolution
sparsity_ratio = np.mean(np.abs(weights) < 1e-10)
print(f"Sparsity: {sparsity_ratio:.1%}")

# Check feature importance
important_features = np.where(np.abs(weights) > threshold)[0]
print(f"Active features: {len(important_features)}")
```

### **Feature Selection Analysis**
```python
# Sort features by importance
feature_importance = np.abs(weights)
sorted_indices = np.argsort(feature_importance)[::-1]

print("Top 10 most important features:")
for i in range(10):
    idx = sorted_indices[i]
    print(f"Feature {idx}: weight = {weights[idx]:.4f}")
```

## üìà **Advanced Topics**

### **Accelerated Proximal GD (FISTA)**
```python
# Nesterov acceleration
t_prev = 1
w_prev = w
for iteration in range(max_iter):
    t = (1 + np.sqrt(1 + 4 * t_prev**2)) / 2
    y = w + ((t_prev - 1) / t) * (w - w_prev)
    
    # Standard proximal step on y
    z = y - alpha * gradient(y)
    w_new = soft_threshold(z, alpha * lambda_l1)
```

### **Adaptive Lambda Selection**
```python
# Decrease lambda during training
lambda_schedule = lambda_init * (decay_rate ** iteration)
```

---

*"Proximal GD: Combine smooth optimization with non-smooth penalties for automatic feature selection"* üéØ‚úÇÔ∏è