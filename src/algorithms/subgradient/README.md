# Subgradient Methods - Kiáº¿n Thá»©c ToÃ¡n Há»c & Setups

## ðŸ“š **LÃ½ Thuyáº¿t ToÃ¡n Há»c**

### **Äá»‹nh NghÄ©a CÆ¡ Báº£n**
Subgradient method Ä‘á»ƒ optimize **non-smooth functions** - functions khÃ´ng cÃ³ gradient everywhere.

**Algorithm:**
```
x_{k+1} = x_k - Î±_k g_k
```
Trong Ä‘Ã³ `g_k âˆˆ âˆ‚f(x_k)` lÃ  subgradient táº¡i x_k

### **Subgradient Definition**
Vector g lÃ  subgradient cá»§a f táº¡i x náº¿u:
```
f(y) â‰¥ f(x) + g^T(y - x)  âˆ€y
```

**Intuition:** g support hyperplane tá»« dÆ°á»›i lÃªn function f

### **Subdifferential**
```
âˆ‚f(x) = {g : g is subgradient of f at x}
```

**Properties:**
- Náº¿u f differentiable táº¡i x: `âˆ‚f(x) = {âˆ‡f(x)}`
- Náº¿u f non-smooth táº¡i x: `âˆ‚f(x)` lÃ  set vá»›i nhiá»u elements

### **Subgradient cá»§a L1 Norm**
For `f(x) = |x|`:
```
âˆ‚|x| = {
  {1},     if x > 0
  {-1},    if x < 0  
  [-1,1],  if x = 0
}
```

For vector `f(w) = ||w||â‚ = Î£áµ¢|wáµ¢|`:
```
âˆ‚||w||â‚ = (âˆ‚|wâ‚|, âˆ‚|wâ‚‚|, ..., âˆ‚|wâ‚™|)
```

### **Subgradient cá»§a Max Function**
For `f(x) = max{fâ‚(x), fâ‚‚(x)}`:
```
âˆ‚f(x) = conv{âˆ‡fáµ¢(x) : i âˆˆ I(x)}
```
Trong Ä‘Ã³ `I(x) = {i : fáµ¢(x) = f(x)}` (active set)

## ðŸŽ¯ **Algorithm Properties**

### **Convergence Rate**
For convex f, subgradient method cÃ³:
```
f(x_k) - f* â‰¤ O(1/âˆšk)
```

**So sÃ¡nh:**
- Gradient descent: O(1/k) 
- Newton method: O(1/kÂ²)
- **Subgradient: O(1/âˆšk) - CHáº¬M Há»šN!**

### **Non-Monotonic Convergence**
**Key difference:** Cost khÃ´ng giáº£m monotonically!
- Gradient descent: f(x_{k+1}) â‰¤ f(x_k)
- **Subgradient: f(x_{k+1}) cÃ³ thá»ƒ > f(x_k)**

**Solution:** Track best point found so far!

### **Step Size Rules**
1. **Constant:** Î±â‚– = Î± (simple nhÆ°ng may not converge)
2. **Diminishing:** Î±â‚– = Î±/âˆšk (guarantees convergence)
3. **Square summable:** Î£Î±â‚– = âˆž, Î£Î±â‚–Â² < âˆž

## ðŸŽ¯ **Setup vÃ  Ã NghÄ©a**

### **Standard Setup**
```python
learning_rate = 0.01     # Constant step size
lambda_l1 = 0.01         # L1 regularization
max_iterations = 2000    # More iterations needed
tolerance = 1e-6         # For best point tracking
```

**ðŸ§  CÃ¡ch nhá»›:**
- "0.01 nhÆ° GD nhÆ°ng behavior khÃ¡c"
- "2000 iterations vÃ¬ convergence cháº­m"
- "Need patience vá»›i subgradient!"

**âš–ï¸ Trade-offs:**
- âœ… Handles non-smooth functions
- âœ… Simple implementation
- âœ… General purpose
- âŒ Slow convergence O(1/âˆšk)
- âŒ Non-monotonic cost

### **Diminishing Step Setup**
```python
learning_rate_init = 0.1    # Start larger
decay_rate = 0.99           # Gradual decay
max_iterations = 3000       # Even more iterations
```

**Formula:** `Î±_k = Î±_0 / âˆšk` hoáº·c `Î±_k = Î±_0 * decay_rate^k`

**ðŸ§  CÃ¡ch nhá»›:**
- "Start big, shrink over time"
- "Like cooling in simulated annealing"
- "3000 iterations vÃ¬ cáº§n thá»i gian"

**âš–ï¸ Trade-offs:**
- âœ… Guaranteed convergence
- âœ… Better theoretical properties
- âŒ Very slow initially
- âŒ Complex tuning

### **Aggressive Setup**
```python
learning_rate = 0.05     # Higher step size
lambda_l1 = 0.05         # Higher regularization  
max_iterations = 1500    # Fewer iterations
early_stopping = True    # Stop if not improving
```

**ðŸ§  CÃ¡ch nhá»›:**
- "Take bigger risks cho faster progress"
- "Stop early if khÃ´ng improve"
- "High risk, high reward"

**âš–ï¸ Trade-offs:**
- âœ… Potentially faster
- âœ… More exploration
- âŒ Less stable
- âŒ May not converge

## ðŸ§® **Mathematical Examples**

### **L1 Regularized Regression**
Problem: `minimize (1/2)||Xw - y||Â² + Î»||w||â‚`

**Subgradient:**
```python
def compute_subgradient(X, y, w, lambda_l1):
    # Smooth part
    smooth_grad = X.T @ (X @ w - y)
    
    # L1 subgradient
    l1_subgrad = np.zeros_like(w)
    l1_subgrad[w > 0] = lambda_l1
    l1_subgrad[w < 0] = -lambda_l1
    # For w[i] = 0, choose 0 (could be any value in [-Î», Î»])
    
    return smooth_grad + l1_subgrad
```

### **Hinge Loss (SVM)**
For `f(w) = max(0, 1 - y(w^T x))`:

**Subgradient:**
```python
def hinge_subgradient(X, y, w):
    margins = y * (X @ w)
    subgrad = np.zeros_like(w)
    
    # Active constraints (margin < 1)
    active = margins < 1
    subgrad = -np.sum(X[active] * y[active].reshape(-1, 1), axis=0)
    
    return subgrad
```

## ðŸŽ¯ **Khi NÃ o DÃ¹ng Subgradient**

### **Perfect Use Cases**
- âœ… **Non-smooth objectives** (L1, Lâˆž, max, min)
- âœ… **Constraint violations** (penalty methods)
- âœ… **Robust optimization** (minimax problems)
- âœ… **No smooth approximation** available
- âœ… **Simple implementation** needed

### **Avoid When**
- âŒ **Smooth alternatives** exist (use proximal GD)
- âŒ **Fast convergence** required
- âŒ **High precision** needed
- âŒ **Limited iterations** budget

### **Examples in Practice**

| Application | Non-smooth part | Why subgradient |
|-------------|-----------------|-----------------|
| **LASSO** | L1 penalty | Sparsity-inducing |
| **SVM** | Hinge loss | Margin violations |
| **Robust regression** | L1 loss | Outlier resistance |
| **TV denoising** | Total variation | Edge preservation |

## ðŸ§  **Memory Aids & Intuition**

### **Subgradient nhÆ° "Blind Navigation"**
```
Gradient Descent = Äi vá»›i GPS chÃ­nh xÃ¡c
- Biáº¿t exact direction (gradient)
- Smooth path

Subgradient = Äi vá»›i compass gáº§n Ä‘Ãºng
- Direction roughly correct (subgradient)
- May zigzag, khÃ´ng smooth
- Eventually reach destination
```

### **Non-Monotonic Behavior**
```
Iteration 1: Cost = 100 â¬‡ï¸
Iteration 2: Cost = 80  â¬‡ï¸  
Iteration 3: Cost = 90  â¬†ï¸ (worse!)
Iteration 4: Cost = 75  â¬‡ï¸ (better than best)
...

Keep tracking: best_cost = 75
```

### **Convergence Intuition**
```
O(1/âˆšk) means:
k = 100  â†’ Error âˆ 1/10 = 0.1
k = 10000 â†’ Error âˆ 1/100 = 0.01

Need 100x more iterations for 10x better accuracy!
```

## ðŸ”§ **Implementation Best Practices**

### **Best Point Tracking**
```python
best_weights = weights.copy()
best_cost = float('inf')

for iteration in range(max_iterations):
    # Subgradient step
    subgrad = compute_subgradient(X, y, weights, lambda_l1)
    weights -= learning_rate * subgrad
    
    # Evaluate and track best
    current_cost = compute_cost(X, y, weights, lambda_l1)
    if current_cost < best_cost:
        best_cost = current_cost
        best_weights = weights.copy()

# Use best_weights, not final weights!
return best_weights
```

### **Adaptive Step Size**
```python
def diminishing_step_size(iteration, alpha_0=0.1):
    return alpha_0 / np.sqrt(iteration + 1)

def scheduled_step_size(iteration, alpha_0=0.1, decay=0.99):
    return alpha_0 * (decay ** iteration)
```

### **Convergence Monitoring**
```python
# Don't use current cost (non-monotonic)
# Use best cost or moving average
window_size = 50
recent_costs = costs[-window_size:]
avg_recent_cost = np.mean(recent_costs)

if iteration > window_size:
    improvement = prev_avg - avg_recent_cost
    if improvement < tolerance:
        print("Converged based on moving average")
        break
```

## ðŸ” **Troubleshooting Guide**

### **Common Issues**

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Oscillating costs** | Cost goes up/down | Normal! Track best point |
| **Very slow progress** | Little improvement | Increase learning rate |
| **Divergence** | Costs explode | Decrease learning rate |
| **Poor final result** | High error | Use diminishing step size |

### **Debugging Checklist**
```python
# 1. Check subgradient computation
subgrad = compute_subgradient(X, y, weights, lambda_l1)
print(f"Subgradient norm: {np.linalg.norm(subgrad):.6f}")

# 2. Monitor best vs current
print(f"Current cost: {current_cost:.6f}")
print(f"Best cost: {best_cost:.6f}")
print(f"Gap: {current_cost - best_cost:.6f}")

# 3. Check step size effect
costs_after_steps = []
for alpha in [0.001, 0.01, 0.1]:
    test_weights = weights - alpha * subgrad
    test_cost = compute_cost(X, y, test_weights, lambda_l1)
    costs_after_steps.append((alpha, test_cost))
```

### **Performance Tuning**
```python
# Experiment with step sizes
alphas = [0.001, 0.01, 0.1, 1.0]
best_alpha = None
best_final_cost = float('inf')

for alpha in alphas:
    final_weights, final_cost = subgradient_method(X, y, alpha)
    if final_cost < best_final_cost:
        best_final_cost = final_cost
        best_alpha = alpha

print(f"Best alpha: {best_alpha}")
```

## ðŸ“ˆ **Advanced Variations**

### **Projected Subgradient**
For constrained problems:
```python
# After subgradient step
weights = weights - alpha * subgrad
# Project onto constraint set
weights = project_onto_constraints(weights)
```

### **Averaging**
Polyak averaging for better convergence:
```python
# Keep running average of iterates
w_avg = (1/(k+1)) * ((k * w_avg) + w_k)
```

### **Bundle Methods**
Use multiple subgradients for better directions.

---

*"Subgradient Methods: Navigate non-smooth landscapes with approximate directions, patience required!"* ðŸ—»ðŸ§­