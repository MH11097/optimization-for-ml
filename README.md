# Dá»± Ãn Tá»‘i Æ¯u Thuáº­t ToÃ¡n - Dá»± ÄoÃ¡n GiÃ¡ Xe CÅ©

**Framework Python Ä‘Æ¡n giáº£n Ä‘á»ƒ so sÃ¡nh cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u trÃªn bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ xe cÅ©.**

## ğŸš€ Khá»Ÿi Cháº¡y Nhanh

```bash
# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Cháº¡y tá»«ng bÆ°á»›c
python src/01_eda.py
python src/02_preprocessing.py
python src/algorithms/gradient_descent/standard_setup.py
python src/algorithms/algorithm_comparator.py --list
```

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
data/                           # Dá»¯ liá»‡u theo workflow sá»‘
â”œâ”€â”€ 00_raw/                    # Dá»¯ liá»‡u gá»‘c
â”œâ”€â”€ 01_eda/                    # Káº¿t quáº£ phÃ¢n tÃ­ch dá»¯ liá»‡u  
â”œâ”€â”€ 02_processed/              # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”œâ”€â”€ 03_algorithms/             # Káº¿t quáº£ cÃ¡c thuáº­t toÃ¡n
â”‚   â”œâ”€â”€ gradient_descent/
â”‚   â”œâ”€â”€ newton_method/
â”‚   â”œâ”€â”€ ridge_regression/
â”‚   â””â”€â”€ stochastic_gd/
â””â”€â”€ 04_comparison/             # So sÃ¡nh cuá»‘i cÃ¹ng

src/                           # Code theo workflow sá»‘
â”œâ”€â”€ 01_eda.py                 # PhÃ¢n tÃ­ch dá»¯ liá»‡u
â”œâ”€â”€ 02_preprocessing.py       # Xá»­ lÃ½ dá»¯ liá»‡u
â”œâ”€â”€ algorithms/               # CÃ¡c thuáº­t toÃ¡n
â”‚   â”œâ”€â”€ gradient_descent/     # Gradient Descent vá»›i nhiá»u setup
â”‚   â”œâ”€â”€ newton_method/        # Newton Method
â”‚   â”œâ”€â”€ ridge_regression/     # Ridge Regression
â”‚   â”œâ”€â”€ stochastic_gd/        # Stochastic GD
â”‚   â”œâ”€â”€ advanced_methods/     # CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ¢ng cao
â”‚   â””â”€â”€ algorithm_comparator.py  # Tool so sÃ¡nh tá»•ng há»£p
â””â”€â”€ utils/                    # Tiá»‡n Ã­ch chung
```

## ğŸ¯ CÃ¡c Thuáº­t ToÃ¡n CÃ³ Sáºµn

### **Gradient Descent**
- `standard_setup.py` - Learning rate 0.01, á»•n Ä‘á»‹nh
- `fast_setup.py` - Learning rate 0.1, nhanh
- `precise_setup.py` - Learning rate 0.001, chÃ­nh xÃ¡c
- `medium_setup.py` - Learning rate 0.05, cÃ¢n báº±ng
- `slow_setup.py` - Learning rate 0.005, tá»« tá»«

### **Newton Method**
- `standard_setup.py` - Setup chuáº©n vá»›i regularization

### **Proximal GD** 
- `standard_setup.py` - Cho L1 regularization (Lasso)

### **Subgradient Methods**
- `standard_setup.py` - Cho non-smooth optimization

### **Ridge Regression**
- Regularized linear regression

### **Stochastic GD**
- Online learning vá»›i mini-batches

### **Advanced Methods**
- Adam, RMSprop, BFGS vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ¢ng cao

## ğŸ”„ CÃ¡ch Sá»­ Dá»¥ng

### **BÆ°á»›c 1: PhÃ¢n TÃ­ch Dá»¯ Liá»‡u**
```bash
python src/01_eda.py
```
- **Input**: `data/00_raw/used_cars_data.csv`
- **Output**: `data/01_eda/` (biá»ƒu Ä‘á»“, phÃ¢n tÃ­ch correlation, thá»‘ng kÃª)

### **BÆ°á»›c 2: Xá»­ LÃ½ Dá»¯ Liá»‡u**
```bash
python src/02_preprocessing.py
```
- **Input**: Dá»¯ liá»‡u gá»‘c
- **Output**: `data/02_processed/` (train/test data Ä‘Ã£ clean)

### **BÆ°á»›c 3: Cháº¡y Thuáº­t ToÃ¡n**
```bash
# Cháº¡y tá»«ng thuáº­t toÃ¡n
python src/algorithms/gradient_descent/standard_setup.py
python src/algorithms/newton_method/standard_setup.py

# Hoáº·c cháº¡y nhiá»u setup gradient descent
python src/algorithms/gradient_descent/fast_setup.py
python src/algorithms/gradient_descent/precise_setup.py
```

### **BÆ°á»›c 4: So SÃ¡nh Káº¿t Quáº£**
```bash
# Xem cÃ¡c káº¿t quáº£ cÃ³ sáºµn
python src/algorithms/algorithm_comparator.py --list

# So sÃ¡nh 2 thuáº­t toÃ¡n cá»¥ thá»ƒ
python src/algorithms/algorithm_comparator.py compare gradient_descent/standard newton_method/standard

# So sÃ¡nh táº¥t cáº£ setup cá»§a gradient descent
python src/algorithms/algorithm_comparator.py analyze gradient_descent

# Cháº¿ Ä‘á»™ interactive
python src/algorithms/algorithm_comparator.py --interactive

# Táº¡o bÃ¡o cÃ¡o toÃ n diá»‡n
python src/algorithms/algorithm_comparator.py report --all
```

## ğŸ“Š TÃ­nh NÄƒng So SÃ¡nh

### **Algorithm Comparator Tool**
File `algorithm_comparator.py` tÃ­ch há»£p táº¥t cáº£ tÃ­nh nÄƒng:

- âœ… **Load káº¿t quáº£** tá»« `/data/03_algorithms/` 
- âœ… **So sÃ¡nh metrics** (MSE, RÂ², thá»i gian training)
- âœ… **Visualization** (6 loáº¡i biá»ƒu Ä‘á»“)
- âœ… **Convergence analysis** tá»« training history
- âœ… **Radar charts** cho so sÃ¡nh Ä‘a chiá»u
- âœ… **Interactive selection** mode
- âœ… **BÃ¡o cÃ¡o comprehensive** 

### **Metrics ÄÆ°á»£c So SÃ¡nh**
- **Test MSE** - Mean Squared Error (cÃ ng nhá» cÃ ng tá»‘t)
- **RÂ² Score** - Coefficient of determination (cÃ ng gáº§n 1 cÃ ng tá»‘t)  
- **Training Time** - Thá»i gian training (giÃ¢y)
- **Convergence** - Sá»‘ iterations Ä‘á»ƒ há»™i tá»¥
- **MAPE** - Mean Absolute Percentage Error

## ğŸ¨ Visualization

Má»—i thuáº­t toÃ¡n táº¡o ra:
- **Training curves** - ÄÆ°á»ng há»™i tá»¥ cá»§a cost function
- **Predictions vs Actual** - So sÃ¡nh dá»± Ä‘oÃ¡n vá»›i thá»±c táº¿
- **Residual plots** - PhÃ¢n tÃ­ch sai sá»‘
- **Performance comparison** - So sÃ¡nh giá»¯a cÃ¡c thuáº­t toÃ¡n

## ğŸ“ˆ Káº¿t Quáº£ Má»—i Thuáº­t ToÃ¡n

Má»—i setup táº¡o folder riÃªng chá»©a:
```
data/03_algorithms/gradient_descent/standard_setup/
â”œâ”€â”€ results.json           # Metrics vÃ  metadata
â”œâ”€â”€ training_history.csv   # Lá»‹ch sá»­ training
â”œâ”€â”€ weights.npy           # Weights Ä‘Ã£ train
â””â”€â”€ standard_setup_results.png  # Visualization
```

## ğŸ’¡ HÆ°á»›ng Dáº«n ThÃªm Thuáº­t ToÃ¡n Má»›i

1. **Copy setup cÃ³ sáºµn:**
```bash
cp src/algorithms/gradient_descent/standard_setup.py src/algorithms/my_algorithm/my_setup.py
```

2. **Sá»­a implementation trong file má»›i**

3. **Cháº¡y vÃ  test:**
```bash
python src/algorithms/my_algorithm/my_setup.py
```

4. **So sÃ¡nh vá»›i tool:**
```bash
python src/algorithms/algorithm_comparator.py compare gradient_descent/standard my_algorithm/my_setup
```

## ğŸ”§ Development

### **Requirements ChÃ­nh**
- Python 3.8+
- numpy, pandas, matplotlib, seaborn
- sklearn (cho comparison)
- pathlib, json (built-in)

### **Thiáº¿t Káº¿ ÄÆ¡n Giáº£n**
- **Workflow sá»‘** - Dá»… theo dÃµi pipeline
- **Scripts Ä‘á»™c láº­p** - Má»—i bÆ°á»›c cháº¡y riÃªng Ä‘Æ°á»£c
- **No complex classes** - Chá»‰ functions Ä‘Æ¡n giáº£n
- **Standardized output** - Format nháº¥t quÃ¡n Ä‘á»ƒ so sÃ¡nh

### **Memory Optimized**
- Chunked data loading
- Efficient numpy operations
- Reasonable memory footprint

## ğŸ“‹ Dataset: Xe CÅ© Má»¹

**3 triá»‡u records** tá»« CarGurus.com vá»›i 66 features:
- **GiÃ¡ xe** (target variable)
- **ThÃ´ng sá»‘ ká»¹ thuáº­t** (engine, transmission, fuel economy)
- **TÃ¬nh tráº¡ng** (mileage, accidents, owner count)
- **ThÃ´ng tin Ä‘á»‹a lÃ½** (city, dealer info)

## ğŸ¯ Má»¥c ÄÃ­ch Dá»± Ãn

- **Há»c thuáº­t toÃ¡n tá»‘i Æ°u** qua vÃ­ dá»¥ thá»±c táº¿
- **So sÃ¡nh performance** cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau  
- **Hiá»ƒu trade-offs** giá»¯a speed, accuracy, complexity
- **Thá»±c hÃ nh** implementation tá»« scratch vs libraries

---

**Dá»± Ã¡n nÃ y táº­p trung vÃ o viá»‡c hiá»ƒu rÃµ cÃ¡c thuáº­t toÃ¡n tá»‘i Æ°u qua bÃ i toÃ¡n dá»± Ä‘oÃ¡n giÃ¡ xe cÅ© Ä‘Æ¡n giáº£n vÃ  dá»… theo dÃµi.**