# Advanced Optimization Methods - Ki·∫øn Th·ª©c To√°n H·ªçc & Setups

## üìö **T·ªïng Quan Advanced Methods**

### **Ph√¢n Lo·∫°i Methods**
Advanced optimization methods gi·∫£i quy·∫øt c√°c limitations c·ªßa basic methods:

| Category | Methods | Main Innovation |
|----------|---------|-----------------|
| **Adaptive Learning Rate** | AdaGrad, RMSprop, Adam | Per-parameter learning rates |
| **Momentum Variants** | Heavy Ball, Nesterov, NADAM | Acceleration techniques |
| **Coordinate Methods** | Coordinate Descent, ADMM | Structured optimization |
| **Variance Reduction** | SVRG, SAGA, SARAH | Reduce stochastic noise |
| **Second-Order** | Natural Gradients, K-FAC | Better curvature information |
| **Meta-Learning** | MAML, Reptile | Learn to optimize |

### **Khi N√†o C·∫ßn Advanced Methods**
- ‚úÖ **Standard methods fail**: Convergence issues
- ‚úÖ **Special structure**: Sparse, constrained, multi-objective
- ‚úÖ **Scale challenges**: Very large or very small problems
- ‚úÖ **Performance critical**: Need best possible results

## üöÄ **AdaGrad - Adaptive Gradient**

### **Core Idea**
Adapt learning rate cho t·ª´ng parameter based on historical gradients:
```
Œ±_t^{(i)} = Œ± / ‚àö(G_t^{(i)} + Œµ)
```

**Update rule:**
```
G_t^{(i)} = G_{t-1}^{(i)} + (g_t^{(i)})¬≤
w_t^{(i)} = w_{t-1}^{(i)} - Œ±_t^{(i)} * g_t^{(i)}
```

### **Setup Configurations**

#### **Standard AdaGrad Setup**
```python
learning_rate = 0.01      # Higher than SGD
epsilon = 1e-8            # Numerical stability
accumulate_gradients = True # Track gradient squares
```

**üß† Intuition:**
- "Parameters v·ªõi large gradients get smaller learning rates"
- "Automatically adapts to feature importance"
- "Good for sparse features"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ No manual LR tuning per parameter
- ‚úÖ Great for sparse data
- ‚úÖ Robust to feature scaling
- ‚ùå Learning rate monotonically decreases
- ‚ùå Can stop learning too early

#### **Modified AdaGrad Setup**
```python
learning_rate = 0.1       # Even higher initial
epsilon = 1e-6            # Less conservative
decay_factor = 0.99       # Forget old gradients slowly
```

## üéØ **RMSprop - Root Mean Square Propagation**

### **Core Idea**
Fix AdaGrad's decreasing learning rate problem v·ªõi exponential moving average:
```
v_t = Œ≤ * v_{t-1} + (1-Œ≤) * g_t¬≤
w_t = w_{t-1} - Œ± * g_t / ‚àö(v_t + Œµ)
```

### **Setup Configurations**

#### **Standard RMSprop Setup**
```python
learning_rate = 0.001     # Standard rate
beta = 0.9                # Exponential decay factor
epsilon = 1e-8            # Numerical stability
```

**üß† Intuition:**
- "Œ≤ = 0.9 means remember 90% of recent gradient history"
- "Like AdaGrad but v·ªõi forgetting mechanism"
- "Good middle ground between AdaGrad v√† SGD"

#### **Aggressive RMSprop Setup**
```python
learning_rate = 0.01      # Higher rate
beta = 0.99               # Longer memory
epsilon = 1e-6            # Less conservative
```

## ‚ö° **Adam - Adaptive Moment Estimation**

### **Core Idea**
Combines momentum (first moment) v·ªõi adaptive learning rates (second moment):
```
m_t = Œ≤‚ÇÅ * m_{t-1} + (1-Œ≤‚ÇÅ) * g_t          # First moment
v_t = Œ≤‚ÇÇ * v_{t-1} + (1-Œ≤‚ÇÇ) * g_t¬≤         # Second moment

mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ·µó)                      # Bias correction
vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ·µó)                      # Bias correction

w_t = w_{t-1} - Œ± * mÃÇ_t / (‚àövÃÇ_t + Œµ)
```

### **Setup Configurations**

#### **Standard Adam Setup**
```python
learning_rate = 0.001     # Conservative default
beta1 = 0.9               # Momentum parameter
beta2 = 0.999             # RMSprop parameter
epsilon = 1e-8            # Numerical stability
```

**üß† Intuition:**
- "Œ≤‚ÇÅ = 0.9: Remember direction (momentum)"
- "Œ≤‚ÇÇ = 0.999: Remember gradient magnitudes"
- "Bias correction prevents slow start"
- "Best of both worlds: momentum + adaptive LR"

#### **Fast Adam Setup**
```python
learning_rate = 0.01      # More aggressive
beta1 = 0.9               # Standard momentum
beta2 = 0.99              # Shorter adaptive memory
epsilon = 1e-6            # Less conservative
```

#### **Stable Adam Setup**
```python
learning_rate = 0.0001    # Very conservative
beta1 = 0.95              # More momentum
beta2 = 0.9999            # Longer adaptive memory
epsilon = 1e-10           # High precision
```

## üåä **Nesterov Accelerated Gradient (NAG)**

### **Core Idea**
"Look ahead" tr∆∞·ªõc khi compute gradient:
```
v_t = Œº * v_{t-1} + Œ± * ‚àáf(w_t - Œº * v_{t-1})
w_t = w_{t-1} - v_t
```

### **Setup Configurations**

#### **Standard NAG Setup**
```python
learning_rate = 0.01      # Standard rate
momentum = 0.9            # Strong momentum
```

**üß† Intuition:**
- "Look ahead = check gradient at future position"
- "Prevents overshooting better than standard momentum"
- "Especially good cho convex functions"

## üé≤ **SVRG - Stochastic Variance Reduced Gradient**

### **Core Idea**
Reduce variance trong stochastic gradients b·∫±ng full gradient snapshots:
```
# Every m iterations
Œº = (1/n) Œ£·µ¢ ‚àáf·µ¢(wÃÉ)  # Full gradient snapshot

# Regular iterations  
g_t = ‚àáf·µ¢(w_t) - ‚àáf·µ¢(wÃÉ) + Œº
w_t = w_{t-1} - Œ± * g_t
```

### **Setup Configurations**

#### **Standard SVRG Setup**
```python
learning_rate = 0.1       # Can be larger due to variance reduction
snapshot_frequency = 100  # Compute full gradient every 100 iterations
```

## üßÆ **Method Comparison Matrix**

### **Convergence Rates**

| Method | Convex | Strongly Convex | Non-convex |
|--------|--------|-----------------|------------|
| **SGD** | O(1/‚àök) | O(1/k) | - |
| **AdaGrad** | O(1/‚àök) | O(log k/k) | - |
| **RMSprop** | O(1/‚àök) | O(1/k) | Good empirically |
| **Adam** | O(1/‚àök) | O(1/k) | Good empirically |
| **SVRG** | O(1/k) | O(exp(-k)) | - |

### **Memory Requirements**

| Method | Per Parameter | Total Memory |
|--------|---------------|--------------|
| **SGD** | O(1) | O(n) |
| **AdaGrad** | O(1) | O(n) |
| **RMSprop** | O(1) | O(n) |
| **Adam** | O(1) | O(2n) |
| **SVRG** | O(1) | O(n) |

### **Hyperparameter Sensitivity**

| Method | LR Sensitivity | Other Parameters |
|--------|----------------|------------------|
| **SGD** | High | Medium (momentum) |
| **AdaGrad** | Medium | Low |
| **RMSprop** | Medium | Medium (Œ≤) |
| **Adam** | Low | Low (Œ≤‚ÇÅ, Œ≤‚ÇÇ) |
| **SVRG** | Medium | Medium (m) |

## üéØ **Method Selection Guide**

### **Theo Problem Type**

| Problem Type | Recommended | Why |
|--------------|-------------|-----|
| **Computer Vision** | Adam, SGD+momentum | Good v·ªõi noisy gradients |
| **NLP** | Adam, AdaGrad | Sparse features common |
| **Tabular Data** | BFGS, Adam | Well-conditioned problems |
| **Reinforcement Learning** | PPO, TRPO | Policy optimization needs |
| **Large Scale** | SGD variants | Memory efficiency |

### **Theo Dataset Size**

| Dataset Size | Method | Reason |
|--------------|--------|--------|
| **Small (< 1K)** | BFGS, Newton | Can afford second-order |
| **Medium (1K-100K)** | Adam, RMSprop | Good balance |
| **Large (100K-1M)** | SGD, SVRG | Efficiency matters |
| **Very Large (> 1M)** | Distributed SGD | Scalability critical |

### **Theo Computational Budget**

| Budget | Method | Trade-off |
|--------|--------|-----------|
| **Low** | SGD | Simple, fast per iteration |
| **Medium** | Adam | Adaptive, reasonable overhead |
| **High** | SVRG, Natural Gradients | Best convergence |

## üîß **Implementation Best Practices**

### **Adam Implementation**
```python
class AdamOptimizer:
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.m = None  # First moment
        self.v = None  # Second moment
        self.t = 0     # Time step
    
    def update(self, weights, gradients):
        if self.m is None:
            self.m = np.zeros_like(weights)
            self.v = np.zeros_like(weights)
        
        self.t += 1
        
        # Update moments
        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients
        self.v = self.beta2 * self.v + (1 - self.beta2) * gradients**2
        
        # Bias correction
        m_hat = self.m / (1 - self.beta1**self.t)
        v_hat = self.v / (1 - self.beta2**self.t)
        
        # Update weights
        weights = weights - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
        return weights
```

### **Adaptive Learning Rate Scheduling**
```python
def adaptive_lr_schedule(optimizer, val_loss_history, patience=10):
    if len(val_loss_history) > patience:
        recent_losses = val_loss_history[-patience:]
        if all(recent_losses[i] >= recent_losses[i-1] 
               for i in range(1, len(recent_losses))):
            # No improvement for 'patience' epochs
            optimizer.lr *= 0.5
            print(f"Reducing learning rate to {optimizer.lr}")
```

## üß† **Memory Aids & Intuition**

### **Optimizer Family Tree**
```
SGD (Grandparent)
‚îú‚îÄ‚îÄ SGD + Momentum (Better direction memory)
‚îÇ   ‚îî‚îÄ‚îÄ Nesterov (Look-ahead momentum)
‚îú‚îÄ‚îÄ AdaGrad (Adaptive per-parameter rates)
‚îÇ   ‚îî‚îÄ‚îÄ RMSprop (Forget old gradients)
‚îÇ       ‚îî‚îÄ‚îÄ Adam (RMSprop + Momentum)
‚îÇ           ‚îî‚îÄ‚îÄ AdamW (Better weight decay)
‚îî‚îÄ‚îÄ Variance Reduction (SVRG, SAGA)
```

### **Optimizer Personalities**
```
SGD        = "Simple, reliable worker"
           - Does the job, no frills
           - Needs good supervision (LR tuning)

Adam       = "Smart, adaptable employee"  
           - Learns what works for each task
           - Generally reliable v·ªõi minimal supervision

SVRG       = "Meticulous perfectionist"
           - Takes time to get full picture
           - Delivers high-quality results

AdaGrad    = "Enthusiastic starter who burns out"
           - Great initial progress
           - Slows down over time
```

### **Learning Rate Intuition**
```
SGD:      üéØ "One size fits all arrows"
AdaGrad:  üèπ "Arrows adapt to target distance"  
RMSprop:  üéØüîÑ "Adaptive arrows v·ªõi memory reset"
Adam:     üèπüß† "Smart arrows v·ªõi direction memory"
```

## üîç **Troubleshooting Advanced Methods**

### **Common Issues**

| Issue | Method | Solution |
|-------|--------|----------|
| **Slow convergence** | Adam | Increase LR or decrease Œ≤‚ÇÇ |
| **Unstable training** | AdaGrad | Add more regularization |
| **Poor generalization** | All adaptive | Try SGD v·ªõi momentum |
| **High memory usage** | Adam | Switch to AdaGrad |

### **Convergence Diagnostics**
```python
def diagnose_optimizer(loss_history, grad_norms):
    # Check for common patterns
    if np.std(loss_history[-50:]) > np.mean(loss_history[-50:]) * 0.1:
        print("Warning: High loss variance - consider reducing LR")
    
    if np.mean(grad_norms[-10:]) < 1e-6:
        print("Warning: Very small gradients - possible convergence")
    
    if len(loss_history) > 100:
        recent_improvement = loss_history[-100] - loss_history[-1]
        if recent_improvement < 1e-4:
            print("Warning: Minimal recent improvement")
```

## üìà **Future Directions**

### **Emerging Methods**
- **Lookahead Optimizer**: Maintains two sets of weights
- **RAdam**: Rectified Adam v·ªõi warm-up
- **AdaBound**: Transitions from Adam to SGD
- **LARS/LAMB**: Large batch optimization

### **Meta-Optimization**
- **Learning to optimize**: Neural networks that learn optimization
- **AutoML for optimizers**: Automatic optimizer selection
- **Federated optimization**: Distributed learning scenarios

---

*"Advanced Methods: Standing on the shoulders of giants to reach optimization excellence"* üöÄüß†‚ö°