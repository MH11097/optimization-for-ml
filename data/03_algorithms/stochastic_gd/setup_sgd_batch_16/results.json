{
  "algorithm": "Stochastic Gradient Descent",
  "loss_function": "OLS",
  "parameters": {
    "learning_rate": 0.01,
    "epochs": 100,
    "batch_size": 16,
    "random_state": 42
  },
  "training_time": 202.49114346504211,
  "convergence": {
    "epochs": 100,
    "final_cost": NaN
  }
}