#!/usr/bin/env python3
"""
NewtonModel - Class cho Pure Newton Method
Há»— trá»£ cÃ¡c loss functions: OLS, Ridge, Lasso
"""

import pandas as pd
import numpy as np
from pathlib import Path
import time
import sys
import os
import json

# Add the src directory to path Ä‘á»ƒ import utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from utils.optimization_utils import (
    du_doan, giai_he_phuong_trinh_tuyen_tinh,
    danh_gia_mo_hinh, in_ket_qua_danh_gia, kiem_tra_dieu_kien_dung,
    tinh_gia_tri_ham_loss, tinh_gradient_ham_loss, tinh_hessian_ham_loss,
    add_bias_column
)
from utils.visualization_utils import (
    ve_duong_hoi_tu, ve_duong_dong_muc_optimization, ve_du_doan_vs_thuc_te
)


class NewtonModel:
    """
    Pure Newton Method Model vá»›i há»— trá»£ nhiá»u loss functions
    
    Parameters:
    - ham_loss: 'ols', 'ridge', 'lasso'
    - regularization: Tham sá»‘ regularization cho Ridge/Lasso vÃ  numerical stability
    - so_lan_thu: Sá»‘ láº§n láº·p tá»‘i Ä‘a
    - diem_dung: NgÆ°á»¡ng há»™i tá»¥ (gradient norm)
    - numerical_regularization: Regularization cho numerical stability cá»§a Hessian
    - convergence_check_freq: Táº§n suáº¥t kiá»ƒm tra há»™i tá»¥ (má»—i N iterations)
    """
    
    def __init__(self, ham_loss='ols', regularization=0.01, so_lan_thu=100000, 
                 diem_dung=1e-10, numerical_regularization=1e-8, convergence_check_freq=1):
        self.ham_loss = ham_loss.lower()
        self.regularization = regularization
        self.so_lan_thu = so_lan_thu
        self.diem_dung = diem_dung
        self.numerical_regularization = numerical_regularization
        self.convergence_check_freq = convergence_check_freq
        
        # Validate supported loss function
        if self.ham_loss not in ['ols', 'ridge', 'lasso']:
            raise ValueError(f"KhÃ´ng há»— trá»£ loss function: {ham_loss}")
        
        # Sá»­ dá»¥ng unified functions vá»›i format má»›i (bias trong X)
        self.loss_func = lambda X, y, w: tinh_gia_tri_ham_loss(self.ham_loss, X, y, w, None, self.regularization)
        self.grad_func = lambda X, y, w: tinh_gradient_ham_loss(self.ham_loss, X, y, w, None, self.regularization)
        self.hess_func = lambda X: tinh_hessian_ham_loss(self.ham_loss, X, None, self.regularization)
        
        # Khá»Ÿi táº¡o cÃ¡c thuá»™c tÃ­nh lÆ°u káº¿t quáº£
        self.weights = None  # BÃ¢y giá» bao gá»“m bias á»Ÿ cuá»‘i
        self.loss_history = []
        self.gradient_norms = []
        self.weights_history = []
        self.step_sizes = []
        self.training_time = 0
        self.converged = False
        self.final_iteration = 0
        self.condition_number = None

    def _get_best_results(self):
        """
        Láº¥y káº¿t quáº£ tá»‘t nháº¥t dá»±a trÃªn gradient norm tháº¥p nháº¥t
        
        Returns:
            dict: Chá»©a best_weights, best_loss, best_gradient_norm, best_iteration
        """
        if not self.gradient_norms:
            raise ValueError("KhÃ´ng cÃ³ lá»‹ch sá»­ gradient norms Ä‘á»ƒ tÃ¬m káº¿t quáº£ tá»‘t nháº¥t")
        
        # TÃ¬m index cÃ³ gradient norm tháº¥p nháº¥t
        best_idx = np.argmin(self.gradient_norms)
        
        return {
            'best_weights': self.weights_history[best_idx],
            'best_loss': self.loss_history[best_idx],
            'best_gradient_norm': self.gradient_norms[best_idx],
            'best_iteration': best_idx * self.convergence_check_freq
        }
        
    def fit(self, X, y):
        """
        Huáº¥n luyá»‡n model vá»›i dá»¯ liá»‡u X, y
        
        Returns:
        - dict: Káº¿t quáº£ training bao gá»“m weights, loss_history, etc.
        """
        print(f"ðŸš€ Training Newton Method - {self.ham_loss.upper()}")
        print(f"   Regularization: {self.regularization}")
        print(f"   Numerical regularization: {self.numerical_regularization}")
        print(f"   Max iterations: {self.so_lan_thu}")
        
        # ThÃªm cá»™t bias vÃ o X
        X_with_bias = add_bias_column(X)
        print(f"   Original features: {X.shape[1]}, With bias: {X_with_bias.shape[1]}")
        
        # Initialize weights (bao gá»“m bias á»Ÿ cuá»‘i)
        n_features_with_bias = X_with_bias.shape[1]
        self.weights = np.random.normal(0, 0.01, n_features_with_bias)
        
        # Reset histories
        self.loss_history = []
        self.gradient_norms = []
        self.weights_history = []
        self.step_sizes = []
        
        start_time = time.time()
        
        # Precompute Hessian using unified function
        H = self.hess_func(X_with_bias)
        
        # Add numerical regularization for stability
        H_reg = H + self.numerical_regularization * np.eye(n_features_with_bias)
        self.condition_number = np.linalg.cond(H_reg)
        
        print(f"   Hessian condition number: {self.condition_number:.2e}")
        
        for lan_thu in range(self.so_lan_thu):
            # TÃ­nh gradient (luÃ´n cáº§n cho Newton step)
            gradient_w, _ = self.grad_func(X_with_bias, y, self.weights)  # _ vÃ¬ khÃ´ng cáº§n gradient_b riÃªng
            
            # Chá»‰ tÃ­nh loss vÃ  lÆ°u history khi cáº§n thiáº¿t
            should_check_converged = (
                (lan_thu + 1) % self.convergence_check_freq == 0 or 
                lan_thu == self.so_lan_thu - 1
            )
            
            if should_check_converged:
                # Chá»‰ tÃ­nh loss khi cáº§n (expensive operation)
                loss_value = self.loss_func(X_with_bias, y, self.weights)
                gradient_norm = np.linalg.norm(gradient_w)
                
                # LÆ°u vÃ o history
                self.loss_history.append(loss_value)
                self.gradient_norms.append(gradient_norm)
                self.weights_history.append(self.weights.copy())
                
                cost_change = 0.0 if len(self.loss_history) == 0 else (self.loss_history[-1] - loss_value) if len(self.loss_history) == 1 else (self.loss_history[-2] - self.loss_history[-1])
                should_stop, converged, reason = kiem_tra_dieu_kien_dung(
                    gradient_norm=gradient_norm,
                    cost_change=cost_change, 
                    iteration=lan_thu,
                    tolerance=self.diem_dung,
                    max_iterations=self.so_lan_thu
                )
                
                if should_stop:
                    if converged:
                        print(f"âœ… Newton Method converged: {reason}")
                    else:
                        print(f"âš ï¸ Newton Method stopped (not converged): {reason}")
                    self.converged = converged
                    self.final_iteration = lan_thu + 1
                    break
            
            # Newton step
            try:
                # For Lasso, might need to recompute Hessian
                if self.ham_loss == 'lasso':
                    H = self.hess_func(X_with_bias)
                    H_reg = H + self.numerical_regularization * np.eye(n_features_with_bias)
                
                delta_w = giai_he_phuong_trinh_tuyen_tinh(H_reg, gradient_w)
                step_size = np.linalg.norm(delta_w)
                self.step_sizes.append(step_size)
                
                self.weights = self.weights - delta_w
                
            except np.linalg.LinAlgError:
                print(f"Linear algebra error at iteration {lan_thu + 1}")
                break
            
            # Progress update
            if (lan_thu + 1) % 10 == 0 and should_check_converged:
                print(f"   VÃ²ng {lan_thu + 1}: Loss = {loss_value:.8f}, Gradient = {gradient_norm:.2e}")
        
        self.training_time = time.time() - start_time
        
        if not self.converged:
            print(f"â¹ï¸ Äáº¡t tá»‘i Ä‘a {self.so_lan_thu} vÃ²ng láº·p")
            self.final_iteration = self.so_lan_thu
        
        print(f"Thá»i gian training: {self.training_time:.4f}s")
        print(f"Loss cuá»‘i: {self.loss_history[-1]:.8f}")
        print(f"Bias cuá»‘i: {self.weights[-1]:.6f}")  # Bias lÃ  pháº§n tá»­ cuá»‘i cá»§a weights
        print(f"Sá»‘ weights (bao gá»“m bias): {len(self.weights)}")
        
        # Láº¥y káº¿t quáº£ tá»‘t nháº¥t thay vÃ¬ káº¿t quáº£ cuá»‘i cÃ¹ng
        best_results = self._get_best_results()
        best_weights = best_results['best_weights']
        best_loss = best_results['best_loss']
        best_gradient_norm = best_results['best_gradient_norm']
        best_iteration = best_results['best_iteration']
        
        print(f"ðŸ† Best results (gradient norm tháº¥p nháº¥t):")
        print(f"   Best iteration: {best_iteration}")
        print(f"   Best loss: {best_loss:.8f}")
        print(f"   Best gradient norm: {best_gradient_norm:.2e}")
        
        return {
            'weights': best_weights,  # Tráº£ vá» best weights thay vÃ¬ final
            'bias': best_weights[-1],  # Bias riÃªng Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch
            'loss_history': self.loss_history,
            'gradient_norms': self.gradient_norms,
            'weights_history': self.weights_history,
            'step_sizes': self.step_sizes,
            'training_time': self.training_time,
            'converged': self.converged,
            'final_iteration': self.final_iteration,
            'best_iteration': best_iteration,
            'best_loss': best_loss,
            'best_gradient_norm': best_gradient_norm,
            'final_loss': self.loss_history[-1],  # Äá»ƒ so sÃ¡nh
            'final_gradient_norm': self.gradient_norms[-1],  # Äá»ƒ so sÃ¡nh
            'condition_number': self.condition_number
        }
    
    def predict(self, X):
        """Dá»± Ä‘oÃ¡n vá»›i dá»¯ liá»‡u X 
        
        Tráº£ vá»:
            predictions: Dá»± Ä‘oÃ¡n trÃªn log scale
            
        LÆ°u Ã½:
            - Model Ä‘Æ°á»£c train trÃªn log-transformed targets
            - Dá»± Ä‘oÃ¡n tráº£ vá» á»Ÿ log scale
            - Bias Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o weights: y = Xw (vá»›i X Ä‘Ã£ cÃ³ cá»™t bias)
            - Sá»­ dá»¥ng np.expm1() Ä‘á»ƒ chuyá»ƒn vá» giÃ¡ gá»‘c khi cáº§n
        """
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        # ThÃªm cá»™t bias vÃ o X cho prediction
        X_with_bias = add_bias_column(X)
        return du_doan(X_with_bias, self.weights, None)
    
    def evaluate(self, X_test, y_test):
        """ÄÃ¡nh giÃ¡ model trÃªn test set"""
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        print(f"\nðŸ“‹ ÄÃ¡nh giÃ¡ model...")
        # Sá»­ dá»¥ng bias tá»« weights (pháº§n tá»­ cuá»‘i) Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i hÃ m cÅ©
        bias_value = self.weights[-1]
        weights_without_bias = self.weights[:-1]
        metrics = danh_gia_mo_hinh(weights_without_bias, X_test, y_test, bias_value)
        in_ket_qua_danh_gia(metrics, self.training_time, 
                           f"Newton Method - {self.ham_loss.upper()}")
        return metrics
    
    def save_results(self, ten_file, base_dir="data/03_algorithms/newton_method"):
        """
        LÆ°u káº¿t quáº£ model vÃ o file
        
        Parameters:
        - ten_file: TÃªn file/folder Ä‘á»ƒ lÆ°u káº¿t quáº£
        - base_dir: ThÆ° má»¥c gá»‘c Ä‘á»ƒ lÆ°u
        """
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        # Setup results directory
        results_dir = Path(base_dir) / ten_file
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Láº¥y káº¿t quáº£ tá»‘t nháº¥t
        best_results = self._get_best_results()
        best_weights = best_results['best_weights']
        best_loss = best_results['best_loss']
        best_gradient_norm = best_results['best_gradient_norm']
        best_iteration = best_results['best_iteration']
        
        # Save comprehensive results.json
        print(f"   LÆ°u káº¿t quáº£ vÃ o {results_dir}/results.json")
        results_data = {
            "algorithm": f"Newton Method - {self.ham_loss.upper()}",
            "loss_function": self.ham_loss.upper(),
            "parameters": {
                "regularization": self.regularization,
                "numerical_regularization": self.numerical_regularization,
                "max_iterations": self.so_lan_thu,
                "tolerance": self.diem_dung
            },
            "training_results": {
                "training_time": self.training_time,
                "converged": self.converged,
                "final_iteration": self.final_iteration,
                "total_iterations": self.so_lan_thu,
                "final_loss": float(self.loss_history[-1]),
                "final_gradient_norm": float(self.gradient_norms[-1]),
                # ThÃªm thÃ´ng tin best results
                "best_iteration": best_iteration,
                "best_loss": float(best_loss),
                "best_gradient_norm": float(best_gradient_norm),
                "improvement_from_final": {
                    "loss_improvement": float(self.loss_history[-1] - best_loss),
                    "gradient_improvement": float(self.gradient_norms[-1] - best_gradient_norm),
                    "iterations_earlier": self.final_iteration - best_iteration
                }
            },
            "weights_analysis": {
                "n_features": len(best_weights) - 1,  # KhÃ´ng tÃ­nh bias
                "n_weights_total": len(best_weights),  # TÃ­nh cáº£ bias
                "bias_value": float(best_weights[-1]),
                "weights_without_bias": best_weights[:-1].tolist(),
                "complete_weight_vector": best_weights.tolist(),
                "weights_stats": {
                    "min": float(np.min(best_weights[:-1])),  # Stats chá»‰ cá»§a weights, khÃ´ng tÃ­nh bias
                    "max": float(np.max(best_weights[:-1])),
                    "mean": float(np.mean(best_weights[:-1])),
                    "std": float(np.std(best_weights[:-1]))
                }
            },
            "convergence_analysis": {
                "iterations_to_converge": self.final_iteration,
                "best_iteration_found": best_iteration,
                "final_cost_change": float(self.loss_history[-1] - self.loss_history[-2]) if len(self.loss_history) > 1 else 0.0,
                "convergence_rate": "quadratic",  # Newton Method cÃ³ quadratic convergence
                "loss_reduction_ratio": float(self.loss_history[0] / best_loss) if len(self.loss_history) > 0 else 1.0,
                "convergence_quality": "superlinear_to_quadratic"
            },
            "numerical_analysis": {
                "hessian_condition_number": float(self.condition_number),
                "average_step_size": float(np.mean(self.step_sizes)) if self.step_sizes else 0,
                "max_step_size": float(np.max(self.step_sizes)) if self.step_sizes else 0,
                "min_step_size": float(np.min(self.step_sizes)) if self.step_sizes else 0,
                "step_size_stability": "Newton_steps",
                "regularization_effect": "Applied for numerical stability"
            },
            "algorithm_specific": {
                "method_type": "pure_newton",
                "second_order_method": True,
                "hessian_computation": "exact",
                "line_search_used": False,
                "damping_applied": "numerical_regularization_only",
                "fast_convergence": self.final_iteration <= 20,
                "returns_best_result": True  # ÄÃ¡nh dáº¥u ráº±ng tráº£ vá» best result
            }
        }
        
        with open(results_dir / "results.json", 'w') as f:
            json.dump(results_data, f, indent=2)
        
        # Save training history
        print(f"   LÆ°u lá»‹ch sá»­ training vÃ o {results_dir}/training_history.csv")
        max_len = len(self.loss_history)
        training_df = pd.DataFrame({
            'iteration': range(0, len(self.loss_history)*self.convergence_check_freq, self.convergence_check_freq),
            'loss': self.loss_history,
            'gradient_norm': self.gradient_norms,
            'step_size': self.step_sizes + [np.nan] * (max_len - len(self.step_sizes))
        })
        training_df.to_csv(results_dir / "training_history.csv", index=False)
        
        print(f"\nâœ… Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {results_dir.absolute()}")
        print(f"ðŸ† Sá»­ dá»¥ng best results tá»« iteration {best_iteration} (gradient norm: {best_gradient_norm:.2e})")
        return results_dir
    
    def plot_results(self, X_test, y_test, ten_file, base_dir="data/03_algorithms/newton_method"):
        """
        Táº¡o cÃ¡c biá»ƒu Ä‘á»“ visualization
        
        Parameters:
        - X_test, y_test: Dá»¯ liá»‡u test Ä‘á»ƒ váº½ predictions
        - ten_file: TÃªn file/folder Ä‘á»ƒ lÆ°u biá»ƒu Ä‘á»“
        - base_dir: ThÆ° má»¥c gá»‘c
        """
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        results_dir = Path(base_dir) / ten_file
        results_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nðŸ“Š Táº¡o biá»ƒu Ä‘á»“...")
        
        # 1. Convergence curves
        print("   - Váº½ Ä‘Æ°á»ng há»™i tá»¥")
        ve_duong_hoi_tu(self.loss_history, self.gradient_norms, 
                        title=f"Newton Method {self.ham_loss.upper()} - Convergence Analysis",
                        save_path=str(results_dir / "convergence_analysis.png"))
        
        # 2. Predictions vs Actual
        print("   - So sÃ¡nh dá»± Ä‘oÃ¡n vs thá»±c táº¿")
        y_pred_test = self.predict(X_test)
        ve_du_doan_vs_thuc_te(y_test, y_pred_test, 
                             title=f"Newton Method {self.ham_loss.upper()} - Predictions vs Actual",
                             save_path=str(results_dir / "predictions_vs_actual.png"))
        
        # 3. Optimization trajectory (Ä‘Æ°á»ng Ä‘á»“ng má»©c) - há»— trá»£ táº¥t cáº£ loss types
        print("   - Váº½ Ä‘Æ°á»ng Ä‘á»“ng má»©c optimization")
        
        # Chuáº©n bá»‹ X_test vá»›i bias cho visualization
        X_test_with_bias = add_bias_column(X_test)
        
        ve_duong_dong_muc_optimization(
            loss_function=self.loss_func,
            weights_history=self.weights_history,  # Pass full history
            X=X_test_with_bias, y=y_test,
            title=f"Newton Method {self.ham_loss.upper()} - Optimization Path",
            save_path=str(results_dir / "optimization_trajectory.png"),
            original_iterations=self.final_iteration,
            convergence_check_freq=self.convergence_check_freq,
            max_trajectory_points=None  # Newton usually has few iterations, show all
        )
        
