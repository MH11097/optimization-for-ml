{
  "algorithm": "Subgradient Descent",
  "parameters": {
    "lambda_penalty": 0.1,
    "max_iterations": 750,
    "tolerance": 1e-08
  },
  "training_time": 22.61434268951416,
  "convergence": {
    "converged": false,
    "iterations": 734,
    "final_loss": 1.0843799171398922,
    "final_gradient_norm": 0.6898660856448797
  }
}