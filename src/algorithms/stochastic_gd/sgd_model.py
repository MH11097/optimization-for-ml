#!/usr/bin/env python3
"""
SGDModel - Class cho Stochastic Gradient Descent
Há»— trá»£ cÃ¡c loss functions: MSE (Mean Squared Error)
"""

import pandas as pd
import numpy as np
from pathlib import Path
import time
import sys
import os
import json
import pickle

# Add the src directory to path Ä‘á»ƒ import utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from utils.optimization_utils import (
    du_doan, danh_gia_mo_hinh, in_ket_qua_danh_gia, kiem_tra_hoi_tu,
    tinh_gia_tri_ham_loss, tinh_gradient_ham_loss, tinh_hessian_ham_loss,
    add_bias_column
)
from utils.visualization_utils import (
    ve_duong_hoi_tu, ve_du_doan_vs_thuc_te, ve_duong_dong_muc_optimization
)


class SGDModel:
    """
    Stochastic Gradient Descent Model
    
    Parameters:
    - learning_rate: Tá»· lá»‡ há»c (step size)
    - so_epochs: Sá»‘ epochs (sá»‘ láº§n duyá»‡t qua toÃ n bá»™ dataset)
    - random_state: Random seed Ä‘á»ƒ tÃ¡i táº¡o káº¿t quáº£
    - batch_size: KÃ­ch thÆ°á»›c batch (1 cho pure SGD, >1 cho mini-batch)
    - ham_loss: Loss function (hiá»‡n táº¡i chá»‰ há»— trá»£ 'mse')
    - convergence_check_freq: Táº§n suáº¥t kiá»ƒm tra há»™i tá»¥ (má»—i N epochs)
    """
    
    def __init__(self, learning_rate=0.01, so_epochs=100, random_state=42, 
                 batch_size=1, ham_loss='ols', tolerance=1e-6, regularization=0.01, convergence_check_freq=10):
        self.learning_rate = learning_rate
        self.so_epochs = so_epochs
        self.random_state = random_state
        self.batch_size = batch_size
        self.ham_loss = ham_loss.lower()
        self.tolerance = tolerance
        self.regularization = regularization
        self.convergence_check_freq = convergence_check_freq  # Má»—i N epochs
        
        # Sá»­ dá»¥ng unified functions vá»›i format má»›i (bias trong X)
        self.loss_func = lambda X, y, w: tinh_gia_tri_ham_loss(self.ham_loss, X, y, w, None, self.regularization)
        self.grad_func = lambda X, y, w: tinh_gradient_ham_loss(self.ham_loss, X, y, w, None, self.regularization)
        
        # Khá»Ÿi táº¡o cÃ¡c thuá»™c tÃ­nh lÆ°u káº¿t quáº£
        self.weights = None  # BÃ¢y giá» bao gá»“m bias á»Ÿ cuá»‘i
        self.loss_history = []
        self.gradient_norms = []
        self.weights_history = []
        self.training_time = 0
        self.converged = False
        self.final_cost = None
        self.final_epoch = 0
        
    def _tinh_chi_phi(self, X, y, weights):
        """TÃ­nh chi phÃ­ sá»­ dá»¥ng unified function"""
        return self.loss_func(X, y, weights)
    
    def _tinh_gradient_sample(self, xi, yi, weights):
        """TÃ­nh gradient cho má»™t sample sá»­ dá»¥ng unified function"""
        # Reshape Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i unified function
        X_sample = xi.reshape(1, -1)  # (1, n_features) - Ä‘Ã£ bao gá»“m bias
        y_sample = np.array([yi])     # (1,)
        
        # Sá»­ dá»¥ng unified function vÃ  láº¥y gradient weights
        gradient_w, _ = self.grad_func(X_sample, y_sample, weights) 
        return gradient_w
    
    def fit(self, X, y):
        """
        Huáº¥n luyá»‡n model vá»›i dá»¯ liá»‡u X, y
        
        Returns:
        - dict: Káº¿t quáº£ training bao gá»“m weights, loss_history, etc.
        """
        print(f"ðŸš€ Training Stochastic Gradient Descent - {self.ham_loss.upper()}")
        print(f"   Learning rate: {self.learning_rate}")
        print(f"   Epochs: {self.so_epochs}, Batch size: {self.batch_size}")
        print(f"   Random state: {self.random_state}")
        
        np.random.seed(self.random_state)
        
        # ThÃªm cá»™t bias vÃ o X
        X_with_bias = add_bias_column(X)
        print(f"   Original features: {X.shape[1]}, With bias: {X_with_bias.shape[1]}")
        
        n_samples, n_features_with_bias = X_with_bias.shape
        self.weights = np.random.normal(0, 0.01, n_features_with_bias)
        
        # Reset histories
        self.loss_history = []
        self.gradient_norms = []
        self.weights_history = []
        
        # Add convergence tracking
        self.converged = False
        self.final_epoch = 0
        
        start_time = time.time()
        
        for epoch in range(self.so_epochs):
            # Shuffle data for each epoch
            indices = np.random.permutation(n_samples)
            X_shuffled = X_with_bias[indices]
            y_shuffled = y[indices]
            
            epoch_gradients = []
            
            # Process in batches
            for i in range(0, n_samples, self.batch_size):
                end_idx = min(i + self.batch_size, n_samples)
                X_batch = X_shuffled[i:end_idx]
                y_batch = y_shuffled[i:end_idx]
                
                # TÃ­nh gradient cho batch
                batch_gradient = np.zeros(n_features_with_bias)
                
                for j in range(len(X_batch)):
                    xi = X_batch[j]
                    yi = y_batch[j]
                    sample_gradient = self._tinh_gradient_sample(xi, yi, self.weights)
                    batch_gradient += sample_gradient
                
                # Average gradient over batch
                batch_gradient /= len(X_batch)
                epoch_gradients.append(batch_gradient)
                
                # Update weights
                self.weights -= self.learning_rate * batch_gradient
            
            # Chá»‰ tÃ­nh cost vÃ  lÆ°u history khi cáº§n thiáº¿t  
            should_log = (
                (epoch + 1) % self.convergence_check_freq == 0 or
                epoch == self.so_epochs - 1 or
                (epoch + 1) % 20 == 0  # Progress logging
            )
            
            if should_log:
                # Chá»‰ tÃ­nh cost khi cáº§n (expensive operation)
                epoch_cost = self._tinh_chi_phi(X_with_bias, y, self.weights)
                epoch_gradient_avg = np.mean(epoch_gradients, axis=0)
                gradient_norm = np.linalg.norm(epoch_gradient_avg)
                
                # LÆ°u vÃ o history
                self.loss_history.append(epoch_cost)
                self.gradient_norms.append(gradient_norm)
                self.weights_history.append(self.weights.copy())
            
            # Check convergence vá»›i táº§n suáº¥t Ä‘á»‹nh sáºµn hoáº·c á»Ÿ epoch cuá»‘i
            if (epoch + 1) % self.convergence_check_freq == 0 or epoch == self.so_epochs - 1:
                # Äáº£m báº£o cÃ³ gradient_norm vÃ  epoch_cost cho convergence check
                if not should_log:
                    epoch_cost = self._tinh_chi_phi(X_with_bias, y, self.weights)
                    epoch_gradient_avg = np.mean(epoch_gradients, axis=0)
                    gradient_norm = np.linalg.norm(epoch_gradient_avg)
                    
                cost_change = 0.0 if len(self.loss_history) == 0 else (self.loss_history[-1] - epoch_cost) if len(self.loss_history) == 1 else (self.loss_history[-2] - self.loss_history[-1])
                converged, reason = kiem_tra_hoi_tu(
                    gradient_norm=gradient_norm,
                    cost_change=cost_change,
                    iteration=epoch,
                    tolerance=self.tolerance,
                    max_iterations=self.so_epochs
                )
                
                if converged:
                    print(f"âœ… SGD stopped: {reason}")
                    self.converged = True
                    self.final_epoch = epoch + 1
                    break
            
            # Progress update - chá»‰ print khi Ä‘Ã£ cÃ³ data
            if (epoch + 1) % 20 == 0 and should_log:
                print(f"   Epoch {epoch + 1}: Cost = {epoch_cost:.6f}, Gradient = {gradient_norm:.6f}")
        
        self.training_time = time.time() - start_time
        self.final_cost = self.loss_history[-1]
        
        if not self.converged:
            print(f"â¹ï¸ Äáº¡t tá»‘i Ä‘a {self.so_epochs} epochs")
            self.final_epoch = self.so_epochs
            
        print(f"Thá»i gian training: {self.training_time:.2f}s")
        print(f"Loss cuá»‘i: {self.final_cost:.6f}")
        print(f"Bias cuá»‘i: {self.weights[-1]:.6f}")  # Bias lÃ  pháº§n tá»­ cuá»‘i cá»§a weights
        print(f"Sá»‘ weights (bao gá»“m bias): {len(self.weights)}")
        
        return {
            'weights': self.weights,  # Bao gá»“m bias á»Ÿ cuá»‘i
            'bias': self.weights[-1],  # Bias riÃªng Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch
            'loss_history': self.loss_history,
            'gradient_norms': self.gradient_norms,
            'weights_history': self.weights_history,
            'training_time': self.training_time,
            'final_cost': self.final_cost,
            'converged': self.converged,
            'final_epoch': self.final_epoch
        }
    
    def predict(self, X):
        """Dá»± Ä‘oÃ¡n vá»›i dá»¯ liá»‡u X 
        
        Tráº£ vá»:
            predictions: Dá»± Ä‘oÃ¡n trÃªn log scale
            
        LÆ°u Ã½:
            - Model Ä‘Æ°á»£c train trÃªn log-transformed targets
            - Dá»± Ä‘oÃ¡n tráº£ vá» á»Ÿ log scale
            - Bias Ä‘Ã£ Ä‘Æ°á»£c tÃ­ch há»£p vÃ o weights: y = Xw (vá»›i X Ä‘Ã£ cÃ³ cá»™t bias)
            - Sá»­ dá»¥ng np.expm1() Ä‘á»ƒ chuyá»ƒn vá» giÃ¡ gá»‘c khi cáº§n
        """
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        # ThÃªm cá»™t bias vÃ o X cho prediction
        X_with_bias = add_bias_column(X)
        return du_doan(X_with_bias, self.weights, None)
    
    def evaluate(self, X_test, y_test):
        """ÄÃ¡nh giÃ¡ model trÃªn test set"""
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        print(f"\nðŸ“‹ ÄÃ¡nh giÃ¡ model...")
        # Sá»­ dá»¥ng bias tá»« weights (pháº§n tá»­ cuá»‘i) Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i hÃ m cÅ©
        bias_value = self.weights[-1]
        weights_without_bias = self.weights[:-1]
        metrics = danh_gia_mo_hinh(weights_without_bias, X_test, y_test, bias_value)
        in_ket_qua_danh_gia(metrics, self.training_time, 
                           f"Stochastic Gradient Descent - {self.ham_loss.upper()}")
        return metrics
    
    def save_results(self, ten_file, base_dir="data/03_algorithms/stochastic_gd"):
        """
        LÆ°u káº¿t quáº£ model vÃ o file
        
        Parameters:
        - ten_file: TÃªn file/folder Ä‘á»ƒ lÆ°u káº¿t quáº£
        - base_dir: ThÆ° má»¥c gá»‘c Ä‘á»ƒ lÆ°u
        """
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        # Setup results directory
        results_dir = Path(base_dir) / ten_file
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # Save model weights vÃ  training info
        print(f"   LÆ°u model vÃ o {results_dir}/model.pkl")
        model_data = {
            'algorithm': f'Stochastic Gradient Descent',
            'weights': self.weights.tolist(),
            'loss_history': self.loss_history,
            'training_time': self.training_time,
            'epochs': self.so_epochs,
            'final_cost': self.final_cost,
            'parameters': {
                'learning_rate': self.learning_rate,
                'batch_size': self.batch_size,
                'random_state': self.random_state
            }
        }
        
        with open(results_dir / "model.pkl", "wb") as f:
            pickle.dump(model_data, f)
        
        # Save comprehensive results.json
        print(f"   LÆ°u káº¿t quáº£ vÃ o {results_dir}/results.json")
        results_data = {
            "algorithm": f"Stochastic Gradient Descent - {self.ham_loss.upper()}",
            "loss_function": self.ham_loss.upper(),
            "parameters": {
                "learning_rate": self.learning_rate,
                "epochs": self.so_epochs,
                "batch_size": self.batch_size,
                "random_state": self.random_state,
                "tolerance": self.tolerance
            },
            "training_results": {
                "training_time": self.training_time,
                "converged": self.converged,
                "final_epoch": self.final_epoch,
                "total_epochs": self.so_epochs,
                "final_cost": float(self.final_cost),
                "final_gradient_norm": float(self.gradient_norms[-1]) if self.gradient_norms else 0
            },
            "weights_analysis": {
                "n_features": len(self.weights) - 1,  # KhÃ´ng tÃ­nh bias
                "n_weights_total": len(self.weights),  # TÃ­nh cáº£ bias
                "bias_value": float(self.weights[-1]),
                "weights_without_bias": self.weights[:-1].tolist(),
                "complete_weight_vector": self.weights.tolist(),
                "weights_stats": {
                    "min": float(np.min(self.weights[:-1])),  # Stats chá»‰ cá»§a weights, khÃ´ng tÃ­nh bias
                    "max": float(np.max(self.weights[:-1])),
                    "mean": float(np.mean(self.weights[:-1])),
                    "std": float(np.std(self.weights[:-1]))
                }
            },
            "convergence_analysis": {
                "epochs_to_converge": self.final_epoch,
                "final_cost_change": float(self.loss_history[-1] - self.loss_history[-2]) if len(self.loss_history) > 1 else 0.0,
                "convergence_rate": "sublinear",  # SGD cÃ³ sublinear convergence
                "cost_reduction_ratio": float(self.loss_history[0] / self.loss_history[-1]) if len(self.loss_history) > 0 else 1.0
            },
            "algorithm_specific": {
                "method_type": "stochastic_gradient_descent",
                "batch_processing": True,
                "batch_size": self.batch_size,
                "epoch_based_training": True,
                "data_shuffling": True,
                "noisy_gradients": "inherent_in_SGD",
                "convergence_type": "probabilistic"
            }
        }
        
        with open(results_dir / "results.json", 'w') as f:
            json.dump(results_data, f, indent=2)
        
        # Save training history
        print(f"   LÆ°u lá»‹ch sá»­ training vÃ o {results_dir}/training_history.csv")
        training_df = pd.DataFrame({
            'epoch': range(0, len(self.loss_history)*self.convergence_check_freq, self.convergence_check_freq),
            'cost': self.loss_history
        })
        training_df.to_csv(results_dir / "training_history.csv", index=False)
        
        print(f"\n Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o: {results_dir.absolute()}")
        return results_dir
    
    def plot_results(self, X_test, y_test, ten_file, base_dir="data/03_algorithms/stochastic_gd"):
        """
        Táº¡o cÃ¡c biá»ƒu Ä‘á»“ visualization
        
        Parameters:
        - X_test, y_test: Dá»¯ liá»‡u test Ä‘á»ƒ váº½ predictions
        - ten_file: TÃªn file/folder Ä‘á»ƒ lÆ°u biá»ƒu Ä‘á»“
        - base_dir: ThÆ° má»¥c gá»‘c
        """
        if self.weights is None:
            raise ValueError("Model chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n. HÃ£y gá»i fit() trÆ°á»›c.")
        
        results_dir = Path(base_dir) / ten_file
        results_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nðŸ“Š Táº¡o biá»ƒu Ä‘á»“...")
        
        # 1. Convergence curves
        print("   - Váº½ Ä‘Æ°á»ng há»™i tá»¥")
        ve_duong_hoi_tu(self.loss_history, self.gradient_norms, 
                        title=f"Newton Method {self.ham_loss.upper()} - Convergence Analysis",
                        save_path=str(results_dir / "convergence_analysis.png"))
        
        # 2. Predictions vs Actual
        print("   - So sÃ¡nh dá»± Ä‘oÃ¡n vs thá»±c táº¿")
        y_pred_test = self.predict(X_test)
        ve_du_doan_vs_thuc_te(y_test, y_pred_test, 
                             title=f"Newton Method {self.ham_loss.upper()} - Predictions vs Actual",
                             save_path=str(results_dir / "predictions_vs_actual.png"))
        
        # 3. Optimization trajectory (Ä‘Æ°á»ng Ä‘á»“ng má»©c) - há»— trá»£ táº¥t cáº£ loss types
        print("   - Váº½ Ä‘Æ°á»ng Ä‘á»“ng má»©c optimization")
        if hasattr(self, 'weights_history') and len(self.weights_history) > 0:
            # Sample weights history for performance (every 10th point)
            step = max(1, len(self.weights_history) // 100)
            sampled_weights = np.array(self.weights_history[::step])
            
            # Chuáº©n bá»‹ X_test vá»›i bias cho visualization
            X_test_with_bias = add_bias_column(X_test)
            
            ve_duong_dong_muc_optimization(
                loss_function=self.loss_func,
                weights_history=sampled_weights,
                X=X_test_with_bias, y=y_test,
                title=f"Stochastic GD {self.ham_loss.upper()} - Optimization Path",
                save_path=str(results_dir / "optimization_trajectory.png")
            )
        else:
            print("     KhÃ´ng cÃ³ weights history Ä‘á»ƒ váº½ contour plot")
        
