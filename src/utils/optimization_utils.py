"""
Tiá»‡n Ã­ch Tá»‘i Æ°u hÃ³a - CÃ¡c hÃ m cáº§n thiáº¿t cho Newton Method

=== Má»¤C ÄÃCH: Tá»I Æ¯U HÃ“A ===

Bao gá»“m táº¥t cáº£ cÃ¡c hÃ m cáº§n thiáº¿t cho:
1. TÃ­nh toÃ¡n Gradient vÃ  Hessian
2. Giáº£i há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh  
3. Kiá»ƒm tra há»™i tá»¥ vÃ  line search
4. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh (MSE, MAE, RÂ²)
5. Dá»± Ä‘oÃ¡n vÃ  tÃ­nh toÃ¡n loss

Code Ä‘Æ¡n giáº£n, dá»… hiá»ƒu, dá»… sá»­ dá»¥ng.
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, Tuple, Optional, Dict, List


# ==============================================================================
# 1. TÃNH TOÃN GRADIENT VÃ€ HESSIAN
# ==============================================================================

def tinh_gradient_hoi_quy_tuyen_tinh(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray, 
                                   he_so_tu_do: float, dieu_chinh: float = 0.0) -> Tuple[np.ndarray, float]:
    """
    TÃ­nh vector gradient cho há»“i quy tuyáº¿n tÃ­nh vá»›i Ä‘iá»u chá»‰nh
    
    HÃ m má»¥c tiÃªu: f(w,b) = (1/2n) * ||Xw + b - y||Â² + (Î»/2) * ||w||Â²
    
    Gradient:
    - âˆ‚f/âˆ‚w = (1/n) * X^T(Xw + b - y) + Î» * w  
    - âˆ‚f/âˆ‚b = (1/n) * Î£(Xw + b - y)
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector má»¥c tiÃªu (n_samples,)
        trong_so: trá»ng sá»‘ hiá»‡n táº¡i (n_features,)
        he_so_tu_do: há»‡ sá»‘ tá»± do hiá»‡n táº¡i (scalar)
        dieu_chinh: há»‡ sá»‘ Ä‘iá»u chá»‰nh Î» (máº·c Ä‘á»‹nh 0.0)
    
    Tráº£ vá»:
        gradient_w: gradient theo trá»ng sá»‘ (n_features,)
        gradient_b: gradient theo há»‡ sá»‘ tá»± do (scalar)
    """
    so_mau = X.shape[0]
    
    # Dá»± Ä‘oÃ¡n hiá»‡n táº¡i: Å· = Xw + b
    du_doan = X @ trong_so + he_so_tu_do
    
    # Sai sá»‘: e = Å· - y
    sai_so = du_doan - y
    
    # Gradient theo trá»ng sá»‘: âˆ‚f/âˆ‚w = (1/n) * X^T * e + Î» * w
    gradient_w = (1/so_mau) * X.T @ sai_so + dieu_chinh * trong_so
    
    # Gradient theo há»‡ sá»‘ tá»± do: âˆ‚f/âˆ‚b = (1/n) * Î£(e)
    gradient_b = (1/so_mau) * np.sum(sai_so)
    
    return gradient_w, gradient_b


def tinh_ma_tran_hessian_hoi_quy_tuyen_tinh(X: np.ndarray, dieu_chinh: float = 0.0) -> np.ndarray:
    """
    TÃ­nh ma tráº­n Hessian cho há»“i quy tuyáº¿n tÃ­nh vá»›i Ä‘iá»u chá»‰nh
    
    Ma tráº­n Hessian: H = (1/n) * X^T * X + Î» * I
    
    ÄÃ¢y lÃ  ma tráº­n Ä‘áº¡o hÃ m báº­c 2 cá»§a hÃ m má»¥c tiÃªu.
    Äá»‘i vá»›i há»“i quy tuyáº¿n tÃ­nh, Hessian lÃ  háº±ng sá»‘ (khÃ´ng phá»¥ thuá»™c vÃ o w, b).
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        dieu_chinh: há»‡ sá»‘ Ä‘iá»u chá»‰nh Î» (máº·c Ä‘á»‹nh 0.0)
    
    Tráº£ vá»:
        H: ma tráº­n Hessian (n_features, n_features)
    
    LÆ°u Ã½: 
    - Ma tráº­n nÃ y lÃ  positive semi-definite khi Î» â‰¥ 0
    - Khi Î» > 0, ma tráº­n trá»Ÿ thÃ nh positive definite vÃ  kháº£ nghá»‹ch
    """
    so_mau, so_dac_trung = X.shape
    
    # TÃ­nh tÃ­ch X^T @ X (ma tráº­n Gram)
    XTX = X.T @ X
    
    # Chia cho sá»‘ máº«u vÃ  thÃªm Ä‘iá»u chá»‰nh
    H = (1/so_mau) * XTX + dieu_chinh * np.eye(so_dac_trung)
    
    return H


# ==============================================================================
# 2. GIáº¢I Há»† PHÆ¯Æ NG TRÃŒNH VÃ€ KIá»‚M TRA MA TRáº¬N
# ==============================================================================

def giai_he_phuong_trinh_tuyen_tinh(A: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    Giáº£i há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh Ax = b má»™t cÃ¡ch an toÃ n
    
    Sá»­ dá»¥ng LU decomposition vá»›i partial pivoting Ä‘á»ƒ á»•n Ä‘á»‹nh sá»‘ há»c.
    
    Tham sá»‘:
        A: ma tráº­n há»‡ sá»‘ (n x n)
        b: vector váº¿ pháº£i (n,)
    
    Tráº£ vá»:
        x: nghiá»‡m cá»§a há»‡ phÆ°Æ¡ng trÃ¬nh (n,)
    
    LÆ°u Ã½: Tá»± Ä‘á»™ng kiá»ƒm tra Ä‘iá»u kiá»‡n vÃ  thÃªm regularization náº¿u cáº§n
    """
    try:
        # Thá»­ giáº£i trá»±c tiáº¿p
        return np.linalg.solve(A, b)
    except np.linalg.LinAlgError:
        # Náº¿u ma tráº­n singular, thÃªm regularization nhá»
        regularization = 1e-10
        A_reg = A + regularization * np.eye(A.shape[0])
        return np.linalg.solve(A_reg, b)


def kiem_tra_positive_definite(matrix: np.ndarray) -> bool:
    """
    Kiá»ƒm tra ma tráº­n cÃ³ pháº£i positive definite khÃ´ng
    
    Ma tráº­n positive definite khi táº¥t cáº£ eigenvalues > 0.
    Äiá»u nÃ y quan trá»ng cho Newton Method.
    
    Tham sá»‘:
        matrix: ma tráº­n cáº§n kiá»ƒm tra (n x n)
    
    Tráº£ vá»:
        bool: True náº¿u positive definite, False náº¿u khÃ´ng
    """
    try:
        # Thá»­ Cholesky decomposition
        np.linalg.cholesky(matrix)
        return True
    except np.linalg.LinAlgError:
        return False


def tinh_condition_number(matrix: np.ndarray) -> float:
    """
    TÃ­nh condition number cá»§a ma tráº­n
    
    Condition number cho biáº¿t má»©c Ä‘á»™ ill-conditioned cá»§a ma tráº­n.
    - Gáº§n 1: ma tráº­n well-conditioned
    - Ráº¥t lá»›n: ma tráº­n ill-conditioned
    
    Tham sá»‘:
        matrix: ma tráº­n cáº§n tÃ­nh (n x n)
    
    Tráº£ vá»:
        float: condition number
    """
    return np.linalg.cond(matrix)


# ==============================================================================
# 3. ÄÃNH GIÃ MÃ” HÃŒNH VÃ€ Dá»° ÄOÃN
# ==============================================================================

def du_doan(X: np.ndarray, w: np.ndarray, he_so_tu_do: float) -> np.ndarray:
    """
    Thá»±c hiá»‡n dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh tuyáº¿n tÃ­nh
    
    CÃ´ng thá»©c: Å· = Xw + he_so_tu_do
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        w: trá»ng sá»‘ Ä‘Ã£ há»c (n_features,)
        he_so_tu_do: há»‡ sá»‘ tá»± do Ä‘Ã£ há»c (scalar)
    
    Tráº£ vá»:
        predictions: dá»± Ä‘oÃ¡n (n_samples,)
    """
    return X @ w + he_so_tu_do


def tinh_mse(y_that: np.ndarray, y_du_doan: np.ndarray) -> float:
    """
    TÃ­nh Mean Squared Error (Sai sá»‘ bÃ¬nh phÆ°Æ¡ng trung bÃ¬nh)
    
    MSE = (1/n) * Î£(y_tháº­t - y_dá»±_Ä‘oÃ¡n)Â²
    
    Tham sá»‘:
        y_that: giÃ¡ trá»‹ tháº­t (n_samples,)
        y_du_doan: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n (n_samples,)
    
    Tráº£ vá»:
        float: MSE
    """
    return np.mean((y_that - y_du_doan) ** 2)


def tinh_mae(y_that: np.ndarray, y_du_doan: np.ndarray) -> float:
    """
    TÃ­nh Mean Absolute Error (Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh)
    
    MAE = (1/n) * Î£|y_tháº­t - y_dá»±_Ä‘oÃ¡n|
    
    Tham sá»‘:
        y_that: giÃ¡ trá»‹ tháº­t (n_samples,)
        y_du_doan: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n (n_samples,)
    
    Tráº£ vá»:
        float: MAE
    """
    return np.mean(np.abs(y_that - y_du_doan))


def tinh_r2_score(y_that: np.ndarray, y_du_doan: np.ndarray) -> float:
    """
    TÃ­nh RÂ² score (Coefficient of determination)
    
    RÂ² = 1 - (SS_res / SS_tot)
    Trong Ä‘Ã³:
    - SS_res = Î£(y_tháº­t - y_dá»±_Ä‘oÃ¡n)Â² (residual sum of squares)
    - SS_tot = Î£(y_tháº­t - È³)Â² (total sum of squares)
    
    Tham sá»‘:
        y_that: giÃ¡ trá»‹ tháº­t (n_samples,)
        y_du_doan: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n (n_samples,)
    
    Tráº£ vá»:
        float: RÂ² score (1.0 = perfect, 0.0 = no better than mean)
    """
    ss_res = np.sum((y_that - y_du_doan) ** 2)
    ss_tot = np.sum((y_that - np.mean(y_that)) ** 2)
    
    if ss_tot == 0:
        return 1.0 if ss_res == 0 else 0.0
    
    return 1 - (ss_res / ss_tot)


# ==============================================================================
# 4. KIá»‚M TRA Há»˜I Tá»¤ VÃ€ LINE SEARCH
# ==============================================================================

def kiem_tra_hoi_tu(gradient_norm: float, cost_change: float, iteration: int,
                   tolerance: float = 1e-6, max_iterations: int = 100) -> Tuple[bool, str]:
    """
    Kiá»ƒm tra Ä‘iá»u kiá»‡n há»™i tá»¥ cho thuáº­t toÃ¡n optimization
    
    Tham sá»‘:
        gradient_norm: chuáº©n cá»§a gradient hiá»‡n táº¡i
        cost_change: thay Ä‘á»•i cost tá»« iteration trÆ°á»›c
        iteration: sá»‘ iteration hiá»‡n táº¡i
        tolerance: ngÆ°á»¡ng há»™i tá»¥
        max_iterations: sá»‘ iteration tá»‘i Ä‘a
    
    Tráº£ vá»:
        converged: cÃ³ há»™i tá»¥ hay khÃ´ng
        reason: lÃ½ do dá»«ng
    """
    # Há»™i tá»¥ theo gradient norm
    if gradient_norm < tolerance:
        return True, f"Há»™i tá»¥ theo chuáº©n gradient: {gradient_norm:.2e} < {tolerance:.2e}"
    
    # Há»™i tá»¥ theo thay Ä‘á»•i cost
    if iteration > 0 and abs(cost_change) < tolerance:
        return True, f"Há»™i tá»¥ theo thay Ä‘á»•i cost: {abs(cost_change):.2e} < {tolerance:.2e}"
    
    # Äáº¡t giá»›i háº¡n iteration
    if iteration >= max_iterations:
        return True, f"Äáº¡t giá»›i háº¡n iteration: {iteration}"
    
    return False, ""


def backtracking_line_search(cost_func: Callable, gradient: np.ndarray, 
                            current_point: np.ndarray, search_direction: np.ndarray,
                            current_cost: float, alpha_init: float = 1.0,
                            rho: float = 0.5, c1: float = 1e-4, max_iter: int = 50) -> float:
    """
    Backtracking line search Ä‘á»ƒ tÃ¬m learning rate tá»‘i Æ°u
    
    Sá»­ dá»¥ng Armijo condition Ä‘á»ƒ Ä‘áº£m báº£o sufficient decrease.
    
    Tham sá»‘:
        cost_func: hÃ m tÃ­nh cost
        gradient: gradient táº¡i Ä‘iá»ƒm hiá»‡n táº¡i
        current_point: Ä‘iá»ƒm hiá»‡n táº¡i
        search_direction: hÆ°á»›ng tÃ¬m kiáº¿m (thÆ°á»ng lÃ  -H^{-1} * gradient)
        current_cost: cost táº¡i Ä‘iá»ƒm hiá»‡n táº¡i
        alpha_init: learning rate ban Ä‘áº§u
        rho: há»‡ sá»‘ giáº£m learning rate
        c1: háº±ng sá»‘ cho Armijo condition
        max_iter: sá»‘ iteration tá»‘i Ä‘a
    
    Tráº£ vá»:
        alpha: learning rate tá»‘i Æ°u tÃ¬m Ä‘Æ°á»£c
    """
    alpha = alpha_init
    directional_derivative = gradient.T @ search_direction
    
    for i in range(max_iter):
        new_point = current_point + alpha * search_direction
        new_cost = cost_func(new_point)
        
        # Armijo condition: f(x + Î±*d) â‰¤ f(x) + c1*Î±*âˆ‡f(x)^T*d
        if new_cost <= current_cost + c1 * alpha * directional_derivative:
            return alpha
        
        alpha *= rho
    
    return alpha


# ==============================================================================
# 5. HÃ€M LOSS CHO CÃC BIáº¾N THá»‚
# ==============================================================================

def tinh_loss_ols(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray, he_so_tu_do: float = 0) -> float:
    """
    TÃ­nh loss cho OLS (Ordinary Least Squares)
    
    Loss = (1/2n) * ||Xw + b - y||Â²
    """
    predictions = du_doan(X, trong_so, he_so_tu_do)
    return 0.5 * tinh_mse(y, predictions)


def tinh_loss_ridge(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray, 
                   he_so_tu_do: float, dieu_chinh: float) -> float:
    """
    TÃ­nh loss cho Ridge Regression
    
    Loss = (1/2n) * ||Xw + b - y||Â² + (Î»/2) * ||w||Â²
    """
    mse_loss = tinh_loss_ols(X, y, trong_so, he_so_tu_do)
    l2_penalty = 0.5 * dieu_chinh * np.sum(trong_so ** 2)
    return mse_loss + l2_penalty


def tinh_loss_lasso_smooth(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray,
                          he_so_tu_do: float, dieu_chinh: float, epsilon: float = 1e-8) -> float:
    """
    TÃ­nh loss cho Lasso Regression (smooth approximation)
    
    Loss = (1/2n) * ||Xw + b - y||Â² + Î» * Î£âˆš(wÂ²+ Îµ)
    """
    mse_loss = tinh_loss_ols(X, y, trong_so, he_so_tu_do)
    smooth_l1 = dieu_chinh * np.sum(np.sqrt(trong_so ** 2 + epsilon))
    return mse_loss + smooth_l1


# ==============================================================================
# 5.1. CÃC HÃ€M CHá»ˆ NHáº¬N WEIGHTS (KHÃ”NG CÃ“ BIAS) - CHO GRADIENT DESCENT
# ==============================================================================

def tinh_gia_tri_ham_OLS(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:
    """
    TÃ­nh giÃ¡ trá»‹ hÃ m OLS táº¡i Ä‘iá»ƒm w (khÃ´ng cÃ³ bias)
    
    HÃ m OLS: L(w) = (1/2n) * ||Xw - y||Â²
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
    
    Tráº£ vá»:
        float: giÃ¡ trá»‹ hÃ m OLS táº¡i w
    """
    n_samples = X.shape[0]
    predictions = X @ w
    residuals = predictions - y
    ols_value = (1 / (2 * n_samples)) * np.sum(residuals ** 2)
    return ols_value


def tinh_gradient_OLS(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:
    """
    TÃ­nh gradient cá»§a hÃ m OLS theo weights (khÃ´ng cÃ³ bias)
    
    âˆ‡L(w) = (1/n) * X^T(Xw - y)
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
    
    Tráº£ vá»:
        gradient: gradient vector (n_features,)
    """
    n_samples = X.shape[0]
    predictions = X @ w
    errors = predictions - y
    gradient = (1 / n_samples) * X.T @ errors
    return gradient


def tinh_hessian_OLS(X: np.ndarray) -> np.ndarray:
    """
    TÃ­nh ma tráº­n Hessian cá»§a hÃ m OLS (khÃ´ng cÃ³ bias)
    
    H = (1/n) * X^T * X
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
    
    Tráº£ vá»:
        hessian: ma tráº­n Hessian (n_features, n_features)
    """
    n_samples = X.shape[0]
    hessian = (1 / n_samples) * X.T @ X
    return hessian


def tinh_gia_tri_ham_Ridge(X: np.ndarray, y: np.ndarray, w: np.ndarray, lambda_reg: float) -> float:
    """
    TÃ­nh giÃ¡ trá»‹ hÃ m Ridge táº¡i Ä‘iá»ƒm w (khÃ´ng cÃ³ bias)
    
    HÃ m Ridge: L(w) = (1/2n) * ||Xw - y||Â² + (Î»/2) * ||w||Â²
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        lambda_reg: há»‡ sá»‘ regularization
    
    Tráº£ vá»:
        float: giÃ¡ trá»‹ hÃ m Ridge táº¡i w
    """
    ols_loss = tinh_gia_tri_ham_OLS(X, y, w)
    l2_penalty = 0.5 * lambda_reg * np.sum(w ** 2)
    return ols_loss + l2_penalty


def tinh_gradient_Ridge(X: np.ndarray, y: np.ndarray, w: np.ndarray, lambda_reg: float) -> np.ndarray:
    """
    TÃ­nh gradient cá»§a hÃ m Ridge theo weights (khÃ´ng cÃ³ bias)
    
    âˆ‡L(w) = (1/n) * X^T(Xw - y) + Î» * w
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        lambda_reg: há»‡ sá»‘ regularization
    
    Tráº£ vá»:
        gradient: gradient vector (n_features,)
    """
    ols_gradient = tinh_gradient_OLS(X, y, w)
    l2_gradient = lambda_reg * w
    return ols_gradient + l2_gradient


def tinh_hessian_Ridge(X: np.ndarray, lambda_reg: float) -> np.ndarray:
    """
    TÃ­nh ma tráº­n Hessian cá»§a hÃ m Ridge (khÃ´ng cÃ³ bias)
    
    H = (1/n) * X^T * X + Î» * I
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        lambda_reg: há»‡ sá»‘ regularization
    
    Tráº£ vá»:
        hessian: ma tráº­n Hessian (n_features, n_features)
    """
    ols_hessian = tinh_hessian_OLS(X)
    n_features = X.shape[1]
    l2_hessian = lambda_reg * np.eye(n_features)
    return ols_hessian + l2_hessian


def tinh_gia_tri_ham_Lasso_smooth(X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                                  lambda_reg: float, epsilon: float = 1e-8) -> float:
    """
    TÃ­nh giÃ¡ trá»‹ hÃ m Lasso vá»›i smooth approximation táº¡i Ä‘iá»ƒm w (khÃ´ng cÃ³ bias)
    
    HÃ m Lasso: L(w) = (1/2n) * ||Xw - y||Â² + Î» * Î£âˆš(wÂ²+ Îµ)
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        lambda_reg: há»‡ sá»‘ regularization
        epsilon: tham sá»‘ smoothing
    
    Tráº£ vá»:
        float: giÃ¡ trá»‹ hÃ m Lasso táº¡i w
    """
    ols_loss = tinh_gia_tri_ham_OLS(X, y, w)
    smooth_l1_penalty = lambda_reg * np.sum(np.sqrt(w ** 2 + epsilon))
    return ols_loss + smooth_l1_penalty


def tinh_gradient_Lasso_smooth(X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                              lambda_reg: float, epsilon: float = 1e-8) -> np.ndarray:
    """
    TÃ­nh gradient cá»§a hÃ m Lasso vá»›i smooth approximation theo weights (khÃ´ng cÃ³ bias)
    
    âˆ‡L(w) = (1/n) * X^T(Xw - y) + Î» * w / âˆš(wÂ² + Îµ)
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        lambda_reg: há»‡ sá»‘ regularization
        epsilon: tham sá»‘ smoothing
    
    Tráº£ vá»:
        gradient: gradient vector (n_features,)
    """
    ols_gradient = tinh_gradient_OLS(X, y, w)
    smooth_l1_gradient = lambda_reg * w / np.sqrt(w ** 2 + epsilon)
    return ols_gradient + smooth_l1_gradient


def tinh_hessian_Lasso_smooth(X: np.ndarray, w: np.ndarray, lambda_reg: float, epsilon: float = 1e-8) -> np.ndarray:
    """
    TÃ­nh ma tráº­n Hessian cá»§a hÃ m Lasso vá»›i smooth approximation (khÃ´ng cÃ³ bias)
    
    H = (1/n) * X^T * X + Î» * diag(Îµ / (wÂ² + Îµ)^(3/2))
    
    Tham sá»‘:
        X: ma tráº­n Ä‘áº·c trÆ°ng (n_samples, n_features)
        w: vector weights (n_features,)
        lambda_reg: há»‡ sá»‘ regularization
        epsilon: tham sá»‘ smoothing
    
    Tráº£ vá»:
        hessian: ma tráº­n Hessian (n_features, n_features)
    """
    ols_hessian = tinh_hessian_OLS(X)
    
    # Diagonal elements cho L1 smooth penalty
    denominator = (w ** 2 + epsilon) ** (3/2)
    l1_diagonal = lambda_reg * epsilon / denominator
    
    # ThÃªm diagonal elements
    l1_hessian = np.diag(l1_diagonal)
    
    return ols_hessian + l1_hessian


# ==============================================================================
# 6. ÄÃNH GIÃ MÃ” HÃŒNH
# ==============================================================================

def danh_gia_mo_hinh(weights: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, he_so_tu_do: float = 0) -> Dict[str, float]:
    """
    ÄÃ¡nh giÃ¡ model trÃªn test set vá»›i Ä‘áº§y Ä‘á»§ cÃ¡c metrics
    
    Tham sá»‘:
        weights: trá»ng sá»‘ Ä‘Ã£ há»c (n_features,)
        X_test: ma tráº­n Ä‘áº·c trÆ°ng test (n_samples, n_features)
        y_test: giÃ¡ trá»‹ tháº­t test (n_samples,)
        he_so_tu_do: há»‡ sá»‘ tá»± do (máº·c Ä‘á»‹nh 0)
    
    Tráº£ vá»:
        dict: dictionary chá»©a cÃ¡c metrics Ä‘Ã¡nh giÃ¡
    """
    # Dá»± Ä‘oÃ¡n
    predictions = du_doan(X_test, weights, he_so_tu_do)
    
    # TÃ­nh cÃ¡c metrics cÆ¡ báº£n
    mse = tinh_mse(y_test, predictions)
    rmse = np.sqrt(mse)
    mae = tinh_mae(y_test, predictions)
    r2 = tinh_r2_score(y_test, predictions)
    
    # MAPE (Mean Absolute Percentage Error) - cáº©n tháº­n vá»›i chia cho 0
    with np.errstate(divide='ignore', invalid='ignore'):
        percentage_errors = np.abs((y_test - predictions) / y_test)
        # Loáº¡i bá» cÃ¡c giÃ¡ trá»‹ inf vÃ  nan
        valid_errors = percentage_errors[np.isfinite(percentage_errors)]
        mape = np.mean(valid_errors) * 100 if len(valid_errors) > 0 else float('inf')
    
    # ThÃªm má»™t sá»‘ metrics bá»• sung
    # Max Error
    max_error = np.max(np.abs(y_test - predictions))
    
    # Explained variance score
    var_y = np.var(y_test)
    var_residual = np.var(y_test - predictions)
    explained_variance = 1 - (var_residual / var_y) if var_y != 0 else 0
    
    return {
        'mse': float(mse),
        'rmse': float(rmse),
        'mae': float(mae),
        'r2': float(r2),
        'mape': float(mape),
        'max_error': float(max_error),
        'explained_variance': float(explained_variance)
    }


def in_ket_qua_danh_gia(metrics: Dict[str, float], training_time: float = None, 
                       algorithm_name: str = "Model"):
    """
    In káº¿t quáº£ Ä‘Ã¡nh giÃ¡ model má»™t cÃ¡ch Ä‘áº¹p máº¯t
    
    Tham sá»‘:
        metrics: dictionary chá»©a cÃ¡c metrics tá»« evaluate_model
        training_time: thá»i gian training (tÃ¹y chá»n)
        algorithm_name: tÃªn thuáº­t toÃ¡n
    """
    print("\n" + "="*60)
    print(f"ğŸ“Š {algorithm_name.upper()} - EVALUATION RESULTS")
    print("="*60)
    
    print(f"\nğŸ¯ REGRESSION METRICS:")
    print(f"   MSE:      {metrics['mse']:.8f}")
    print(f"   RMSE:     {metrics['rmse']:.6f}")
    print(f"   MAE:      {metrics['mae']:.6f}")
    print(f"   RÂ² Score: {metrics['r2']:.6f}")
    
    if metrics['mape'] != float('inf'):
        print(f"   MAPE:     {metrics['mape']:.2f}%")
    else:
        print(f"   MAPE:     N/A (division by zero)")
        
    print(f"   Max Error: {metrics['max_error']:.6f}")
    print(f"   Explained Variance: {metrics['explained_variance']:.6f}")
    
    if training_time is not None:
        print(f"\nâ±ï¸ PERFORMANCE:")
        print(f"   Training Time: {training_time:.4f}s")
    
    # ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng model
    print(f"\nğŸ“ˆ MODEL QUALITY ASSESSMENT:")
    
    if metrics['r2'] >= 0.9:
        r2_assessment = "EXCELLENT (RÂ² â‰¥ 0.9)"
        r2_color = "ğŸŸ¢"
    elif metrics['r2'] >= 0.8:
        r2_assessment = "VERY GOOD (RÂ² â‰¥ 0.8)"
        r2_color = "ğŸŸ¡"
    elif metrics['r2'] >= 0.7:
        r2_assessment = "GOOD (RÂ² â‰¥ 0.7)"
        r2_color = "ğŸŸ "
    elif metrics['r2'] >= 0.5:
        r2_assessment = "MODERATE (RÂ² â‰¥ 0.5)"
        r2_color = "ğŸ”´"
    else:
        r2_assessment = "POOR (RÂ² < 0.5)"
        r2_color = "âš«"
    
    print(f"   {r2_color} RÂ² Assessment: {r2_assessment}")
    
    # MAPE assessment
    if metrics['mape'] != float('inf'):
        if metrics['mape'] <= 5:
            mape_assessment = "EXCELLENT (MAPE â‰¤ 5%)"
            mape_color = "ğŸŸ¢"
        elif metrics['mape'] <= 10:
            mape_assessment = "VERY GOOD (MAPE â‰¤ 10%)"
            mape_color = "ğŸŸ¡"
        elif metrics['mape'] <= 20:
            mape_assessment = "GOOD (MAPE â‰¤ 20%)"
            mape_color = "ğŸŸ "
        else:
            mape_assessment = "NEEDS IMPROVEMENT (MAPE > 20%)"
            mape_color = "ğŸ”´"
        
        print(f"   {mape_color} MAPE Assessment: {mape_assessment}")


# ==============================================================================
# 7. TIá»†N ÃCH DEBUG VÃ€ IN THÃ”NG TIN
# ==============================================================================

def in_thong_tin_ma_tran(matrix: np.ndarray, ten_ma_tran: str = "Matrix"):
    """
    In thÃ´ng tin chi tiáº¿t vá» ma tráº­n
    """
    print(f"\n=== {ten_ma_tran} ===")
    print(f"KÃ­ch thÆ°á»›c: {matrix.shape}")
    print(f"Condition number: {tinh_condition_number(matrix):.2e}")
    print(f"Positive definite: {kiem_tra_positive_definite(matrix)}")
    print(f"Eigenvalues min/max: {np.min(np.linalg.eigvals(matrix)):.2e} / {np.max(np.linalg.eigvals(matrix)):.2e}")


def in_thong_tin_gradient(gradient_w: np.ndarray, gradient_b: float):
    """
    In thÃ´ng tin vá» gradient
    """
    print(f"\n=== Gradient Info ===")
    print(f"||âˆ‡w||: {np.linalg.norm(gradient_w):.2e}")
    print(f"|âˆ‡b|: {abs(gradient_b):.2e}")
    print(f"||âˆ‡f||: {np.sqrt(np.sum(gradient_w**2) + gradient_b**2):.2e}")


# ==============================================================================
# 9. Bá»” SUNG LOSS FUNCTIONS KHÃC
# ==============================================================================

def tinh_loss_elastic_net(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray,
                         he_so_tu_do: float, alpha: float, l1_ratio: float) -> float:
    """
    TÃ­nh loss cho Elastic Net Regression
    
    Loss = MSE + alpha * (l1_ratio * L1 + (1-l1_ratio) * L2)
    """
    mse_loss = tinh_loss_ols(X, y, trong_so, he_so_tu_do)
    l1_penalty = alpha * l1_ratio * np.sum(np.abs(trong_so))
    l2_penalty = alpha * (1 - l1_ratio) * 0.5 * np.sum(trong_so ** 2)
    return mse_loss + l1_penalty + l2_penalty


def tinh_loss_huber(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray,
                   he_so_tu_do: float, delta: float = 1.0) -> float:
    """
    TÃ­nh Huber loss - robust loss function
    
    Huber(e) = 0.5 * eÂ² if |e| â‰¤ Î´ else Î´ * (|e| - 0.5 * Î´)
    """
    predictions = du_doan(X, trong_so, he_so_tu_do)
    errors = predictions - y
    
    # Ãp dá»¥ng Huber loss
    abs_errors = np.abs(errors)
    quadratic = 0.5 * errors ** 2
    linear = delta * (abs_errors - 0.5 * delta)
    
    # Chá»n quadratic hoáº·c linear dá»±a trÃªn threshold
    huber_losses = np.where(abs_errors <= delta, quadratic, linear)
    return np.mean(huber_losses)


def tinh_gradient_ridge(X: np.ndarray, y: np.ndarray, w: np.ndarray, lam: float) -> np.ndarray:
    """
    TÃ­nh gradient cho Ridge regression
    
    âˆ‡f = X^T(Xw - y)/n + Î»w
    """
    n = X.shape[0]
    predictions = X @ w
    errors = predictions - y
    gradient = (X.T @ errors) / n + lam * w
    return gradient


def tinh_gradient_lasso_smooth(X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                              lam: float, epsilon: float = 1e-8) -> np.ndarray:
    """
    TÃ­nh gradient cho Lasso regression (smooth approximation)
    
    âˆ‡f = X^T(Xw - y)/n + Î» * w/âˆš(wÂ² + Îµ)
    """
    n = X.shape[0]
    predictions = X @ w
    errors = predictions - y
    mse_gradient = (X.T @ errors) / n
    
    # Smooth L1 gradient
    l1_gradient = lam * w / np.sqrt(w ** 2 + epsilon)
    return mse_gradient + l1_gradient


def tinh_hessian_ridge(X: np.ndarray, lam: float) -> np.ndarray:
    """
    TÃ­nh Hessian cho Ridge regression
    
    H = X^TX/n + Î»I
    """
    n = X.shape[0]
    XTX = X.T @ X
    I = np.eye(X.shape[1])
    return XTX / n + lam * I