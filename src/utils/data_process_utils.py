"""
Ti·ªán √≠ch X·ª≠ l√Ω D·ªØ li·ªáu - C√°c h√†m l√†m vi·ªác v·ªõi d·ªØ li·ªáu

=== M·ª§C ƒê√çCH: X·ª¨ L√ù D·ªÆ LI·ªÜU ===

Bao g·ªìm t·∫•t c·∫£ c√°c h√†m c·∫ßn thi·∫øt cho:
1. ƒê·ªçc v√† t·∫£i d·ªØ li·ªáu an to√†n
2. L√†m s·∫°ch v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu  
3. T·ªëi ∆∞u h√≥a memory v√† performance
4. Chia batch v√† chunking
5. Validate v√† transform d·ªØ li·ªáu

Code ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu, d·ªÖ s·ª≠ d·ª•ng.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, List, Optional, Dict, Any
import warnings
import json


# ==============================================================================
# 1. ƒê·ªåC V√Ä T·∫¢I D·ªÆ LI·ªÜU
# ==============================================================================

def tai_du_lieu_chunked(file_path: str, 
                        chunk_size: int = 10000, 
                        max_rows: Optional[int] = None,
                        columns: Optional[List[str]] = None) -> pd.DataFrame:
    """
    T·∫£i d·ªØ li·ªáu l·ªõn theo chunks ƒë·ªÉ ti·∫øt ki·ªám memory
    
    H·ªØu √≠ch cho file CSV r·∫•t l·ªõn kh√¥ng th·ªÉ load h·∫øt v√†o memory.
    
    Tham s·ªë:
        file_path: ƒë∆∞·ªùng d·∫´n ƒë·∫øn file
        chunk_size: s·ªë rows m·ªói chunk
        max_rows: gi·ªõi h·∫°n t·ªïng s·ªë rows (None = kh√¥ng gi·ªõi h·∫°n)
        columns: danh s√°ch columns c·∫ßn load (None = load t·∫•t c·∫£)
    
    Tr·∫£ v·ªÅ:
        DataFrame: d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c g·ªôp t·ª´ c√°c chunks
    """
    chunks = []
    total_rows = 0
    
    try:
        for chunk in pd.read_csv(file_path, chunksize=chunk_size, usecols=columns):
            # L√†m s·∫°ch chunk
            chunk = lam_sach_ten_cot(chunk)
            chunk = toi_uu_memory_dataframe(chunk)
            
            chunks.append(chunk)
            total_rows += len(chunk)
            if total_rows % 100000 == 0:
                print(f"Loaded {total_rows} rows")
            # Ki·ªÉm tra gi·ªõi h·∫°n rows
            if max_rows and total_rows >= max_rows:
                break

    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc chunks: {e}")
        if chunks:
            print(f"ƒê√£ ƒë·ªçc ƒë∆∞·ª£c {len(chunks)} chunks tr∆∞·ªõc khi l·ªói")
        else:
            raise
    
    # G·ªôp t·∫•t c·∫£ chunks
    if chunks:
        df = pd.concat(chunks, ignore_index=True)
        if max_rows:
            df = df.head(max_rows)
        return df
    else:
        return pd.DataFrame()


# ==============================================================================
# 2. L√ÄM S·∫†CH V√Ä TI·ªÄN X·ª¨ L√ù
# ==============================================================================

def lam_sach_ten_cot(df: pd.DataFrame) -> pd.DataFrame:
    """
    L√†m s·∫°ch t√™n c·ªôt ƒë·ªÉ d·ªÖ s·ª≠ d·ª•ng
    
    - Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    - Chuy·ªÉn v·ªÅ lowercase
    - Thay th·∫ø k√Ω t·ª± ƒë·∫∑c bi·ªát b·∫±ng underscore
    
    Tham s·ªë:
        df: DataFrame c·∫ßn l√†m s·∫°ch t√™n c·ªôt
    
    Tr·∫£ v·ªÅ:
        DataFrame: v·ªõi t√™n c·ªôt ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
    """
    df = df.copy()
    
    # L√†m s·∫°ch t√™n c·ªôt
    new_columns = []
    for col in df.columns:
        # Chuy·ªÉn v·ªÅ string v√† strip
        new_col = str(col).strip()
        
        # Chuy·ªÉn v·ªÅ lowercase
        new_col = new_col.lower()
        
        # Thay th·∫ø kho·∫£ng tr·∫Øng v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
        import re
        new_col = re.sub(r'[^a-zA-Z0-9_]', '_', new_col)
        
        # Lo·∫°i b·ªè underscore li√™n ti·∫øp
        new_col = re.sub(r'_+', '_', new_col)
        
        # Lo·∫°i b·ªè underscore ·ªü ƒë·∫ßu/cu·ªëi
        new_col = new_col.strip('_')
        
        new_columns.append(new_col)
    
    df.columns = new_columns
    return df


def xu_ly_gia_tri_null(df: pd.DataFrame, strategy: str = 'auto') -> pd.DataFrame:
    """
    X·ª≠ l√Ω gi√° tr·ªã null trong DataFrame
    
    Tham s·ªë:
        df: DataFrame c·∫ßn x·ª≠ l√Ω
        strategy: chi·∫øn l∆∞·ª£c x·ª≠ l√Ω ('auto', 'drop', 'fill_mean', 'fill_median', 'fill_mode')
    
    Tr·∫£ v·ªÅ:
        DataFrame: ƒë√£ x·ª≠ l√Ω null values
    """
    df = df.copy()
    
    if strategy == 'auto':
        # T·ª± ƒë·ªông ch·ªçn strategy t·ªët nh·∫•t cho t·ª´ng c·ªôt
        for col in df.columns:
            null_ratio = df[col].isnull().sum() / len(df)
            
            if null_ratio > 0.5:
                # Qu√° nhi·ªÅu null, x√≥a c·ªôt
                df = df.drop(columns=[col])
            elif df[col].dtype in ['int64', 'float64']:
                # Numeric: fill median
                df[col] = df[col].fillna(df[col].median())
            else:
                # Categorical: fill mode
                mode_value = df[col].mode()
                if len(mode_value) > 0:
                    df[col] = df[col].fillna(mode_value[0])
    
    elif strategy == 'drop':
        df = df.dropna()
    
    elif strategy == 'fill_mean':
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
    
    elif strategy == 'fill_median':
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
    
    elif strategy == 'fill_mode':
        for col in df.columns:
            mode_value = df[col].mode()
            if len(mode_value) > 0:
                df[col] = df[col].fillna(mode_value[0])
    
    return df


def tach_dac_trung_va_target(df: pd.DataFrame, target_col: str) -> Tuple[pd.DataFrame, pd.Series]:
    """
    T√°ch DataFrame th√†nh ƒë·∫∑c tr∆∞ng (X) v√† target (y)
    
    Tham s·ªë:
        df: DataFrame ch·ª©a to√†n b·ªô d·ªØ li·ªáu
        target_col: t√™n c·ªôt target
    
    Tr·∫£ v·ªÅ:
        X: DataFrame ƒë·∫∑c tr∆∞ng
        y: Series target
    """
    if target_col not in df.columns:
        raise ValueError(f"C·ªôt target '{target_col}' kh√¥ng t·ªìn t·∫°i trong DataFrame")
    
    y = df[target_col].copy()
    X = df.drop(columns=[target_col]).copy()
    
    return X, y


def chuan_hoa_du_lieu(X: pd.DataFrame, method: str = 'standard') -> Tuple[pd.DataFrame, Dict[str, Any]]:
    """
    Chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫∑c tr∆∞ng
    
    Tham s·ªë:
        X: DataFrame ƒë·∫∑c tr∆∞ng
        method: ph∆∞∆°ng ph√°p chu·∫©n h√≥a ('standard', 'minmax', 'robust')
    
    Tr·∫£ v·ªÅ:
        X_scaled: DataFrame ƒë√£ chu·∫©n h√≥a
        scaler_params: tham s·ªë ƒë·ªÉ inverse transform
    """
    X_scaled = X.copy()
    scaler_params = {}
    
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        if method == 'standard':
            # Z-score normalization: (x - mean) / std
            mean_val = X[col].mean()
            std_val = X[col].std()
            if std_val > 0:
                X_scaled[col] = (X[col] - mean_val) / std_val
                scaler_params[col] = {'mean': mean_val, 'std': std_val, 'method': 'standard'}
        
        elif method == 'minmax':
            # Min-max scaling: (x - min) / (max - min)
            min_val = X[col].min()
            max_val = X[col].max()
            if max_val > min_val:
                X_scaled[col] = (X[col] - min_val) / (max_val - min_val)
                scaler_params[col] = {'min': min_val, 'max': max_val, 'method': 'minmax'}
        
        elif method == 'robust':
            # Robust scaling: (x - median) / IQR
            median_val = X[col].median()
            q75 = X[col].quantile(0.75)
            q25 = X[col].quantile(0.25)
            iqr = q75 - q25
            if iqr > 0:
                X_scaled[col] = (X[col] - median_val) / iqr
                scaler_params[col] = {'median': median_val, 'iqr': iqr, 'method': 'robust'}
    
    return X_scaled, scaler_params


# ==============================================================================
# 3. T·ªêI ∆ØU H√ìA MEMORY V√Ä PERFORMANCE
# ==============================================================================

def toi_uu_memory_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """
    T·ªëi ∆∞u h√≥a memory usage c·ªßa DataFrame
    
    T·ª± ƒë·ªông chuy·ªÉn c√°c c·ªôt v·ªÅ dtype ph√π h·ª£p nh·∫•t ƒë·ªÉ ti·∫øt ki·ªám memory.
    
    Tham s·ªë:
        df: DataFrame c·∫ßn t·ªëi ∆∞u
    
    Tr·∫£ v·ªÅ:
        DataFrame: ƒë√£ ƒë∆∞·ª£c t·ªëi ∆∞u memory
    """
    df = df.copy()
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if col_type == 'object':
            # Th·ª≠ convert sang numeric
            numeric_converted = pd.to_numeric(df[col], errors='ignore')
            if numeric_converted.dtype != 'object':
                df[col] = numeric_converted
                col_type = df[col].dtype
        
        if col_type in ['int64', 'int32']:
            # T·ªëi ∆∞u integer columns
            c_min = df[col].min()
            c_max = df[col].max()
            
            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                df[col] = df[col].astype(np.int32)
        
        elif col_type in ['float64', 'float32']:
            # T·ªëi ∆∞u float columns
            c_min = df[col].min()
            c_max = df[col].max()
            
            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                df[col] = df[col].astype(np.float32)
        
        elif col_type == 'object':
            # Th·ª≠ convert sang category n·∫øu c√≥ √≠t unique values
            num_unique = df[col].nunique()
            num_total = len(df[col])
            
            if num_unique / num_total < 0.5:  # N·∫øu < 50% unique
                df[col] = df[col].astype('category')
    
    return df


def lay_thong_tin_du_lieu(df: pd.DataFrame) -> Dict[str, Any]:
    """
    L·∫•y th√¥ng tin t·ªïng quan v·ªÅ DataFrame
    
    Tham s·ªë:
        df: DataFrame c·∫ßn ph√¢n t√≠ch
    
    Tr·∫£ v·ªÅ:
        dict: th√¥ng tin chi ti·∫øt v·ªÅ d·ªØ li·ªáu
    """
    info = {
        'shape': df.shape,
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'null_counts': df.isnull().sum().to_dict(),
        'dtypes': df.dtypes.to_dict(),
        'numeric_columns': list(df.select_dtypes(include=[np.number]).columns),
        'categorical_columns': list(df.select_dtypes(include=['category', 'object']).columns),
        'duplicate_rows': df.duplicated().sum(),
    }
    
    # Th·ªëng k√™ cho numeric columns
    if info['numeric_columns']:
        info['numeric_stats'] = df[info['numeric_columns']].describe().to_dict()
    
    return info


# ==============================================================================
# 4. CHIA BATCH V√Ä CHUNKING
# ==============================================================================

def tao_batches(X: np.ndarray, y: np.ndarray, batch_size: int = 32, 
               shuffle: bool = True) -> List[Tuple[np.ndarray, np.ndarray]]:
    """
    Chia d·ªØ li·ªáu th√†nh c√°c batches cho training
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng
        y: vector target
        batch_size: k√≠ch th∆∞·ªõc m·ªói batch
        shuffle: c√≥ shuffle d·ªØ li·ªáu kh√¥ng
    
    Tr·∫£ v·ªÅ:
        List[Tuple]: danh s√°ch c√°c (X_batch, y_batch)
    """
    n_samples = len(X)
    indices = np.arange(n_samples)
    
    if shuffle:
        np.random.shuffle(indices)
    
    batches = []
    for start_idx in range(0, n_samples, batch_size):
        end_idx = min(start_idx + batch_size, n_samples)
        batch_indices = indices[start_idx:end_idx]
        
        X_batch = X[batch_indices]
        y_batch = y[batch_indices]
        
        batches.append((X_batch, y_batch))
    
    return batches


def chia_train_test(X: np.ndarray, y: np.ndarray, test_size: float = 0.2, 
                   random_state: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng
        y: vector target  
        test_size: t·ª∑ l·ªá test set (0.0 - 1.0)
        random_state: seed cho reproducibility
    
    Tr·∫£ v·ªÅ:
        X_train, X_test, y_train, y_test
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    n_samples = len(X)
    n_test = int(n_samples * test_size)
    
    # Shuffle indices
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    
    # Split
    test_indices = indices[:n_test]
    train_indices = indices[n_test:]
    
    X_train = X[train_indices]
    X_test = X[test_indices] 
    y_train = y[train_indices]
    y_test = y[test_indices]
    
    return X_train, X_test, y_train, y_test


# ==============================================================================
# 5. VALIDATE V√Ä TRANSFORM
# ==============================================================================

def kiem_tra_du_lieu_dau_vao(X: np.ndarray, y: np.ndarray) -> bool:
    """
    Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa d·ªØ li·ªáu ƒë·∫ßu v√†o
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng
        y: vector target
    
    Tr·∫£ v·ªÅ:
        bool: True n·∫øu d·ªØ li·ªáu h·ª£p l·ªá
    """
    try:
        # Ki·ªÉm tra shape
        if len(X.shape) != 2:
            print("L·ªói: X ph·∫£i l√† ma tr·∫≠n 2D")
            return False
        
        if len(y.shape) != 1:
            print("L·ªói: y ph·∫£i l√† vector 1D")
            return False
        
        if X.shape[0] != y.shape[0]:
            print("L·ªói: S·ªë samples trong X v√† y kh√¥ng kh·ªõp")
            return False
        
        # Ki·ªÉm tra gi√° tr·ªã
        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            print("C·∫£nh b√°o: X ch·ª©a NaN ho·∫∑c Inf")
            return False
        
        if np.any(np.isnan(y)) or np.any(np.isinf(y)):
            print("C·∫£nh b√°o: y ch·ª©a NaN ho·∫∑c Inf")
            return False
        
        print(f"D·ªØ li·ªáu h·ª£p l·ªá: {X.shape[0]} samples, {X.shape[1]} features")
        return True
        
    except Exception as e:
        print(f"L·ªói khi ki·ªÉm tra d·ªØ li·ªáu: {e}")
        return False


def chuyen_pandas_to_numpy(df: pd.DataFrame) -> np.ndarray:
    """
    Chuy·ªÉn DataFrame pandas sang numpy array an to√†n
    
    Tham s·ªë:
        df: DataFrame c·∫ßn chuy·ªÉn ƒë·ªïi
    
    Tr·∫£ v·ªÅ:
        np.ndarray: m·∫£ng numpy
    """
    # Ch·ªâ l·∫•y numeric columns
    numeric_df = df.select_dtypes(include=[np.number])
    
    if numeric_df.empty:
        raise ValueError("Kh√¥ng c√≥ c·ªôt n√†o l√† numeric ƒë·ªÉ chuy·ªÉn ƒë·ªïi")
    
    return numeric_df.values.astype(np.float64)


def load_du_lieu():
    """
    Load d·ªØ li·ªáu t·ª´ 02_processed directory (data ƒë√£ ƒë∆∞·ª£c preprocessed)
    
    Tr·∫£ v·ªÅ:
        X_train, X_test, y_train, y_test: numpy arrays
        
    Note: Target data ƒë√£ ƒë∆∞·ª£c log-transformed trong preprocessing pipeline
    """
    data_dir = Path("data/02_processed")
    
    # Check if directory exists
    if not data_dir.exists():
        raise FileNotFoundError(f"Data directory {data_dir} kh√¥ng t·ªìn t·∫°i. H√£y ch·∫°y preprocessing tr∆∞·ªõc.")
    
    # Load data files
    try:
        X_train = tai_du_lieu_chunked(data_dir / "X_train.csv").values
        X_test = tai_du_lieu_chunked(data_dir / "X_test.csv").values
        y_train = tai_du_lieu_chunked(data_dir / "y_train.csv").values.ravel()
        y_test = tai_du_lieu_chunked(data_dir / "y_test.csv").values.ravel()
        
        print(f"‚úÖ ƒê√£ load d·ªØ li·ªáu: Train {X_train.shape}, Test {X_test.shape}")
        print(f"   Target (log): y_train [{y_train.min():.3f}, {y_train.max():.3f}]")
        print(f"   Target (log): y_test [{y_test.min():.3f}, {y_test.max():.3f}]")
        
        # Load and display metadata if available
        feature_info_path = data_dir / "feature_info.json"
        if feature_info_path.exists():
            with open(feature_info_path, 'r') as f:
                feature_info = json.load(f)
            
            print(f"   üìã S·ªë features: {feature_info['n_features']}")
            print(f"   üîÑ Bi·∫øn ƒë·ªïi: {feature_info['target_info']['transformation']}")
            print(f"   ‚úÖ S·∫µn s√†ng cho thu·∫≠t to√°n")
        
        return X_train, X_test, y_train, y_test
        
    except FileNotFoundError as e:
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file data: {e}. H√£y ch·∫°y preprocessing pipeline tr∆∞·ªõc.")
    except Exception as e:
        raise RuntimeError(f"L·ªói khi load data: {e}")


def in_thong_tin_du_lieu(df: pd.DataFrame, ten_dataset: str = "Dataset"):
    """
    In th√¥ng tin t·ªïng quan v·ªÅ dataset
    """
    info = lay_thong_tin_du_lieu(df)
    
    print(f"\n=== {ten_dataset} ===")
    print(f"K√≠ch th∆∞·ªõc: {info['shape']}")
    print(f"Memory usage: {info['memory_usage_mb']:.2f} MB")
    print(f"Duplicate rows: {info['duplicate_rows']}")
    print(f"Numeric columns: {len(info['numeric_columns'])}")
    print(f"Categorical columns: {len(info['categorical_columns'])}")
    
    # Null values
    null_cols = {k: v for k, v in info['null_counts'].items() if v > 0}
    if null_cols:
        print(f"Null values: {null_cols}")
    else:
        print("Kh√¥ng c√≥ null values")