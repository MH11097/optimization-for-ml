#!/usr/bin/env python3
"""
SciPy Newton-CG Comparison - So s√°nh k·∫øt qu·∫£ v·ªõi scipy.optimize.minimize(method='Newton-CG')
Newton-CG l√† phi√™n b·∫£n truncated Newton method, g·∫ßn v·ªõi pure Newton's method
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import time
import json
from scipy.optimize import minimize
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Add the src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from utils.data_process_utils import load_du_lieu
from utils.optimization_utils import (
    tinh_gia_tri_ham_loss, tinh_gradient_ham_loss, tinh_hessian_ham_loss, 
    add_bias_column
)


def create_scipy_newton_functions(X, y, loss_type='ols', regularization=0.01):
    """
    T·∫°o objective, gradient v√† hessian functions cho SciPy Newton-CG
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng v·ªõi bias ƒë√£ th√™m (n_samples, n_features + 1)
        y: vector target (n_samples,)
        loss_type: lo·∫°i loss function ('ols', 'ridge', 'lasso')
        regularization: h·ªá s·ªë regularization
    
    Tr·∫£ v·ªÅ:
        objective_func: h√†m t√≠nh loss
        gradient_func: h√†m t√≠nh gradient
        hessian_func: h√†m t√≠nh hessian
    """
    def objective_func(w):
        """T√≠nh gi√° tr·ªã loss function"""
        return tinh_gia_tri_ham_loss(loss_type, X, y, w, bias=None, regularization=regularization)
    
    def gradient_func(w):
        """T√≠nh gradient"""
        grad_w, grad_b = tinh_gradient_ham_loss(loss_type, X, y, w, bias=None, regularization=regularization)
        return grad_w  # grad_b s·∫Ω l√† 0 cho format m·ªõi v·ªõi bias trong X
    
    def hessian_func(w):
        """T√≠nh hessian matrix"""
        if loss_type == 'lasso':
            # Lasso c·∫ßn weights ƒë·ªÉ t√≠nh Hessian
            return tinh_hessian_ham_loss(loss_type, X, w, regularization=regularization)
        else:
            # OLS v√† Ridge kh√¥ng c·∫ßn weights
            return tinh_hessian_ham_loss(loss_type, X, regularization=regularization)
    
    return objective_func, gradient_func, hessian_func


def run_scipy_newton_optimization(X_train, y_train, loss_type='ols', regularization=0.01, max_iter=100):
    """
    Ch·∫°y t·ªëi ∆∞u h√≥a b·∫±ng SciPy Newton-CG method
    
    Tham s·ªë:
        X_train: ma tr·∫≠n ƒë·∫∑c tr∆∞ng train (n_samples, n_features)
        y_train: vector target train (n_samples,)
        loss_type: lo·∫°i loss function
        regularization: h·ªá s·ªë regularization
        max_iter: s·ªë iteration t·ªëi ƒëa
    
    Tr·∫£ v·ªÅ:
        dict: k·∫øt qu·∫£ t·ªëi ∆∞u h√≥a
    """
    print(f"\nüî¨ Running SciPy Newton-CG optimization for {loss_type.upper()}...")
    
    # Th√™m bias column
    X_with_bias = add_bias_column(X_train)
    n_features_with_bias = X_with_bias.shape[1]
    
    # Kh·ªüi t·∫°o weights
    np.random.seed(42)
    initial_weights = np.random.randn(n_features_with_bias) * 0.01
    
    print(f"   Features (with bias): {n_features_with_bias}")
    print(f"   Initial weights range: [{initial_weights.min():.6f}, {initial_weights.max():.6f}]")
    print(f"   Loss type: {loss_type}")
    print(f"   Regularization: {regularization}")
    
    # T·∫°o objective, gradient v√† hessian functions
    objective_func, gradient_func, hessian_func = create_scipy_newton_functions(
        X_with_bias, y_train, loss_type, regularization
    )
    
    # T√≠nh initial loss
    initial_loss = objective_func(initial_weights)
    initial_gradient_norm = np.linalg.norm(gradient_func(initial_weights))
    print(f"   Initial loss: {initial_loss:.8f}")
    print(f"   Initial gradient norm: {initial_gradient_norm:.2e}")
    
    # Ki·ªÉm tra Hessian matrix condition
    try:
        initial_hessian = hessian_func(initial_weights)
        hessian_cond = np.linalg.cond(initial_hessian)
        hessian_det = np.linalg.det(initial_hessian)
        print(f"   Initial Hessian condition number: {hessian_cond:.2e}")
        print(f"   Initial Hessian determinant: {hessian_det:.2e}")
    except Exception as e:
        print(f"   Warning: Could not compute initial Hessian info: {e}")
    
    # Ch·∫°y optimization v·ªõi SciPy Newton-CG
    print(f"   Starting SciPy Newton-CG optimization...")
    start_time = time.time()
    
    try:
        result = minimize(
            fun=objective_func,
            x0=initial_weights,
            method='Newton-CG',  # Newton's method v·ªõi Conjugate Gradient ƒë·ªÉ gi·∫£i h·ªá tuy·∫øn t√≠nh
            jac=gradient_func,
            hess=hessian_func,
            options={
                'maxiter': max_iter,
                'xtol': 1e-8,    # Tolerance for convergence in parameters
                'gtol': 1e-6,    # Gradient tolerance
                'disp': False
            }
        )
        
        training_time = time.time() - start_time
        
        # Extract results
        final_weights = result.x
        final_loss = result.fun
        converged = result.success
        iterations = result.nit if hasattr(result, 'nit') else result.nfev
        final_gradient_norm = np.linalg.norm(gradient_func(final_weights))
        
        print(f"   ‚úÖ Optimization completed!")
        print(f"   Training time: {training_time:.4f} seconds")
        print(f"   Converged: {converged}")
        print(f"   Iterations: {iterations}")
        print(f"   Final loss: {final_loss:.8f}")
        print(f"   Final gradient norm: {final_gradient_norm:.2e}")
        print(f"   Final weights range: [{final_weights.min():.6f}, {final_weights.max():.6f}]")
        print(f"   Bias (last weight): {final_weights[-1]:.6f}")
        
        return {
            'weights': final_weights.tolist(),
            'final_loss': float(final_loss),
            'training_time': float(training_time),
            'converged': bool(converged),
            'iterations': int(iterations),
            'final_gradient_norm': float(final_gradient_norm),
            'algorithm': f'SciPy_Newton_CG_{loss_type.upper()}',
            'loss_type': loss_type,
            'regularization': float(regularization),
            'message': str(result.message),
            'initial_loss': float(initial_loss),
            'initial_gradient_norm': float(initial_gradient_norm)
        }
    
    except Exception as e:
        training_time = time.time() - start_time
        print(f"   ‚ùå Optimization failed: {e}")
        print(f"   Training time before failure: {training_time:.4f} seconds")
        
        # Return fallback result
        return {
            'weights': initial_weights.tolist(),
            'final_loss': float(initial_loss),
            'training_time': float(training_time),
            'converged': False,
            'iterations': 0,
            'final_gradient_norm': float(initial_gradient_norm),
            'algorithm': f'SciPy_Newton_CG_{loss_type.upper()}',
            'loss_type': loss_type,
            'regularization': float(regularization),
            'message': f"Optimization failed: {e}",
            'initial_loss': float(initial_loss),
            'initial_gradient_norm': float(initial_gradient_norm),
            'error': str(e)
        }


def evaluate_scipy_results(result, X_test, y_test):
    """
    ƒê√°nh gi√° k·∫øt qu·∫£ SciPy tr√™n test set
    
    Tham s·ªë:
        result: k·∫øt qu·∫£ t·ª´ run_scipy_newton_optimization
        X_test: ma tr·∫≠n ƒë·∫∑c tr∆∞ng test
        y_test: vector target test
    
    Tr·∫£ v·ªÅ:
        dict: metrics ƒë√°nh gi√°
    """
    # Th√™m bias column cho X_test
    X_test_with_bias = add_bias_column(X_test)
    
    # D·ª± ƒëo√°n
    weights = result['weights']
    predictions_log = X_test_with_bias @ weights
    
    # Evaluate on log scale
    mse_log = mean_squared_error(y_test, predictions_log)
    r2_log = r2_score(y_test, predictions_log)
    mae_log = mean_absolute_error(y_test, predictions_log)
    
    # Convert to original scale
    predictions_original = np.expm1(predictions_log)
    y_test_original = np.expm1(y_test)
    
    # Evaluate on original scale
    mse_original = mean_squared_error(y_test_original, predictions_original)
    r2_original = r2_score(y_test_original, predictions_original)
    mae_original = mean_absolute_error(y_test_original, predictions_original)
    rmse_original = np.sqrt(mse_original)
    
    # MAPE calculation
    with np.errstate(divide='ignore', invalid='ignore'):
        percentage_errors = np.abs((y_test_original - predictions_original) / y_test_original)
        valid_errors = percentage_errors[np.isfinite(percentage_errors)]
        mape = np.mean(valid_errors) * 100 if len(valid_errors) > 0 else float('inf')
    
    return {
        'predictions_log_min': float(predictions_log.min()),
        'predictions_log_max': float(predictions_log.max()),
        'predictions_original_min': float(predictions_original.min()),
        'predictions_original_max': float(predictions_original.max()),
        'metrics_log_scale': {
            'mse': float(mse_log),
            'r2': float(r2_log),
            'mae': float(mae_log)
        },
        'metrics_original_scale': {
            'mse': float(mse_original),
            'rmse': float(rmse_original),
            'mae': float(mae_original),
            'r2': float(r2_original),
            'mape': float(mape)
        }
    }


def run_comparison_experiments():
    """
    Ch·∫°y c√°c th√≠ nghi·ªám so s√°nh cho OLS, Ridge, Lasso v·ªõi Newton's method
    """
    print("="*80)
    print("üî¨ SCIPY NEWTON-CG COMPARISON - NEWTON'S METHOD ALGORITHMS")
    print("="*80)
    
    # Load data
    print("\nüìÇ Loading data...")
    X_train, X_test, y_train, y_test = load_du_lieu()
    
    print(f"   Data shapes: Train {X_train.shape}, Test {X_test.shape}")
    print(f"   Target range (log scale): y_train [{y_train.min():.3f}, {y_train.max():.3f}]")
    
    # Configurations to test
    # Note: Newton's method work best with regularization ƒë·ªÉ tr√°nh singular Hessian
    configs = [
        {'loss_type': 'ols', 'regularization': 1e-6},     # Tiny regularization for numerical stability
        {'loss_type': 'ridge', 'regularization': 0.01},
        {'loss_type': 'lasso', 'regularization': 0.01}    # Lasso c√≥ th·ªÉ kh√≥ h·ªôi t·ª• v·ªõi Newton
    ]
    
    all_results = []
    
    for config in configs:
        print(f"\n" + "="*50)
        print(f"üß™ EXPERIMENT: {config['loss_type'].upper()} Newton's Method")
        print(f"   Regularization: {config['regularization']}")
        print("="*50)
        
        # Run optimization
        result = run_scipy_newton_optimization(
            X_train, y_train, 
            loss_type=config['loss_type'],
            regularization=config['regularization'],
            max_iter=100  # Newton th∆∞·ªùng h·ªôi t·ª• nhanh h∆°n
        )
        
        # Evaluate
        print(f"\nüìä Evaluating on test set...")
        evaluation = evaluate_scipy_results(result, X_test, y_test)
        
        # Combine results
        combined_result = {
            **result,
            **evaluation,
            'parameters': config
        }
        
        # Print evaluation summary
        print(f"   üìà LOG SCALE METRICS:")
        print(f"      MSE: {evaluation['metrics_log_scale']['mse']:.8f}")
        print(f"      R¬≤:  {evaluation['metrics_log_scale']['r2']:.6f}")
        print(f"      MAE: {evaluation['metrics_log_scale']['mae']:.6f}")
        
        print(f"   üéØ ORIGINAL SCALE METRICS:")
        print(f"      MSE:  {evaluation['metrics_original_scale']['mse']:,.2f}")
        print(f"      RMSE: {evaluation['metrics_original_scale']['rmse']:,.2f}")
        print(f"      MAE:  {evaluation['metrics_original_scale']['mae']:,.2f}")
        print(f"      R¬≤:   {evaluation['metrics_original_scale']['r2']:.6f}")
        if evaluation['metrics_original_scale']['mape'] != float('inf'):
            print(f"      MAPE: {evaluation['metrics_original_scale']['mape']:.2f}%")
        else:
            print(f"      MAPE: N/A")
        
        all_results.append(combined_result)
        
        # Save individual result
        output_dir = Path(f"data/03_algorithms/newton_method/scipy_newton_{config['loss_type']}")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        with open(output_dir / "results.json", "w") as f:
            json.dump(combined_result, f, indent=2)
        
        print(f"   üíæ Results saved to: {output_dir / 'results.json'}")
    
    # Summary comparison
    print(f"\n" + "="*80)
    print("üìã SUMMARY COMPARISON - NEWTON'S METHOD")
    print("="*80)
    
    print(f"\n{'Algorithm':<25} {'Loss Type':<10} {'Converged':<10} {'R¬≤ (log)':<10} {'R¬≤ (orig)':<10} {'Time (s)':<10}")
    print("-"*85)
    
    for result in all_results:
        algorithm = result['algorithm'][:24]  # Truncate for display
        loss_type = result['loss_type']
        converged = "‚úÖ" if result['converged'] else "‚ùå"
        r2_log = result['metrics_log_scale']['r2']
        r2_orig = result['metrics_original_scale']['r2']
        time_s = result['training_time']
        
        print(f"{algorithm:<25} {loss_type:<10} {converged:<10} {r2_log:<10.6f} {r2_orig:<10.6f} {time_s:<10.4f}")
    
    print(f"\n‚úÖ All SciPy Newton-CG comparisons completed!")
    print(f"üìÅ Results saved in data/03_algorithms/newton_method/scipy_newton_*/")
    
    # Newton's method specific insights
    print(f"\nüîç NEWTON'S METHOD INSIGHTS:")
    print(f"   ‚Ä¢ Newton's method th∆∞·ªùng h·ªôi t·ª• r·∫•t nhanh (v√†i iterations)")
    print(f"   ‚Ä¢ Y√™u c·∫ßu Hessian matrix ph·∫£i positive definite")
    print(f"   ‚Ä¢ SciPy Newton-CG s·ª≠ d·ª•ng Conjugate Gradient ƒë·ªÉ gi·∫£i h·ªá tuy·∫øn t√≠nh")
    print(f"   ‚Ä¢ Th√≠ch h·ª£p cho c√°c b√†i to√°n smooth v√† well-conditioned")
    
    return all_results


def main():
    """Main function"""
    results = run_comparison_experiments()
    return results


if __name__ == "__main__":
    main()