# Ridge Regression - Kiáº¿n Thá»©c ToÃ¡n Há»c & Setups

## ğŸ“š **LÃ½ Thuyáº¿t ToÃ¡n Há»c**

### **Äá»‹nh NghÄ©a CÆ¡ Báº£n**
Ridge Regression thÃªm L2 regularization vÃ o linear regression Ä‘á»ƒ prevent overfitting vÃ  handle multicollinearity.

**Objective function:**
```
f(w) = (1/2n) ||Xw - y||Â² + (Î»/2) ||w||Â²
```

Trong Ä‘Ã³:
- `(1/2n) ||Xw - y||Â²`: Original MSE loss
- `(Î»/2) ||w||Â²`: L2 regularization penalty
- `Î» â‰¥ 0`: Regularization parameter

### **Closed-Form Solution**
Ridge regression cÃ³ analytical solution:
```
w* = (X^T X + Î»I)^{-1} X^T y
```

**So vá»›i OLS:** `w* = (X^T X)^{-1} X^T y`

### **Gradient vÃ  Hessian**
**Gradient:**
```
âˆ‡f(w) = (1/n) X^T (Xw - y) + Î»w
```

**Hessian:**
```
âˆ‡Â²f(w) = (1/n) X^T X + Î»I
```

**Key insight:** Hessian luÃ´n positive definite khi Î» > 0!

### **Regularization Effect**
L2 penalty "shrinks" weights toward zero:
- **Small Î»**: Gáº§n nhÆ° OLS, risk overfitting
- **Large Î»**: Weights gáº§n 0, risk underfitting
- **Optimal Î»**: Balance bias-variance tradeoff

## ğŸ¯ **CÃ¡c Setup vÃ  Ã NghÄ©a**

### **1. Light Regularization Setup**
```python
lambda_reg = 0.01         # Small penalty
solver = "analytical"     # Closed-form solution
normalize_features = True # Important for Ridge
```

**ğŸ§  CÃ¡ch nhá»›:**
- "0.01 = 1% penalty trÃªn weights"
- "Analytical = tÃ­nh exact solution"
- "Normalize vÃ¬ Ridge sensitive to scale"

**âš–ï¸ Trade-offs:**
- âœ… Fast convergence (closed-form)
- âœ… Minimal bias introduced
- âœ… Good for well-conditioned problems
- âŒ May not prevent overfitting enough
- âŒ Still sensitive to multicollinearity

### **2. Standard Regularization Setup**
```python
lambda_reg = 1.0          # Moderate penalty
solver = "gradient_descent" # Iterative method
max_iterations = 1000     # Sufficient for convergence
learning_rate = 0.01      # Standard GD rate
```

**ğŸ§  CÃ¡ch nhá»›:**
- "Î» = 1.0 = equal weight MSE vs regularization"
- "GD khi matrix too large for inversion"
- "1000 iterations for safety"

**âš–ï¸ Trade-offs:**
- âœ… Good bias-variance balance
- âœ… Handles multicollinearity well
- âœ… Scalable to large datasets
- âŒ Requires hyperparameter tuning
- âŒ Slower than analytical

### **3. Strong Regularization Setup**
```python
lambda_reg = 100.0        # Heavy penalty
solver = "coordinate_descent" # Efficient for regularized
alpha_grid = [0.1, 1, 10, 100] # Cross-validation
cv_folds = 5              # K-fold validation
```

**ğŸ§  CÃ¡ch nhá»›:**
- "Î» = 100 = regularization dominates"
- "Coordinate descent efficient for Ridge"
- "Grid search Ä‘á»ƒ tÃ¬m optimal Î»"

**âš–ï¸ Trade-offs:**
- âœ… Strong overfitting prevention
- âœ… Very stable solutions
- âœ… Automatic Î» selection
- âŒ High bias, may underfit
- âŒ Computational overhead from CV

## ğŸ“Š **Regularization Parameter Deep Dive**

### **Lambda Selection Strategies**

| Strategy | Method | Pros | Cons |
|----------|--------|------|------|
| **Fixed small** | Î» = 0.01 | Fast, simple | May overfit |
| **Fixed moderate** | Î» = 1.0 | Good default | Not optimal |
| **Cross-validation** | Grid search | Optimal for data | Expensive |
| **Analytical** | GCV/AIC/BIC | Fast approximation | Asymptotic |

### **Lambda Effects Visualization**
```
Î» = 0:     â”€â”€â”€â”€â”€â”€â”€â”€â—‹ (OLS, possible overfitting)
Î» = 0.01:  â”€â”€â”€â”€â”€â”€â—‹   (Light shrinkage)
Î» = 1.0:   â”€â”€â”€â”€â—‹     (Moderate shrinkage)  
Î» = 100:   â—‹         (Heavy shrinkage)
Î» = âˆ:     â€¢         (All weights â†’ 0)
```

### **Bias-Variance Tradeoff**
```
BiasÂ²    = ||E[Åµ] - w*||Â²     (Increases with Î»)
Variance = E[||Åµ - E[Åµ]||Â²]   (Decreases with Î»)
MSE      = BiasÂ² + Variance + ÏƒÂ²
```

**Optimal Î» minimizes total MSE**

## ğŸ§® **Mathematical Properties**

### **Condition Number Improvement**
Original: `Îº(X^T X) = Î»_max / Î»_min`
Ridge: `Îº(X^T X + Î»I) = (Î»_max + Î») / (Î»_min + Î»)`

**Effect:** Ridge dramatically improves conditioning!

### **Effective Degrees of Freedom**
```
df(Î») = tr(X(X^T X + Î»I)^{-1} X^T) = Î£ Ïƒáµ¢Â²/(Ïƒáµ¢Â² + Î»)
```
- `Ïƒáµ¢`: singular values of X
- `df(0) = p` (OLS)
- `df(âˆ) = 0` (null model)

### **Shrinkage Factors**
Each principal component shrunk by factor:
```
sáµ¢ = Ïƒáµ¢Â²/(Ïƒáµ¢Â² + Î»)
```
- Large Ïƒáµ¢ (important directions): Less shrinkage
- Small Ïƒáµ¢ (noise directions): More shrinkage

## ğŸ¯ **Solver Comparison**

### **Analytical vs Iterative**

| Aspect | Analytical | Gradient Descent | Coordinate Descent |
|--------|------------|------------------|--------------------|
| **Speed** | O(nÂ³) one-shot | O(knÂ²) iterations | O(kn) iterations |
| **Memory** | O(nÂ²) | O(n) | O(n) |
| **Accuracy** | Exact | Approximate | Approximate |
| **Scale limit** | n < 10K | Any n | Any n |

### **When to Use Each Solver**

```python
if n_features < 1000:
    solver = "analytical"     # Fast and exact
elif sparse_data:
    solver = "coordinate_descent"  # Efficient for sparse
else:
    solver = "gradient_descent"    # General purpose
```

## ğŸ”§ **Implementation Details**

### **Feature Normalization**
```python
# Essential for Ridge!
X_normalized = (X - X.mean(axis=0)) / X.std(axis=0)

# Why? Ridge penalty ||w||Â² treats all features equally
# Without normalization: features with large scale get unfairly penalized
```

### **Analytical Solution Implementation**
```python
def ridge_analytical(X, y, lambda_reg):
    n, p = X.shape
    
    # Add regularization to normal equations
    A = X.T @ X + lambda_reg * np.eye(p)
    b = X.T @ y
    
    # Solve linear system (more stable than inversion)
    weights = np.linalg.solve(A, b)
    return weights
```

### **Cross-Validation for Lambda**
```python
def ridge_cv(X, y, lambda_grid, cv_folds=5):
    best_lambda = None
    best_score = float('inf')
    
    for lambda_reg in lambda_grid:
        cv_scores = []
        for train_idx, val_idx in kfold_split(X, cv_folds):
            # Train on fold
            w = ridge_fit(X[train_idx], y[train_idx], lambda_reg)
            # Validate
            val_error = mse(X[val_idx] @ w, y[val_idx])
            cv_scores.append(val_error)
        
        avg_score = np.mean(cv_scores)
        if avg_score < best_score:
            best_score = avg_score
            best_lambda = lambda_reg
    
    return best_lambda
```

## ğŸ§  **Memory Aids & Intuition**

### **Ridge vs Housebuilding Analogy**
```
OLS = XÃ¢y nhÃ  khÃ´ng cÃ³ building code
- Tá»± do complete, cÃ³ thá»ƒ unstable
- Perfect fit to current data
- Risk collapse vá»›i new data

Ridge = XÃ¢y nhÃ  vá»›i safety regulations  
- Ãt freedom hÆ¡n nhÆ°ng safer
- Small trade-off in current fit
- Much more stable vá»›i new conditions
```

### **Lambda nhÆ° Volume Control**
```
Î» = 0:    ğŸ”ŠğŸ”ŠğŸ”Š (Full volume, possible noise)
Î» = 0.01: ğŸ”ŠğŸ”Š   (Slight reduction)  
Î» = 1.0:  ğŸ”Š     (Moderate volume)
Î» = 100:  ğŸ”ˆ     (Very quiet)
Î» = âˆ:    ğŸ”‡     (Muted)
```

### **Regularization Path Visualization**
```
No Reg:    wâ‚=5.2, wâ‚‚=-3.1, wâ‚ƒ=8.7  (Large weights)
Light:     wâ‚=4.8, wâ‚‚=-2.9, wâ‚ƒ=8.1  (Slight shrinkage)
Moderate:  wâ‚=3.2, wâ‚‚=-1.8, wâ‚ƒ=5.4  (Clear shrinkage)
Heavy:     wâ‚=0.8, wâ‚‚=-0.4, wâ‚ƒ=1.2  (Heavy shrinkage)
```

## ğŸ” **Troubleshooting Guide**

### **Common Issues & Solutions**

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Still overfitting** | High val error vs train | Increase Î» |
| **Underfitting** | Both train/val error high | Decrease Î» |
| **Numerical instability** | Weird weight values | Check feature scaling |
| **Poor performance** | Ridge worse than OLS | Check if regularization needed |

### **Diagnostic Checks**
```python
# 1. Check conditioning improvement
Îº_original = np.linalg.cond(X.T @ X)
Îº_ridge = np.linalg.cond(X.T @ X + lambda_reg * np.eye(p))
print(f"Condition number: {Îº_original:.2e} â†’ {Îº_ridge:.2e}")

# 2. Check effective degrees of freedom
eigenvals = np.linalg.eigvals(X.T @ X)
df_eff = np.sum(eigenvals / (eigenvals + lambda_reg))
print(f"Effective DF: {df_eff:.1f}/{p}")

# 3. Check weight shrinkage
w_ols = ols_fit(X, y)
w_ridge = ridge_fit(X, y, lambda_reg)
shrinkage = np.linalg.norm(w_ridge) / np.linalg.norm(w_ols)
print(f"Weight shrinkage: {shrinkage:.3f}")
```

### **Lambda Selection Guidelines**
```python
# Rule of thumb starting points
n, p = X.shape

if n >> p:  # Many samples
    lambda_start = 0.01
elif n â‰ˆ p:  # Equal samples/features  
    lambda_start = 1.0
else:  # Few samples (n < p)
    lambda_start = 10.0

# Then use CV to refine
lambda_grid = lambda_start * np.logspace(-2, 2, 50)
```

## ğŸ“ˆ **Advanced Topics**

### **Bayesian Interpretation**
Ridge regression equivalent to MAP estimation vá»›i Gaussian prior:
```
w ~ N(0, ÏƒÂ²/Î» I)
y|w ~ N(Xw, ÏƒÂ²I)
```

### **Kernel Ridge Regression**
For non-linear problems:
```
w* = X^T (XX^T + Î»I)^{-1} y
f(x_new) = k(x_new, X) (K + Î»I)^{-1} y
```

### **Group Ridge Regression**
Penalize groups of features together:
```
Penalty = Î» Î£â±¼ ||wâ±¼||â‚‚
```

## ğŸ“– **Connections to Other Methods**

### **Relationship to SVD**
```
X = UÎ£V^T  (SVD decomposition)
w_ridge = V D_Î» U^T y
where D_Î» = diag(Ïƒáµ¢/(Ïƒáµ¢Â² + Î»))
```

### **Connection to PCA**
Ridge regression shrinks along principal components:
- High variance directions: Less shrinkage
- Low variance directions: More shrinkage

### **Limiting Cases**
```
Î» â†’ 0:   Ridge â†’ OLS
Î» â†’ âˆ:   Ridge â†’ Zero weights
```

---

*"Ridge Regression: Add just enough constraint to stabilize the solution without losing too much fit"* âš–ï¸ğŸ”ï¸âœ¨