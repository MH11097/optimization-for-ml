# BFGS (Quasi-Newton) Methods - Ki·∫øn Th·ª©c To√°n H·ªçc & Setups

## üìö **L√Ω Thuy·∫øt To√°n H·ªçc**

### **ƒê·ªãnh Nghƒ©a C∆° B·∫£n**
BFGS (Broyden-Fletcher-Goldfarb-Shanno) l√† quasi-Newton method approximates Hessian matrix thay v√¨ t√≠nh tr·ª±c ti·∫øp.

**C√¥ng th·ª©c c·∫≠p nh·∫≠t:**
```
x_{k+1} = x_k - Œ±_k B_k^{-1} ‚àáf(x_k)
```

Trong ƒë√≥:
- `x_k`: ƒëi·ªÉm hi·ªán t·∫°i
- `Œ±_k`: step size (th∆∞·ªùng t·ª´ line search)
- `B_k`: approximate Hessian matrix
- `‚àáf(x_k)`: gradient t·∫°i x_k

### **BFGS Update Formula**
Thay v√¨ t√≠nh Hessian tr·ª±c ti·∫øp, BFGS updates approximation:

**Secant condition:**
```
B_{k+1} s_k = y_k
```

Trong ƒë√≥:
- `s_k = x_{k+1} - x_k` (step vector)
- `y_k = ‚àáf(x_{k+1}) - ‚àáf(x_k)` (gradient change)

**BFGS Update:**
```
B_{k+1} = B_k + (y_k y_k^T)/(y_k^T s_k) - (B_k s_k s_k^T B_k)/(s_k^T B_k s_k)
```

### **Sherman-Morrison-Woodbury Formula**
Thay v√¨ update B_k, ta update H_k = B_k^{-1}:
```
H_{k+1} = (I - œÅ_k s_k y_k^T) H_k (I - œÅ_k y_k s_k^T) + œÅ_k s_k s_k^T
```

Trong ƒë√≥: `œÅ_k = 1/(y_k^T s_k)`

### **Convergence Properties**
- **Superlinear convergence**: Faster than linear, slower than quadratic
- **Global convergence** v·ªõi line search
- **Finite termination** cho quadratic functions (nh∆∞ Newton)
- **Memory efficient**: O(n¬≤) thay v√¨ O(n¬≥) nh∆∞ Newton

## üéØ **C√°c Setup v√† √ù Nghƒ©a**

### **1. Standard Setup**
```python
max_iterations = 100       # More than Newton
tolerance = 1e-6          # Good precision
line_search = True        # Essential for stability
initial_hessian = "identity"  # Simple start
```

**üß† C√°ch nh·ªõ:**
- "100 iterations v√¨ c·∫ßn build up Hessian approximation"
- "Line search nh∆∞ GPS ƒë·ªÉ t√¨m optimal step"
- "Identity matrix = starting from scratch"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ No Hessian computation needed
- ‚úÖ Better than GD, cheaper than Newton
- ‚úÖ Good for medium-sized problems
- ‚ùå Still needs O(n¬≤) memory
- ‚ùå Slower initial convergence

### **2. Fast Setup**
```python
max_iterations = 50        # Fewer iterations
tolerance = 1e-5          # Slightly relaxed
line_search = "backtracking"  # Simple line search
initial_hessian = "scaled_identity"  # Better start
```

**üß† C√°ch nh·ªõ:**
- "50 iterations v·ªõi better initialization"
- "Backtracking nh∆∞ trial-and-error for step size"
- "Scaled identity = educated guess"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ Faster convergence initially
- ‚úÖ Good for prototyping
- ‚ùå May not reach high precision
- ‚ùå Depends on good scaling

### **3. Robust Setup**
```python
max_iterations = 200       # Many iterations allowed
tolerance = 1e-8          # High precision
line_search = "strong_wolfe"  # Rigorous conditions
initial_hessian = "diagonal"  # Feature-based scaling
restart_frequency = 50     # Periodic restart
```

**üß† C√°ch nh·ªõ:**
- "200 iterations for worst-case scenarios"
- "Strong Wolfe = bulletproof line search"
- "Diagonal initialization considers feature scales"
- "Restart every 50 = fresh start prevention"

**‚öñÔ∏è Trade-offs:**
- ‚úÖ Very robust and reliable
- ‚úÖ Handles ill-conditioned problems
- ‚úÖ High final precision
- ‚ùå More computational overhead
- ‚ùå Complex parameter tuning

## üìä **Line Search Deep Dive**

### **Why Line Search?**
BFGS direction c√≥ th·ªÉ kh√¥ng optimal, c·∫ßn find best step size:
```
Œ±* = argmin_Œ± f(x_k + Œ± * d_k)
```

### **Line Search Types**

| Type | Conditions | Cost | Robustness |
|------|------------|------|------------|
| **Backtracking** | Armijo only | Low | Good |
| **Strong Wolfe** | Armijo + Curvature | Medium | Excellent |
| **Exact** | True minimum | High | Perfect |

### **Armijo Condition**
```
f(x_k + Œ± d_k) ‚â§ f(x_k) + c‚ÇÅ Œ± ‚àáf(x_k)^T d_k
```
- `c‚ÇÅ = 1e-4`: "Sufficient decrease parameter"
- Ensures progress in function value

### **Wolfe Curvature Condition**
```
‚àáf(x_k + Œ± d_k)^T d_k ‚â• c‚ÇÇ ‚àáf(x_k)^T d_k
```
- `c‚ÇÇ = 0.9`: "Curvature parameter"
- Ensures sufficient step size

## üßÆ **Mathematical Properties**

### **Convergence Rate**
**Superlinear convergence:**
```
lim_{k‚Üí‚àû} ||x_{k+1} - x*|| / ||x_k - x*|| = 0
```

**In practice:**
- Iterations 1-10: Linear-like (building Hessian)
- Iterations 10+: Near-quadratic (good approximation)

### **Memory Complexity**

| Operation | BFGS | Newton | GD |
|-----------|------|--------|----| 
| **Memory** | O(n¬≤) | O(n¬≤) | O(n) |
| **Per iteration** | O(n¬≤) | O(n¬≥) | O(n) |
| **Hessian computation** | No | Yes | No |

### **Comparison v·ªõi Other Methods**

| Method | Convergence | Memory | Robustness | Use Case |
|--------|-------------|--------|------------|----------|
| **GD** | Linear | O(n) | High | Large problems |
| **Newton** | Quadratic | O(n¬≤) | Medium | Small problems |
| **BFGS** | Superlinear | O(n¬≤) | High | Medium problems |
| **L-BFGS** | Superlinear | O(m¬∑n) | High | Large problems |

## üéØ **Khi N√†o D√πng BFGS**

### **Ideal Conditions**
- ‚úÖ **Medium-sized problems** (100-10,000 variables)
- ‚úÖ **Smooth functions** (twice differentiable)
- ‚úÖ **Need faster than GD** but cheaper than Newton
- ‚úÖ **Good gradient information** available

### **Avoid When**
- ‚ùå **Very large problems** (use L-BFGS instead)
- ‚ùå **Non-smooth functions** (use subgradient methods)
- ‚ùå **Memory constraints** (use GD)
- ‚ùå **Very noisy gradients** (use stochastic methods)

### **Perfect Use Cases**
1. **Logistic Regression** (smooth, medium-sized)
2. **Neural Network Training** (small networks)
3. **Parameter Estimation** (statistical models)
4. **Engineering Optimization** (design problems)

## üîß **Implementation Details**

### **Initial Hessian Strategies**

```python
# 1. Identity matrix (simple)
H_0 = np.eye(n)

# 2. Scaled identity (better)
Œ≥ = (y_0^T s_0) / (y_0^T y_0)
H_0 = Œ≥ * np.eye(n)

# 3. Diagonal scaling (best)
H_0 = np.diag(feature_scales)
```

### **Memory Management**
```python
# Store only necessary vectors
class BFGSState:
    def __init__(self, n):
        self.H = np.eye(n)  # Current inverse Hessian
        self.s_history = []  # Step vectors
        self.y_history = []  # Gradient changes
        
    def update(self, s_k, y_k):
        # BFGS update formula
        rho = 1.0 / (y_k.T @ s_k)
        # ... update H using Sherman-Morrison
```

### **Numerical Stability**
```python
# Check curvature condition
if y_k.T @ s_k > 1e-8:
    # Safe to update
    self.bfgs_update(s_k, y_k)
else:
    # Skip update or use damped BFGS
    self.skip_update()
```

## üß† **Memory Aids & Intuition**

### **BFGS vs Detective Analogy**
```
Gradient Descent = Rookie detective
- Ch·ªâ c√≥ basic clues (gradient)
- T·ª´ t·ª´ thu th·∫≠p evidence

Newton Method = Experienced detective v·ªõi lab
- C√≥ full forensic analysis (Hessian)
- Expensive nh∆∞ng accurate

BFGS = Smart detective v·ªõi experience
- Learn from previous cases (update approximation)
- Efficient v·ªõi accumulated knowledge
```

### **Line Search nh∆∞ GPS Navigation**
```
No line search = ƒêi theo h∆∞·ªõng m√† kh√¥ng care distance
- C√≥ th·ªÉ overshoot ho·∫∑c understep

With line search = GPS t√≠nh optimal distance
- Take right direction with right step size
- Much more efficient journey
```

### **Convergence Pattern**
```
Phase 1 (0-10 iter):  üê¢ Learning the landscape
Phase 2 (10-30 iter): üê∞ Good approximation built
Phase 3 (30+ iter):   üöÄ Near-Newton performance
```

## üîç **Troubleshooting Guide**

### **Common Issues & Solutions**

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Slow initial convergence** | High cost first 10 iterations | Normal, wait for Hessian buildup |
| **Line search failures** | Œ± becomes very small | Check gradient accuracy |
| **Memory issues** | Out of RAM | Switch to L-BFGS |
| **Poor final accuracy** | Stalls before tolerance | Check conditioning |

### **Diagnostic Checks**
```python
# 1. Check curvature condition
if y_k.T @ s_k <= 0:
    print("Warning: Negative curvature detected")
    
# 2. Monitor condition number
Œ∫ = np.linalg.cond(H_k)
if Œ∫ > 1e12:
    print("Warning: Ill-conditioned Hessian approximation")
    
# 3. Check step sizes
if Œ± < 1e-8:
    print("Warning: Very small steps, possible convergence")
```

### **Performance Tuning**
```python
# 1. Adjust line search parameters
c1 = 1e-4  # Decrease for more aggressive steps
c2 = 0.9   # Increase for fewer line search iterations

# 2. Restart strategy
if iteration % restart_freq == 0:
    H = np.eye(n)  # Reset to identity
    
# 3. Scaling
if problem_has_different_scales:
    use_diagonal_initialization()
```

## üìà **Advanced Variants**

### **L-BFGS (Limited Memory)**
For large problems, store only m recent (s,y) pairs:
```python
# Memory: O(m¬∑n) instead of O(n¬≤)
m = 10  # Typical value
history_size = min(m, iteration)
```

### **Damped BFGS**
For robustness, interpolate with previous Hessian:
```python
if y_k.T @ s_k < threshold:
    y_k_damped = Œ∏ * y_k + (1-Œ∏) * B_k @ s_k
```

### **BFGS with Trust Region**
Combine with trust region for global convergence:
```python
step = solve_trust_region(B_k, gradient, radius)
```

## üìñ **Further Reading**

### **Key Papers**
1. **Broyden (1970)**: Original quasi-Newton idea
2. **Fletcher (1970)**: BFGS derivation
3. **Nocedal & Wright**: Numerical Optimization textbook

### **Modern Variations**
- **L-BFGS**: Limited memory version
- **BFGS-B**: Box-constrained version
- **Hessian-free methods**: For very large problems

---

*"BFGS: Learn the curvature as you go, combining the best of gradient descent and Newton methods"* üß†‚ö°üéØ