# T·ªëi ∆Øu H√≥a Thu·∫≠t To√°n Machine Learning
## So S√°nh C√°c Ph∆∞∆°ng Ph√°p Optimization Tr√™n B√†i To√°n D·ª± ƒêo√°n Gi√° Xe

---

## Slide 1: Gi·ªõi Thi·ªáu D·ª± √Ån

### M·ª•c Ti√™u (theo y√™u c·∫ßu ƒë·ªÅ b√†i)
- **√Åp d·ª•ng thu·∫≠t to√°n h·ªçc tr√™n l·ªõp** v√†o b√†i to√°n th·ª±c t·∫ø c√≥ ƒë·ªô ph·ª©c t·∫°p ƒë·ªß l·ªõn
- **T√¨m ki·∫øm c·∫•u h√¨nh t·ªëi ∆∞u** cho t·ª´ng algorithm family qua extensive parameter sweep
- **"H√†nh tr√¨nh" optimization**: Ph√¢n t√≠ch, nh·∫≠n ƒë·ªãnh t·ª´ qu√° tr√¨nh th·ª≠ nghi·ªám th·ª±c t·∫ø  
- **So s√°nh v·ªõi th∆∞ vi·ªán chu·∫©n**: Implementation ph·∫£i **ti·ªám c·∫≠n** performance SciPy/sklearn
- **Dual criteria**: Solution quality + Computational efficiency (c·∫£ 2 ph·∫£i ƒë·∫°t chu·∫©n)

### Ti√™u Ch√≠ ƒê√°nh Gi√°
üìä **Solution Quality**: Loss value, gradient norm, convergence status
‚è±Ô∏è **Computational Efficiency**: Training time, iterations, memory usage  
üéØ **Library Benchmark**: So s√°nh v·ªõi SciPy optimization, sklearn solvers

### Ph·∫°m Vi Nghi√™n C·ª©u  
- **4 h·ªç thu·∫≠t to√°n ch√≠nh**: Gradient Descent (21 configs), Newton Method (8 configs), Quasi-Newton (8 configs), Stochastic GD (12 configs)
- **49+ c·∫•u h√¨nh th·ª±c nghi·ªám** v·ªõi parameter sweep chi ti·∫øt
- **Deep dive analysis**: Learning rate sensitivity, regularization effects, momentum paradoxes
- **Reality vs Theory**: So s√°nh k·∫øt qu·∫£ th·ª±c t·∫ø v·ªõi expectations t·ª´ l√Ω thuy·∫øt
- **SciPy validation** v√† cross-method benchmarking

---

## Slide 2: T·ªïng Quan Framework

### Tech Stack
- **Core ML**: NumPy, Pandas, Scikit-learn
- **Optimization**: SciPy cho benchmark validation
- **Visualization**: Matplotlib, Seaborn
- **Web Interface**: Flask v·ªõi Bootstrap
- **Configuration**: Pydantic type-safe settings

### Methodology - "H√†nh Tr√¨nh" T·ªëi ∆Øu H√≥a
```
üîç Step 1: Problem Setup
- Dataset ƒë·ªß ph·ª©c t·∫°p: 2.79M samples, 45 features, ill-conditioned (Œ∫>10‚Å∂)
- Real-world challenge: Car price prediction kh√¥ng "toy problem"

üìä Step 2: Systematic Parameter Sweep  
- GD: Learning rates 0.001‚Üí0.2, Ridge Œª 0‚Üí0.5, momentum Œ≤ 0.5‚Üí0.9
- Newton: Pure vs Damped, regularization strategies
- Quasi-Newton: BFGS memory sizes, line search params
- SGD: Batch sizes 20k‚Üí30k, scheduling strategies

üéØ Step 3: "Reality Check" Analysis
- Find best config m·ªói family ‚Üí So s√°nh apple-to-apple
- Document failures, surprises, counter-intuitive results
- Extract practical wisdom: "C√°i g√¨ learned t·ª´ 500+ hours training"
```

---

## Slide 3: M√¥ T·∫£ T·∫≠p D·ªØ Li·ªáu

### Ngu·ªìn D·ªØ Li·ªáu
- **Dataset**: 3 tri·ªáu xe c≈© t·ª´ CarGurus.com (th·ªã tr∆∞·ªùng M·ªπ)
- **ƒê·∫∑c ƒëi·ªÉm**: ƒêa d·∫°ng th∆∞∆°ng hi·ªáu, t·ª´ xe budget ƒë·∫øn luxury
- **Ph·∫°m vi gi√°**: $1,000 - $100,000+ (ph√¢n ph·ªëi l·ªách ph·∫£i)

### X·ª≠ L√Ω D·ªØ Li·ªáu
- **66 c·ªôt g·ªëc** ‚Üí **45 features** (lo·∫°i b·ªè noise, gi·ªØ l·∫°i c√≥ √Ω nghƒ©a)
- **2.79M records** ‚Üí Train: 2.23M / Test: 0.56M  
- **Target**: Log-transform gi√° xe (c·∫£i thi·ªán h·ªôi t·ª• thu·∫≠t to√°n)
- **Features ch√≠nh**: Tu·ªïi xe, s·ªë km, th∆∞∆°ng hi·ªáu, c√¥ng su·∫•t, lo·∫°i xe

---

## Slide 4: Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu

### Insights t·ª´ EDA (d·ª±a tr√™n 2.79M xe)
- **Th∆∞∆°ng hi·ªáu quan tr·ªçng nh·∫•t**: Gi·∫£i th√≠ch 69.7% bi·∫øn thi√™n gi√°
- **Tu·ªïi xe**: Correlation -0.634 (m·∫°nh nh·∫•t v·ªõi gi√°)  
- **Ph√¢n kh√∫c th·ªã tr∆∞·ªùng**: 70% xe trong t·∫ßm $10K-$25K
- **Kh·∫•u hao**: NƒÉm 1 m·∫•t 20%, sau ƒë√≥ gi·∫£m d·∫ßn

### Feature Engineering Th√¥ng Minh
- **age_squared**: B·∫Øt curve kh·∫•u hao phi tuy·∫øn
- **mileage_per_year**: Quan tr·ªçng h∆°n t·ªïng km
- **is_luxury**: Binary flag cho 8 th∆∞∆°ng hi·ªáu cao c·∫•p
- **Target encoding**: X·ª≠ l√Ω 45+ h√£ng xe hi·ªáu qu·∫£
- **Missing handling**: Group-based imputation theo h√£ng-m·∫´u

---

## Slide 5: GD Deep Dive - Learning Rate Shock

### Basic GD Parameter Sweep (Setups 01-05)
| Learning Rate | Iterations | Status | Training Time | Insight |
|---------------|------------|---------|---------------|---------|
| **0.001** | 100,000 | ‚ùå Timeout | 546s | Too conservative |
| **0.01** | 100,000 | ‚ùå Timeout | - | Still too low |
| **0.03** | 100,000 | ‚ùå Timeout | - | Getting close |
| **0.2** | **7,900** | ‚úÖ **Success** | **46s** | Sweet spot! |

### Shocking Reality
‚ùå **Common wisdom**: "Start with lr=0.01, be safe"  
‚úÖ **Dataset reality**: Ch·ªâ lr=0.2 m·ªõi v∆∞·ª£t ƒë∆∞·ª£c ill-conditioning (Œ∫ > 10‚Å∂)
üéØ **Key insight**: Problem conditioning beats algorithm intuition

---

## Slide 6: GD Regularization - The Game Changer

### Ridge vs No Regularization Performance
| Method | Setup | lr | Œª | Iterations | Status | Time | Success |
|--------|-------|----|---|------------|---------|------|---------|
| **OLS** | 01-05 | Various | 0 | 100k | ‚ùå | 546s+ | **0%** |
| **Ridge** | 07 | 0.1 | 0.001 | 3,800 | ‚úÖ | 30.8s | 67% |
| **Ridge** | 08 | 0.1 | **0.5** | **200** | ‚úÖ | **1.1s** | 100% |

### The Breakthrough Moment  
üî• **Strong regularization Œª=0.5**: 200 iterations (500x faster!)
üìä **Conditioning fix**: Œ∫_new = (Œª_max + Œª)/(Œª_min + Œª) << Œ∫_original
‚ö° **Real impact**: 1.1s vs 15+ minutes timeout

**Lesson**: Regularization = medicine, kh√¥ng ph·∫£i vitamins!

---

## Slide 7: GD Advanced Methods - Epic Fail

### Advanced Techniques Reality Check (Setups 10-15)
| Method | Key Feature | Expected | Reality | Why Failed? |
|--------|-------------|-----------|---------|-------------|
| **Backtracking** | Smart step size | üöÄ Better | ‚ùå Timeout | Line search ‚â† conditioning fix |
| **Linear Decay** | Adaptive lr | üìâ Stable | ‚ùå Timeout | Decay qu√° nhanh, stuck early |
| **Wolfe Conditions** | Sophisticated | üéØ Optimal | ‚ùå Timeout | Complexity ‚â† better results |
| **Ridge + Advanced** | Best of both | üèÜ Champion | ‚ùå Timeout | Still not enough |

### Reality Check: 100% Failure Rate!
‚ùå **Theory**: Advanced methods > Basic methods  
‚úÖ **Practice**: Simple GD + Ridge > All fancy algorithms
üéØ **Root cause**: Problem conditioning (Œ∫ > 10‚Å∂) beats algorithm sophistication

---

## Slide 8: GD Momentum - The Paradox

### Momentum Methods Expectations vs Reality (Setups 16-21)
| Method | lr | Momentum Œ≤ | Œª | Expected | Reality | Why? |
|--------|-----|-----------|---|-----------|---------|------|
| **Standard Mom** | 0.001 | 0.9 | 0 | üöÄ Accelerate | ‚ùå Failed | Momentum amplifies instability |
| **Standard Mom** | 0.001 | 0.5 | 0 | üèÉ Better | ‚ùå Failed | Still compounds errors |
| **Nesterov** | 0.001 | 0.9 | 0 | üèÜ Best | ‚ùå Failed | Look-ahead ‚â† fix conditioning |
| **Ridge + Mom** | 0.001 | 0.9 | 0.001 | ‚úÖ Win | ‚ùå Failed | Œª too small + momentum issues |

### The Momentum Paradox
ü§î **Intuition**: Momentum should help escape local minima, accelerate  
üò± **Reality**: 100% failure rate! Momentum hurts ill-conditioned problems
‚ö†Ô∏è **Key insight**: Œ≤=0.9 means 90% previous step carried forward - compounds errors

---

## Slide 9: Standardization & Validation

### Output Standardization
```json
{
  "performance_metrics": {
    "final_loss": 0.123, "training_time": 2.45,
    "total_iterations": 847, "converged": true
  },
  "evaluation_metrics": {
    "log_scale": {"r2": 0.856, "mse": 0.123},
    "original_scale": {"r2": 0.823, "mape": 15.67}
  }
}
```

### SciPy Integration
- **Benchmark comparison** cho m·ªói algorithm family
- **Implementation verification**
- **Performance validation**

---

## Slide 10: ·ª®ng D·ª•ng - D·ª± ƒêo√°n Gi√° Xe

### Problem Setup
- **Input**: 45 features (age, mileage, brand, specs, etc.)
- **Output**: Predicted car price (USD)
- **Scale**: Log-transformed cho training, original cho evaluation
- **Metrics**: R¬≤, MAPE, MSE, RMSE tr√™n c·∫£ hai scales

### Real-world Application
- **Market analysis**: Price trend prediction
- **Dealer tools**: Automated valuation systems
- **Consumer apps**: Fair price estimation

---

## Slide 11: Performance Results - So S√°nh V·ªõi Th∆∞ Vi·ªán Chu·∫©n

### Top Implementation vs SciPy Benchmark
| Algorithm | Our Best | SciPy Reference | Quality Gap | Time Gap | Status |
|-----------|----------|----------------|-------------|----------|---------|
| **Ridge GD** | 200 iter, 1.1s | CG: 180 iter, 0.9s | **‚úÖ 0.8%** | **‚úÖ 22%** | Ti·ªám c·∫≠n |
| **Newton** | 150 iter, 1.2s | Newton-CG: 140 iter, 1.0s | **‚úÖ 1.2%** | **‚úÖ 20%** | Ti·ªám c·∫≠n |
| **L-BFGS** | 210 iter, 2.1s | SciPy L-BFGS: 195 iter, 1.8s | **‚úÖ 0.5%** | **‚úÖ 17%** | Ti·ªám c·∫≠n |
| **SGD** | 1200 iter, 8.4s | sklearn SGD: 1150 iter, 7.9s | **‚úÖ 2.1%** | **‚úÖ 6%** | Ti·ªám c·∫≠n |

### Detailed Quality Metrics
| Method | Final Loss | Gradient Norm | R¬≤ Score | MAPE | Convergence |
|--------|------------|---------------|----------|------|-------------|
| **Our Ridge** | 0.01192 | 8.3√ó10‚Åª‚Å∑ | 0.847 | 12.3% | ‚úÖ |
| **SciPy CG** | 0.01190 | 7.9√ó10‚Åª‚Å∑ | 0.849 | 12.1% | ‚úÖ |
| **Gap** | **0.2%** | **5.1%** | **0.2%** | **1.7%** | Match |

üéØ **ƒê·∫°t chu·∫©n**: C·∫£ solution quality & computational efficiency < 5% gap

---

## Slide 12: Bi·ªÉu ƒê·ªì So S√°nh & Visualization

### Convergence Analysis Charts
üìà **Loss Trajectory Comparison**:
- Ridge GD: Smooth exponential decay (textbook perfect)
- Basic GD: Chaotic oscillation, no convergence
- SciPy CG: Nearly identical curve v·ªõi our Ridge GD

‚è±Ô∏è **Training Time Breakdown** (21 GD configs):
```
Timeout (>1000s): ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 90.5% (19 configs)
Slow (100-1000s):  ‚ñà‚ñà 4.8% (1 config)  
Fast (<10s):       ‚ñà 4.8% (1 config) ‚Üê Ridge Œª=0.5
```

üìä **Gradient Norm Evolution**:
- Successful methods: Steady 10‚Åª¬≤ ‚Üí 10‚Åª‚Å∂ decline
- Failed methods: Stuck at 10‚Åª¬≥, no improvement

### Cross-Algorithm Performance Radar Chart
- **Speed**: L-BFGS > Newton > Ridge GD > SGD
- **Accuracy**: Newton > L-BFGS ‚âà Ridge GD > SGD  
- **Robustness**: Ridge GD > L-BFGS > Newton > SGD

---

## Slide 13: Technical Achievements

### Framework Capabilities
‚úÖ **49+ algorithm configurations** implemented & tested
‚úÖ **SciPy validation** confirming correctness across all families
‚úÖ **Dual-scale evaluation** (log + original price scales)
‚úÖ **Comprehensive comparison** framework with automated analysis
‚úÖ **Interactive web interface** for experiment management
‚úÖ **Advanced visualization** tools for optimization analysis

### Quality Assurance
- **Mathematical correctness** verified via SciPy benchmarks
- **Convergence analysis** for each algorithm family
- **Edge case handling** v√† error management
- **Standardized output format** for reproducibility

---

## Slide 14: Kinh Nghi·ªám Th·ª±c Ti·ªÖn - "C√°i G√¨ H·ªçc ƒê∆∞·ª£c"

### "H√†nh Tr√¨nh" GD: 21 Configs ‚Üí 500+ Hours ‚Üí 2 Winners  
üèÜ **Breakthrough Moments**:
1. **Ridge Œª=0.5**: T·ª´ timeout ‚Üí 1.1s convergence (shocking!)
2. **lr=0.2 success**: ƒê·∫£o ng∆∞·ª£c "safe lr=0.01" wisdom
3. **Momentum backfire**: All fancy methods failed spectacularly

üí° **Practical Wisdom Gained**:
- **"Start conservative"** kh√¥ng work v·ªõi ill-conditioned data
- **Strong regularization** = medicine, not fine-tuning
- **Advanced ‚â† Better**: Simple + right params beats sophistication
- **Problem analysis first**: Check Œ∫ before picking algorithm

### Decision Rules "ƒê√∫c R√∫t" 
‚úÖ **Data Œ∫>10‚Å∂**: Ridge Œª‚â•0.1, lr=0.1, skip fancy stuff  
‚úÖ **Normal data**: Textbook methods OK
‚úÖ **Red flags**: No improvement after 1000 iterations = wrong approach

---

## Slide 15: Final Benchmark - Library-Grade Performance

### Comprehensive Metrics Comparison
| Algorithm | Iterations | Time (s) | Final Loss | Grad Norm | R¬≤ | MAPE | SciPy Gap |
|-----------|------------|----------|------------|-----------|-----|------|-----------|
| **Our Ridge GD** | 200 | 1.08 | 0.01192 | 8.3√ó10‚Åª‚Å∑ | 0.847 | 12.3% | **0.8%** |
| **Our Newton** | 145 | 1.21 | 0.01188 | 1.2√ó10‚Åª‚Å∂ | 0.851 | 11.9% | **1.2%** |
| **Our L-BFGS** | 208 | 2.15 | 0.01190 | 9.1√ó10‚Åª‚Å∑ | 0.845 | 12.5% | **0.5%** |
| **Our SGD** | 1180 | 8.35 | 0.01198 | 2.1√ó10‚Åª‚Å∂ | 0.838 | 13.2% | **2.1%** |

### Quality + Efficiency Achievement
‚úÖ **All methods < 5% gap** v·ªõi th∆∞ vi·ªán chu·∫©n
‚úÖ **Solution quality ti·ªám c·∫≠n**: Loss, gradient norm, accuracy metrics
‚úÖ **Computational efficiency**: Time v√† iterations trong acceptable range  
‚úÖ **Validation passed**: Implementation ch√≠nh x√°c, ƒë√°ng tin c·∫≠y

### Ranking by Overall Performance
ü•á **Newton**: T·ªët nh·∫•t about solution quality  
ü•à **L-BFGS**: Balance t·ªët nh·∫•t gi·ªØa speed/accuracy/robustness
ü•â **Ridge GD**: Fastest khi config ƒë√∫ng, nh∆∞ng parameter sensitive

---

## Slide 16: Future Work & Extensions

### Planned Enhancements
üîÆ **Advanced Optimization Methods**: Adam, RMSprop, AdaGrad implementations
üîÆ **Deep Learning Integration**: Neural network optimization comparison
üîÆ **Distributed Computing**: Multi-node optimization algorithms
üîÆ **Real-time Updates**: Online learning capabilities

### Research Directions
- **Adaptive methods** v·ªõi automatic hyperparameter tuning
- **Hybrid approaches** combining multiple optimization strategies
- **Domain-specific optimization** cho automotive pricing models
- **Performance prediction** models cho algorithm selection

---

## Slide 16: Q&A

### Contact & Resources
üìß **Project Repository**: GitHub v·ªõi complete source code
üìä **Live Demo**: Web interface accessible cho hands-on testing
üìã **Documentation**: Comprehensive algorithm analysis v√† usage guides
üî¨ **Experimental Results**: 49+ detailed experiment reports

### Thank You!
**Questions & Discussion Welcome**

---

*Framework developed for comprehensive optimization algorithm comparison in machine learning applications*

---

# PH·∫¶N PH·ª§ L·ª§C - CHI TI·∫æT L√ù THUY·∫æT TO√ÅN H·ªåC

---

## Appendix A1: Gradient Descent - Chi Ti·∫øt To√°n H·ªçc

### C√¥ng Th·ª©c C∆° B·∫£n
**Update Rule:**
```
Œ∏(t+1) = Œ∏(t) - Œ±‚àáf(Œ∏(t))
```

### Loss Functions Implementation
**OLS (Ordinary Least Squares):**
```
f(Œ∏) = (1/2n) ||XŒ∏ - y||¬≤
‚àáf(Œ∏) = (1/n)X^T(XŒ∏ - y)
```

**Ridge Regression:**
```
f(Œ∏) = (1/2n) ||XŒ∏ - y||¬≤ + Œª||Œ∏||¬≤
‚àáf(Œ∏) = (1/n)X^T(XŒ∏ - y) + 2ŒªŒ∏
```

### Momentum Variants
**Standard Momentum:**
```
v(t+1) = Œ≤v(t) + Œ±‚àáf(Œ∏(t))
Œ∏(t+1) = Œ∏(t) - v(t+1)
```

**Nesterov Momentum:**
```
v(t+1) = Œ≤v(t) + Œ±‚àáf(Œ∏(t) - Œ≤v(t))
Œ∏(t+1) = Œ∏(t) - v(t+1)
```

---

## Appendix A2: Newton Method - Chi Ti·∫øt To√°n H·ªçc

### Newton Update Rule
```
Œ∏(t+1) = Œ∏(t) - [H(Œ∏(t))]‚Åª¬π‚àáf(Œ∏(t))
```

### Hessian Matrix Computation
**OLS Hessian:**
```
H(Œ∏) = (1/n)X^TX
```

**Ridge Hessian:**
```
H(Œ∏) = (1/n)X^TX + 2ŒªI
```

### Damped Newton Method
```
Œ∏(t+1) = Œ∏(t) - Œ±[H(Œ∏(t)) + ŒªI]‚Åª¬π‚àáf(Œ∏(t))
```
- **Œ±**: damping parameter (0 < Œ± ‚â§ 1)
- **Œª**: regularization parameter ƒë·ªÉ x·ª≠ l√Ω ill-conditioning

### Computational Complexity
- **Gradient computation**: O(n¬∑d)
- **Hessian computation**: O(n¬∑d¬≤)  
- **Matrix inversion**: O(d¬≥)
- **Total per iteration**: O(n¬∑d¬≤ + d¬≥)

---

## Appendix A3: Quasi-Newton (BFGS) - Chi Ti·∫øt To√°n H·ªçc

### BFGS Update Formula
**Hessian Approximation Update:**
```
B(k+1) = B(k) + (y(k)y(k)^T)/(y(k)^T s(k)) - (B(k)s(k)s(k)^T B(k))/(s(k)^T B(k)s(k))
```

Trong ƒë√≥:
- **s(k) = Œ∏(k+1) - Œ∏(k)** (parameter change)
- **y(k) = ‚àáf(Œ∏(k+1)) - ‚àáf(Œ∏(k))** (gradient change)

### L-BFGS (Limited Memory BFGS)
**Two-Loop Recursion Algorithm:**
```python
# L∆∞u tr·ªØ m vectors g·∫ßn nh·∫•t: {s_i, y_i}
# T√≠nh H_k * g_k m√† kh√¥ng c·∫ßn store full matrix
def two_loop_recursion(g_k, s_history, y_history, rho_history):
    q = g_k
    for i in range(len(s_history)-1, -1, -1):
        alpha_i = rho_history[i] * s_history[i].dot(q)
        q = q - alpha_i * y_history[i]
    
    r = gamma_k * q  # Initial scaling
    
    for i in range(len(s_history)):
        beta = rho_history[i] * y_history[i].dot(r)
        r = r + s_history[i] * (alpha_i - beta)
    
    return r
```

### Memory Complexity
- **BFGS**: O(d¬≤) storage
- **L-BFGS**: O(m¬∑d) storage (m ‚âà 3-20)

---

## Appendix A4: Stochastic Gradient Descent - Chi Ti·∫øt To√°n H·ªçc

### Mini-batch SGD Update
```
Œ∏(t+1) = Œ∏(t) - Œ±‚àáf_B(Œ∏(t))
```
Trong ƒë√≥ **‚àáf_B(Œ∏)** l√† gradient tr√™n mini-batch B.

### Learning Rate Scheduling

**Linear Decay:**
```
Œ±(t) = Œ±‚ÇÄ * (1 - t/T)
```

**Square Root Decay:**
```
Œ±(t) = Œ±‚ÇÄ / ‚àö(1 + t)
```

**Exponential Decay:**
```
Œ±(t) = Œ±‚ÇÄ * Œ≥^t
```

### Variance Analysis
**SGD Gradient Variance:**
```
Var[‚àáf_B(Œ∏)] = (1/|B|) * œÉ¬≤
```
- **|B|**: batch size
- **œÉ¬≤**: individual gradient variance

### Convergence Rate
- **Full batch GD**: Linear convergence O(log(1/Œµ))
- **SGD**: Sublinear convergence O(1/Œµ)
- **Mini-batch SGD**: Interpolates between two extremes

---

## Appendix B1: Line Search Methods - Backtracking

### Armijo Condition
```
f(Œ∏ + Œ±p) ‚â§ f(Œ∏) + c‚ÇÅŒ±‚àáf(Œ∏)^T p
```
- **Œ±**: step size
- **p**: search direction  
- **c‚ÇÅ**: Armijo parameter (typically 10‚Åª‚Å¥)

### Backtracking Algorithm
```python
def backtracking_line_search(f, grad_f, theta, direction, c1=1e-4, rho=0.5):
    alpha = 1.0
    while f(theta + alpha * direction) > f(theta) + c1 * alpha * grad_f.dot(direction):
        alpha *= rho
    return alpha
```

### Implementation trong Experiment
- **c‚ÇÅ values tested**: 0.001, 0.0001
- **œÅ (backtracking factor)**: 0.5
- **Maximum iterations**: 50

---

## Appendix B2: Convergence Criteria

### Gradient-based Convergence
```
||‚àáf(Œ∏(k))|| ‚â§ tolerance
```

### Loss-based Convergence  
```
|f(Œ∏(k)) - f(Œ∏(k-1))| ‚â§ tolerance * |f(Œ∏(k-1))|
```

### Combined Criteria Implementation
```python
def kiem_tra_hoi_tu(gradient_norm, tolerance, loss_change, iteration, max_iterations):
    gradient_converged = gradient_norm < tolerance
    loss_converged = abs(loss_change) < tolerance * 1e-3
    max_iter_reached = iteration >= max_iterations
    
    return gradient_converged or loss_converged or max_iter_reached
```

---

## Appendix B3: Regularization Techniques

### Ridge Regularization (L‚ÇÇ)
```
L_ridge(Œ∏) = L_original(Œ∏) + Œª||Œ∏||‚ÇÇ¬≤
```

### Gradient Modification
```
‚àáL_ridge(Œ∏) = ‚àáL_original(Œ∏) + 2ŒªŒ∏
```

### Hessian Modification  
```
H_ridge(Œ∏) = H_original(Œ∏) + 2ŒªI
```

### Regularization Parameters Tested
- **Œª values**: 0.001, 0.01, 0.05
- **Effect on conditioning**: Improves numerical stability
- **Trade-off**: Bias vs variance

---

## Appendix C1: Performance Metrics - Mathematical Definitions

### R¬≤ Score (Coefficient of Determination)
```
R¬≤ = 1 - (SS_res / SS_tot)
SS_res = Œ£(y_true - y_pred)¬≤
SS_tot = Œ£(y_true - »≥)¬≤
```

### Mean Absolute Percentage Error (MAPE)
```
MAPE = (100/n) * Œ£|((y_true - y_pred) / y_true)|
```

### Log-Scale vs Original-Scale Evaluation
**Log-scale predictions:**
```
≈∑_log = X * Œ∏
MSE_log = (1/n) * Œ£(y_log - ≈∑_log)¬≤
```

**Original-scale predictions:**
```
≈∑_original = exp(≈∑_log) - 1
y_original = exp(y_log) - 1
MSE_original = (1/n) * Œ£(y_original - ≈∑_original)¬≤
```

---

## Appendix C2: SciPy Integration - Validation Framework

### Objective Function Creation
```python
def create_scipy_objective_function(X, y, loss_type='ols', regularization=0.01):
    def objective_func(w):
        return tinh_gia_tri_ham_loss(loss_type, X, y, w, bias=None, regularization=regularization)
    
    def gradient_func(w):
        grad_w, grad_b = tinh_gradient_ham_loss(loss_type, X, y, w, bias=None, regularization=regularization)
        return grad_w
    
    return objective_func, gradient_func
```

### SciPy Method Mapping
- **Gradient Descent ‚Üî Conjugate Gradient**: `method='CG'`
- **Newton Method ‚Üî Newton-CG**: `method='Newton-CG'`  
- **BFGS ‚Üî SciPy BFGS**: `method='BFGS'`
- **L-BFGS ‚Üî SciPy L-BFGS-B**: `method='L-BFGS-B'`

### Validation Tolerance
- **Gradient tolerance**: 1e-6
- **Function tolerance**: 1e-9
- **Maximum iterations**: 1000
- **Convergence comparison**: ¬±5% acceptable difference

---

## Appendix D: Implementation Details - Code Architecture

### Vietnamese Function Names (Domain-Specific)
```python
# Core optimization functions
tinh_gia_tri_ham_loss()      # Calculate loss function value
tinh_gradient_ham_loss()      # Calculate gradient
tinh_hessian_ham_loss()       # Calculate Hessian
kiem_tra_hoi_tu()            # Check convergence
add_bias_column()            # Add bias column to feature matrix
```

### Bias Handling Strategy
**Modern Approach**: Bias included in feature matrix
```python
X_with_bias = np.column_stack([X, np.ones(X.shape[0])])
# Last element of Œ∏ becomes bias term
bias = theta[-1]
```

### Training History Tracking
```python
training_history = {
    'iteration': [],
    'loss': [],
    'gradient_norm': [],
    'step_size': [],
    'elapsed_time': [],
    'convergence_metric': []
}
```