"""
Ti·ªán √≠ch T·ªëi ∆∞u h√≥a - C√°c h√†m c·∫ßn thi·∫øt cho Newton Method

=== M·ª§C ƒê√çCH: T·ªêI ∆ØU H√ìA ===

Bao g·ªìm t·∫•t c·∫£ c√°c h√†m c·∫ßn thi·∫øt cho:
1. T√≠nh to√°n Gradient v√† Hessian
2. Gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh  
3. Ki·ªÉm tra h·ªôi t·ª• v√† line search
4. ƒê√°nh gi√° m√¥ h√¨nh (MSE, MAE, R¬≤)
5. D·ª± ƒëo√°n v√† t√≠nh to√°n loss

Code ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu, d·ªÖ s·ª≠ d·ª•ng.
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Callable, Tuple, Optional, Dict, List


# ==============================================================================
# 0. DATA PREPROCESSING UTILITIES
# ==============================================================================

def add_bias_column(X: np.ndarray) -> np.ndarray:
    """
    Th√™m c·ªôt bias (c·ªôt to√†n s·ªë 1) v√†o cu·ªëi ma tr·∫≠n X
    
    Chuy·ªÉn t·ª´ format: Xw + b = y
    Sang format: X_new @ w_new = y (v·ªõi w_new = [w; b])
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
    
    Tr·∫£ v·ªÅ:
        X_with_bias: ma tr·∫≠n m·ªü r·ªông (n_samples, n_features + 1)
                     v·ªõi c·ªôt cu·ªëi c√πng l√† c·ªôt bias (to√†n s·ªë 1)
    """
    n_samples = X.shape[0]
    bias_column = np.ones((n_samples, 1))
    X_with_bias = np.hstack([X, bias_column])
    return X_with_bias


# ==============================================================================
# 1. T√çNH TO√ÅN GRADIENT V√Ä HESSIAN
# ==============================================================================

def tinh_gradient_hoi_quy_tuyen_tinh(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray, 
                                   he_so_tu_do: float, dieu_chinh: float = 0.0) -> Tuple[np.ndarray, float]:
    """
    T√≠nh vector gradient cho h·ªìi quy tuy·∫øn t√≠nh v·ªõi ƒëi·ªÅu ch·ªânh
    
    H√†m m·ª•c ti√™u: f(w,b) = (1/2n) * ||Xw + b - y||¬≤ + (Œª/2) * ||w||¬≤
    
    Gradient:
    - ‚àÇf/‚àÇw = (1/n) * X^T(Xw + b - y) + Œª * w  
    - ‚àÇf/‚àÇb = (1/n) * Œ£(Xw + b - y)
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        y: vector m·ª•c ti√™u (n_samples,)
        trong_so: tr·ªçng s·ªë hi·ªán t·∫°i (n_features,)
        he_so_tu_do: h·ªá s·ªë t·ª± do hi·ªán t·∫°i (scalar)
        dieu_chinh: h·ªá s·ªë ƒëi·ªÅu ch·ªânh Œª (m·∫∑c ƒë·ªãnh 0.0)
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo tr·ªçng s·ªë (n_features,)
        gradient_b: gradient theo h·ªá s·ªë t·ª± do (scalar)
    """
    so_mau = X.shape[0]
    
    # D·ª± ƒëo√°n hi·ªán t·∫°i: ≈∑ = Xw + b
    du_doan = X @ trong_so + he_so_tu_do
    
    # Sai s·ªë: e = ≈∑ - y
    sai_so = du_doan - y
    
    # Gradient theo tr·ªçng s·ªë: ‚àÇf/‚àÇw = (1/n) * X^T * e + Œª * w
    gradient_w = (1/so_mau) * X.T @ sai_so + dieu_chinh * trong_so
    
    # Gradient theo h·ªá s·ªë t·ª± do: ‚àÇf/‚àÇb = (1/n) * Œ£(e)
    gradient_b = (1/so_mau) * np.sum(sai_so)
    
    return gradient_w, gradient_b


def tinh_ma_tran_hessian_hoi_quy_tuyen_tinh(X: np.ndarray, dieu_chinh: float = 0.0) -> np.ndarray:
    """
    T√≠nh ma tr·∫≠n Hessian cho h·ªìi quy tuy·∫øn t√≠nh v·ªõi ƒëi·ªÅu ch·ªânh
    
    Ma tr·∫≠n Hessian: H = (1/n) * X^T * X + Œª * I
    
    ƒê√¢y l√† ma tr·∫≠n ƒë·∫°o h√†m b·∫≠c 2 c·ªßa h√†m m·ª•c ti√™u.
    ƒê·ªëi v·ªõi h·ªìi quy tuy·∫øn t√≠nh, Hessian l√† h·∫±ng s·ªë (kh√¥ng ph·ª• thu·ªôc v√†o w, b).
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        dieu_chinh: h·ªá s·ªë ƒëi·ªÅu ch·ªânh Œª (m·∫∑c ƒë·ªãnh 0.0)
    
    Tr·∫£ v·ªÅ:
        H: ma tr·∫≠n Hessian (n_features, n_features)
    
    L∆∞u √Ω: 
    - Ma tr·∫≠n n√†y l√† positive semi-definite khi Œª ‚â• 0
    - Khi Œª > 0, ma tr·∫≠n tr·ªü th√†nh positive definite v√† kh·∫£ ngh·ªãch
    """
    so_mau, so_dac_trung = X.shape
    
    # T√≠nh t√≠ch X^T @ X (ma tr·∫≠n Gram)
    XTX = X.T @ X
    
    # Chia cho s·ªë m·∫´u v√† th√™m ƒëi·ªÅu ch·ªânh
    H = (1/so_mau) * XTX + dieu_chinh * np.eye(so_dac_trung)
    
    return H


# ==============================================================================
# 2. GI·∫¢I H·ªÜ PH∆Ø∆†NG TR√åNH V√Ä KI·ªÇM TRA MA TR·∫¨N
# ==============================================================================

def giai_he_phuong_trinh_tuyen_tinh(A: np.ndarray, b: np.ndarray) -> np.ndarray:
    """
    Gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh Ax = b m·ªôt c√°ch an to√†n
    
    S·ª≠ d·ª•ng LU decomposition v·ªõi partial pivoting ƒë·ªÉ ·ªïn ƒë·ªãnh s·ªë h·ªçc.
    
    Tham s·ªë:
        A: ma tr·∫≠n h·ªá s·ªë (n x n)
        b: vector v·∫ø ph·∫£i (n,)
    
    Tr·∫£ v·ªÅ:
        x: nghi·ªám c·ªßa h·ªá ph∆∞∆°ng tr√¨nh (n,)
    
    L∆∞u √Ω: T·ª± ƒë·ªông ki·ªÉm tra ƒëi·ªÅu ki·ªán v√† th√™m regularization n·∫øu c·∫ßn
    """
    try:
        # Th·ª≠ gi·∫£i tr·ª±c ti·∫øp
        return np.linalg.solve(A, b)
    except np.linalg.LinAlgError:
        # N·∫øu ma tr·∫≠n singular, th√™m regularization nh·ªè
        regularization = 1e-10
        A_reg = A + regularization * np.eye(A.shape[0])
        return np.linalg.solve(A_reg, b)


def kiem_tra_positive_definite(matrix: np.ndarray) -> bool:
    """
    Ki·ªÉm tra ma tr·∫≠n c√≥ ph·∫£i positive definite kh√¥ng
    
    Ma tr·∫≠n positive definite khi t·∫•t c·∫£ eigenvalues > 0.
    ƒêi·ªÅu n√†y quan tr·ªçng cho Newton Method.
    
    Tham s·ªë:
        matrix: ma tr·∫≠n c·∫ßn ki·ªÉm tra (n x n)
    
    Tr·∫£ v·ªÅ:
        bool: True n·∫øu positive definite, False n·∫øu kh√¥ng
    """
    try:
        # Th·ª≠ Cholesky decomposition
        np.linalg.cholesky(matrix)
        return True
    except np.linalg.LinAlgError:
        return False


def tinh_condition_number(matrix: np.ndarray) -> float:
    """
    T√≠nh condition number c·ªßa ma tr·∫≠n
    
    Condition number cho bi·∫øt m·ª©c ƒë·ªô ill-conditioned c·ªßa ma tr·∫≠n.
    - G·∫ßn 1: ma tr·∫≠n well-conditioned
    - R·∫•t l·ªõn: ma tr·∫≠n ill-conditioned
    
    Tham s·ªë:
        matrix: ma tr·∫≠n c·∫ßn t√≠nh (n x n)
    
    Tr·∫£ v·ªÅ:
        float: condition number
    """
    return np.linalg.cond(matrix)


# ==============================================================================
# 3. ƒê√ÅNH GI√Å M√î H√åNH V√Ä D·ª∞ ƒêO√ÅN
# ==============================================================================

def du_doan(X: np.ndarray, w: np.ndarray, bias: float = None) -> np.ndarray:
    """
    Th·ª±c hi·ªán d·ª± ƒëo√°n v·ªõi m√¥ h√¨nh tuy·∫øn t√≠nh
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: ≈∑ = Xw + bias (khi bias != None)
    - Format m·ªõi: ≈∑ = Xw (khi X ƒë√£ bao g·ªìm c·ªôt bias v√† w bao g·ªìm bias weight)
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        w: tr·ªçng s·ªë ƒë√£ h·ªçc (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
    
    Tr·∫£ v·ªÅ:
        predictions: d·ª± ƒëo√°n tr√™n log scale (n_samples,)
        
    L∆∞u √Ω: 
        - Model ƒë∆∞·ª£c train tr√™n log-transformed targets
        - Predictions tr·∫£ v·ªÅ ·ªü log scale ƒë·ªÉ consistency
        - S·ª≠ d·ª•ng np.expm1() ƒë·ªÉ chuy·ªÉn v·ªÅ original scale khi c·∫ßn
    """
    if bias is not None:
        # Format c≈©: Xw + bias
        predictions_log = X @ w + bias
    else:
        # Format m·ªõi: Xw (v·ªõi X ƒë√£ bao g·ªìm c·ªôt bias)
        predictions_log = X @ w
    return predictions_log


def tinh_mse(y_that: np.ndarray, y_du_doan: np.ndarray) -> float:
    """
    T√≠nh Mean Squared Error (Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh)
    
    MSE = (1/n) * Œ£(y_th·∫≠t - y_d·ª±_ƒëo√°n)¬≤
    
    Tham s·ªë:
        y_that: gi√° tr·ªã th·∫≠t (n_samples,)
        y_du_doan: gi√° tr·ªã d·ª± ƒëo√°n (n_samples,)
    
    Tr·∫£ v·ªÅ:
        float: MSE
    """
    return np.mean((y_that - y_du_doan) ** 2)


def tinh_mae(y_that: np.ndarray, y_du_doan: np.ndarray) -> float:
    """
    T√≠nh Mean Absolute Error (Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh)
    
    MAE = (1/n) * Œ£|y_th·∫≠t - y_d·ª±_ƒëo√°n|
    
    Tham s·ªë:
        y_that: gi√° tr·ªã th·∫≠t (n_samples,)
        y_du_doan: gi√° tr·ªã d·ª± ƒëo√°n (n_samples,)
    
    Tr·∫£ v·ªÅ:
        float: MAE
    """
    return np.mean(np.abs(y_that - y_du_doan))


def tinh_r2_score(y_that: np.ndarray, y_du_doan: np.ndarray) -> float:
    """
    T√≠nh R¬≤ score (Coefficient of determination)
    
    R¬≤ = 1 - (SS_res / SS_tot)
    Trong ƒë√≥:
    - SS_res = Œ£(y_th·∫≠t - y_d·ª±_ƒëo√°n)¬≤ (residual sum of squares)
    - SS_tot = Œ£(y_th·∫≠t - »≥)¬≤ (total sum of squares)
    
    Tham s·ªë:
        y_that: gi√° tr·ªã th·∫≠t (n_samples,)
        y_du_doan: gi√° tr·ªã d·ª± ƒëo√°n (n_samples,)
    
    Tr·∫£ v·ªÅ:
        float: R¬≤ score (1.0 = perfect, 0.0 = no better than mean)
    """
    ss_res = np.sum((y_that - y_du_doan) ** 2)
    ss_tot = np.sum((y_that - np.mean(y_that)) ** 2)
    
    if ss_tot == 0:
        return 1.0 if ss_res == 0 else 0.0
    
    return 1 - (ss_res / ss_tot)


# ==============================================================================
# 4. NUMERICAL STABILITY CHECKS
# ==============================================================================

def check_for_numerical_issues(gradient_norm: float, loss_value: Optional[float] = None, 
                               weights: Optional[np.ndarray] = None, iteration: int = 0) -> Tuple[bool, str]:
    """
    Ki·ªÉm tra c√°c v·∫•n ƒë·ªÅ v·ªÅ t√≠nh ·ªïn ƒë·ªãnh s·ªë h·ªçc trong qu√° tr√¨nh optimization
    
    Tham s·ªë:
        gradient_norm: chu·∫©n c·ªßa gradient hi·ªán t·∫°i
        loss_value: gi√° tr·ªã loss hi·ªán t·∫°i (t√πy ch·ªçn)
        weights: vector tr·ªçng s·ªë hi·ªán t·∫°i (t√πy ch·ªçn)
        iteration: s·ªë iteration hi·ªán t·∫°i
    
    Tr·∫£ v·ªÅ:
        has_issues: True n·∫øu ph√°t hi·ªán v·∫•n ƒë·ªÅ s·ªë h·ªçc
        issue_description: m√¥ t·∫£ chi ti·∫øt v·∫•n ƒë·ªÅ
    """
    issues = []
    
    # Ki·ªÉm tra gradient norm
    if np.isnan(gradient_norm):
        issues.append(f"Gradient norm = NaN")
    elif np.isinf(gradient_norm):
        issues.append(f"Gradient norm = ¬±Inf")
    
    # Ki·ªÉm tra loss value n·∫øu c√≥
    if loss_value is not None:
        if np.isnan(loss_value):
            issues.append(f"Loss = NaN")
        elif np.isinf(loss_value):
            issues.append(f"Loss = ¬±Inf")
    
    # Ki·ªÉm tra weights n·∫øu c√≥
    if weights is not None:
        if np.any(np.isnan(weights)):
            nan_count = np.sum(np.isnan(weights))
            issues.append(f"Weights contain {nan_count} NaN values")
        if np.any(np.isinf(weights)):
            inf_count = np.sum(np.isinf(weights))
            issues.append(f"Weights contain {inf_count} ¬±Inf values")
    
    if issues:
        issue_description = f"NUMERICAL INSTABILITY at iteration {iteration}: " + ", ".join(issues)
        return True, issue_description
    
    return False, "No numerical issues detected"


# ==============================================================================
# 5. KI·ªÇM TRA H·ªòI T·ª§ V√Ä LINE SEARCH
# ==============================================================================

def kiem_tra_hoi_tu(gradient_norm: float, cost_change: float, iteration: int,
                   tolerance: float = 1e-6, max_iterations: int = 100, 
                   loss_value: Optional[float] = None, weights: Optional[np.ndarray] = None) -> Tuple[bool, str]:
    """
    Ki·ªÉm tra ƒëi·ªÅu ki·ªán h·ªôi t·ª• cho thu·∫≠t to√°n optimization
    KI·ªÇM TRA TH·ª® T·ª∞: 1) Numerical stability, 2) Max iterations, 3) Convergence conditions
    
    Tham s·ªë:
        gradient_norm: chu·∫©n c·ªßa gradient hi·ªán t·∫°i
        cost_change: thay ƒë·ªïi cost t·ª´ iteration tr∆∞·ªõc
        iteration: s·ªë iteration hi·ªán t·∫°i
        tolerance: ng∆∞·ª°ng h·ªôi t·ª•
        max_iterations: s·ªë iteration t·ªëi ƒëa
        loss_value: gi√° tr·ªã loss hi·ªán t·∫°i (t√πy ch·ªçn, ƒë·ªÉ ki·ªÉm tra numerical stability)
        weights: vector tr·ªçng s·ªë hi·ªán t·∫°i (t√πy ch·ªçn, ƒë·ªÉ ki·ªÉm tra numerical stability)
    
    Tr·∫£ v·ªÅ:
        converged: c√≥ h·ªôi t·ª• hay kh√¥ng
        reason: l√Ω do d·ª´ng
    """
    # 1. KI·ªÇM TRA NUMERICAL STABILITY TR∆Ø·ªöC TI√äN (∆∞u ti√™n cao nh·∫•t)
    has_issues, issue_description = check_for_numerical_issues(
        gradient_norm, loss_value, weights, iteration
    )
    if has_issues:
        return False, issue_description
    
    # 2. ƒê·∫°t gi·ªõi h·∫°n iteration
    if iteration >= max_iterations:
        return False, f"ƒê·∫°t gi·ªõi h·∫°n iteration: {iteration}"
    
    # 3. Ki·ªÉm tra ƒëi·ªÅu ki·ªán gradient norm
    gradient_converged = gradient_norm < tolerance
    
    # 4. Ki·ªÉm tra ƒëi·ªÅu ki·ªán thay ƒë·ªïi cost (ch·ªâ sau iteration ƒë·∫ßu ti√™n)  
    cost_converged = iteration > 0 and abs(cost_change) < tolerance
    
    # 5. Y√äU C·∫¶U ƒê·ªíNG TH·ªúI C·∫¢ HAI ƒêI·ªÄU KI·ªÜN
    if gradient_converged and cost_converged:
        return True, f"H·ªôi t·ª• ƒë·ªìng th·ªùi: gradient norm {gradient_norm:.2e} < {tolerance:.2e} V√Ä cost change {abs(cost_change):.2e} < {tolerance:.2e}"
    
    # 6. Ch∆∞a h·ªôi t·ª• - hi·ªÉn th·ªã tr·∫°ng th√°i hi·ªán t·∫°i
    if iteration > 0:
        return False, f"Ch∆∞a h·ªôi t·ª•: gradient={gradient_norm:.2e} ({'‚úì' if gradient_converged else '‚úó'}), cost_change={abs(cost_change):.2e} ({'‚úì' if cost_converged else '‚úó'})"
    else:
        return False, f"Ch∆∞a h·ªôi t·ª•: gradient={gradient_norm:.2e} ({'‚úì' if gradient_converged else '‚úó'}), cost_change=N/A"


def backtracking_line_search(cost_func: Callable, gradient: np.ndarray, 
                            current_point: np.ndarray, search_direction: np.ndarray,
                            current_cost: float, alpha_init: float = 1.0,
                            rho: float = 0.5, c1: float = 1e-4, max_iter: int = 50) -> float:
    """
    Backtracking line search ƒë·ªÉ t√¨m learning rate t·ªëi ∆∞u
    
    S·ª≠ d·ª•ng Armijo condition ƒë·ªÉ ƒë·∫£m b·∫£o sufficient decrease.
    
    Tham s·ªë:
        cost_func: h√†m t√≠nh cost
        gradient: gradient t·∫°i ƒëi·ªÉm hi·ªán t·∫°i
        current_point: ƒëi·ªÉm hi·ªán t·∫°i
        search_direction: h∆∞·ªõng t√¨m ki·∫øm (th∆∞·ªùng l√† -H^{-1} * gradient)
        current_cost: cost t·∫°i ƒëi·ªÉm hi·ªán t·∫°i
        alpha_init: learning rate ban ƒë·∫ßu
        rho: h·ªá s·ªë gi·∫£m learning rate
        c1: h·∫±ng s·ªë cho Armijo condition
        max_iter: s·ªë iteration t·ªëi ƒëa
    
    Tr·∫£ v·ªÅ:
        alpha: learning rate t·ªëi ∆∞u t√¨m ƒë∆∞·ª£c
    """
    alpha = alpha_init
    directional_derivative = gradient.T @ search_direction
    
    for i in range(max_iter):
        new_point = current_point + alpha * search_direction
        new_cost = cost_func(new_point)
        
        # Armijo condition: f(x + Œ±*d) ‚â§ f(x) + c1*Œ±*‚àáf(x)^T*d
        if new_cost <= current_cost + c1 * alpha * directional_derivative:
            return alpha
        
        alpha *= rho
    
    return alpha


# ==============================================================================
# 5. H√ÄM LOSS CHO C√ÅC BI·∫æN TH·ªÇ
# ==============================================================================

def tinh_loss_ols(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray, he_so_tu_do: float = 0) -> float:
    """
    T√≠nh loss cho OLS (Ordinary Least Squares)
    
    Loss = (1/2n) * ||Xw + b - y||¬≤
    """
    predictions = du_doan(X, trong_so, he_so_tu_do)
    return 0.5 * tinh_mse(y, predictions)


def tinh_loss_ridge(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray, 
                   he_so_tu_do: float, dieu_chinh: float) -> float:
    """
    T√≠nh loss cho Ridge Regression
    
    Loss = (1/2n) * ||Xw + b - y||¬≤ + (Œª/2) * ||w||¬≤
    """
    mse_loss = tinh_loss_ols(X, y, trong_so, he_so_tu_do)
    l2_penalty = 0.5 * dieu_chinh * np.sum(trong_so ** 2)
    return mse_loss + l2_penalty


def tinh_loss_lasso_smooth(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray,
                          he_so_tu_do: float, dieu_chinh: float, epsilon: float = 1e-8) -> float:
    """
    T√≠nh loss cho Lasso Regression (smooth approximation)
    
    Loss = (1/2n) * ||Xw + b - y||¬≤ + Œª * Œ£‚àö(w¬≤+ Œµ)
    """
    mse_loss = tinh_loss_ols(X, y, trong_so, he_so_tu_do)
    smooth_l1 = dieu_chinh * np.sum(np.sqrt(trong_so ** 2 + epsilon))
    return mse_loss + smooth_l1


# ==============================================================================
# 5.1. C√ÅC H√ÄM CH·ªà NH·∫¨N WEIGHTS (KH√îNG C√ì BIAS) - CHO GRADIENT DESCENT
# ==============================================================================

def tinh_gia_tri_ham_OLS(X: np.ndarray, y: np.ndarray, w: np.ndarray, bias: float = None) -> float:
    """
    T√≠nh gi√° tr·ªã h√†m OLS t·∫°i ƒëi·ªÉm w
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: L(w,b) = (1/2n) * ||Xw + b - y||¬≤ (khi bias != None)
    - Format m·ªõi: L(w) = (1/2n) * ||Xw - y||¬≤ (khi X ƒë√£ bao g·ªìm c·ªôt bias)
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
    
    Tr·∫£ v·ªÅ:
        float: gi√° tr·ªã h√†m OLS
    """
    n_samples = X.shape[0]
    if bias is not None:
        # Format c≈©: Xw + bias
        predictions = X @ w + bias
    else:
        # Format m·ªõi: Xw (v·ªõi X ƒë√£ bao g·ªìm c·ªôt bias)
        predictions = X @ w
    residuals = predictions - y
    ols_value = (1 / (2 * n_samples)) * np.sum(residuals ** 2)
    return ols_value


def tinh_gradient_OLS(X: np.ndarray, y: np.ndarray, w: np.ndarray, bias: float = None) -> Tuple[np.ndarray, float]:
    """
    T√≠nh gradient c·ªßa h√†m OLS theo weights
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: ‚àáL(w,b) = ((1/n) * X^T(Xw + b - y), (1/n) * Œ£(Xw + b - y)) (khi bias != None)
    - Format m·ªõi: ‚àáL(w) = (1/n) * X^T(Xw - y) (khi X ƒë√£ bao g·ªìm c·ªôt bias)
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo weights (n_features,) ho·∫∑c (n_features + 1,)
        gradient_b: gradient theo bias (scalar, ho·∫∑c 0.0 cho format m·ªõi)
    """
    n_samples = X.shape[0]
    
    if bias is not None:
        # Format c≈©: t√°ch ri√™ng gradient cho w v√† b
        predictions = X @ w + bias
        errors = predictions - y
        gradient_w = (1 / n_samples) * X.T @ errors
        gradient_b = (1 / n_samples) * np.sum(errors)
        return gradient_w, gradient_b
    else:
        # Format m·ªõi: gradient th·ªëng nh·∫•t cho w (bao g·ªìm bias)
        predictions = X @ w
        errors = predictions - y
        gradient_w = (1 / n_samples) * X.T @ errors
        return gradient_w, 0.0







def tinh_gia_tri_ham_Ridge_with_bias(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, regularization: float) -> float:
    """
    T√≠nh gi√° tr·ªã h√†m Ridge regression v·ªõi bias term
    
    H√†m Ridge: L(w,b) = (1/2n) * ||Xw + b - y||¬≤ + (Œª/2) * ||w||¬≤
    L∆∞u √Ω: Kh√¥ng regularize bias term
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        b: bias term (scalar)
        regularization: h·ªá s·ªë regularization Œª
    
    Tr·∫£ v·ªÅ:
        float: gi√° tr·ªã h√†m Ridge t·∫°i (w, b)
    """
    n_samples = X.shape[0]
    predictions = X @ w + b
    residuals = predictions - y
    data_loss = (1 / (2 * n_samples)) * np.sum(residuals ** 2)
    reg_loss = (regularization / 2) * np.sum(w ** 2)  # Kh√¥ng regularize bias
    return data_loss + reg_loss


def tinh_gradient_Ridge_with_bias(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, regularization: float) -> Tuple[np.ndarray, float]:
    """
    T√≠nh gradient c·ªßa h√†m Ridge theo weights v√† bias
    
    ‚àáL(w,b) = ((1/n) * X^T(Xw + b - y) + Œª*w, (1/n) * Œ£(Xw + b - y))
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        b: bias term (scalar)
        regularization: h·ªá s·ªë regularization Œª
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo weights (n_features,)
        gradient_b: gradient theo bias (scalar)
    """
    n_samples = X.shape[0]
    predictions = X @ w + b
    errors = predictions - y
    
    gradient_w = (1 / n_samples) * X.T @ errors + regularization * w  # Regularize weights
    gradient_b = (1 / n_samples) * np.sum(errors)  # Kh√¥ng regularize bias
    
    return gradient_w, gradient_b


def tinh_gia_tri_ham_Lasso_smooth_with_bias(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, regularization: float) -> float:
    """
    T√≠nh gi√° tr·ªã h√†m Lasso (smooth approximation) v·ªõi bias term
    
    H√†m Lasso: L(w,b) = (1/2n) * ||Xw + b - y||¬≤ + Œª * Œ£|w_i|
    S·ª≠ d·ª•ng smooth approximation: |x| ‚âà ‚àö(x¬≤ + Œµ¬≤) v·ªõi Œµ = 1e-8
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        b: bias term (scalar)
        regularization: h·ªá s·ªë regularization Œª
    
    Tr·∫£ v·ªÅ:
        float: gi√° tr·ªã h√†m Lasso t·∫°i (w, b)
    """
    n_samples = X.shape[0]
    predictions = X @ w + b
    residuals = predictions - y
    data_loss = (1 / (2 * n_samples)) * np.sum(residuals ** 2)
    
    # Smooth approximation c·ªßa |w|
    epsilon = 1e-8
    reg_loss = regularization * np.sum(np.sqrt(w ** 2 + epsilon))  # Kh√¥ng regularize bias
    
    return data_loss + reg_loss


def tinh_gradient_Lasso_smooth_with_bias(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float, regularization: float) -> Tuple[np.ndarray, float]:
    """
    T√≠nh gradient c·ªßa h√†m Lasso (smooth) theo weights v√† bias
    
    ‚àáL(w,b) = ((1/n) * X^T(Xw + b - y) + Œª * w/‚àö(w¬≤ + Œµ¬≤), (1/n) * Œ£(Xw + b - y))
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        y: vector target (n_samples,)
        w: vector weights (n_features,)
        b: bias term (scalar)
        regularization: h·ªá s·ªë regularization Œª
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo weights (n_features,)
        gradient_b: gradient theo bias (scalar)
    """
    n_samples = X.shape[0]
    predictions = X @ w + b
    errors = predictions - y
    
    # Gradient c·ªßa data term
    gradient_w_data = (1 / n_samples) * X.T @ errors
    gradient_b = (1 / n_samples) * np.sum(errors)
    
    # Gradient c·ªßa regularization term (smooth approximation)
    epsilon = 1e-8
    gradient_w_reg = regularization * w / np.sqrt(w ** 2 + epsilon)
    
    gradient_w = gradient_w_data + gradient_w_reg
    
    return gradient_w, gradient_b


def tinh_hessian_OLS(X: np.ndarray) -> np.ndarray:
    """
    T√≠nh ma tr·∫≠n Hessian c·ªßa h√†m OLS (kh√¥ng c√≥ bias)
    
    H = (1/n) * X^T * X
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
    
    Tr·∫£ v·ªÅ:
        hessian: ma tr·∫≠n Hessian (n_features, n_features)
    """
    n_samples = X.shape[0]
    hessian = (1 / n_samples) * X.T @ X
    return hessian


def tinh_gia_tri_ham_Ridge(X: np.ndarray, y: np.ndarray, w: np.ndarray, bias: float = None, lambda_reg: float = 0.01) -> float:
    """
    T√≠nh gi√° tr·ªã h√†m Ridge
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: L(w,b) = (1/2n) * ||Xw + b - y||¬≤ + (Œª/2) * ||w||¬≤ (khi bias != None)
    - Format m·ªõi: L(w) = (1/2n) * ||Xw - y||¬≤ + (Œª/2) * ||w[:-1]||¬≤ (khi X ƒë√£ bao g·ªìm c·ªôt bias)
    
    L∆∞u √Ω: Kh√¥ng regularize bias term trong c·∫£ 2 format
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
        lambda_reg: h·ªá s·ªë regularization
    
    Tr·∫£ v·ªÅ:
        float: gi√° tr·ªã h√†m Ridge
    """
    ols_loss = tinh_gia_tri_ham_OLS(X, y, w, bias)
    
    if bias is not None:
        # Format c≈©: ch·ªâ regularize weights, kh√¥ng regularize bias
        l2_penalty = 0.5 * lambda_reg * np.sum(w ** 2)
    else:
        # Format m·ªõi: ch·ªâ regularize weights (kh√¥ng bao g·ªìm bias ·ªü cu·ªëi)
        l2_penalty = 0.5 * lambda_reg * np.sum(w[:-1] ** 2)
    
    return ols_loss + l2_penalty


def tinh_gradient_Ridge(X: np.ndarray, y: np.ndarray, w: np.ndarray, bias: float = None, lambda_reg: float = 0.01) -> Tuple[np.ndarray, float]:
    """
    T√≠nh gradient c·ªßa h√†m Ridge theo weights
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: ‚àáL(w,b) = ((1/n) * X^T(Xw + b - y) + Œª*w, (1/n) * Œ£(Xw + b - y)) (khi bias != None)
    - Format m·ªõi: ‚àáL(w) = (1/n) * X^T(Xw - y) + Œª*[w[:-1]; 0] (khi X ƒë√£ bao g·ªìm c·ªôt bias)
    
    L∆∞u √Ω: Kh√¥ng regularize bias term trong c·∫£ 2 format
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
        lambda_reg: h·ªá s·ªë regularization
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo weights (n_features,) ho·∫∑c (n_features + 1,)
        gradient_b: gradient theo bias (scalar, ho·∫∑c 0.0 cho format m·ªõi)
    """
    gradient_w, gradient_b = tinh_gradient_OLS(X, y, w, bias)
    
    if bias is not None:
        # Format c≈©: ch·ªâ regularize weights, kh√¥ng regularize bias
        l2_gradient = lambda_reg * w
        gradient_w = gradient_w + l2_gradient
        return gradient_w, gradient_b
    else:
        # Format m·ªõi: ch·ªâ regularize weights (kh√¥ng bao g·ªìm bias ·ªü cu·ªëi)
        l2_gradient = np.zeros_like(w)
        l2_gradient[:-1] = lambda_reg * w[:-1]  # Kh√¥ng regularize bias (ph·∫ßn t·ª≠ cu·ªëi)
        gradient_w = gradient_w + l2_gradient
        return gradient_w, 0.0


def tinh_hessian_Ridge(X: np.ndarray, lambda_reg: float) -> np.ndarray:
    """
    T√≠nh ma tr·∫≠n Hessian c·ªßa h√†m Ridge (kh√¥ng c√≥ bias)
    
    H = (1/n) * X^T * X + Œª * I
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        lambda_reg: h·ªá s·ªë regularization
    
    Tr·∫£ v·ªÅ:
        hessian: ma tr·∫≠n Hessian (n_features, n_features)
    """
    ols_hessian = tinh_hessian_OLS(X)
    n_features = X.shape[1]
    l2_hessian = lambda_reg * np.eye(n_features)
    return ols_hessian + l2_hessian


def tinh_gia_tri_ham_Lasso_smooth(X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                                  bias: float = None, lambda_reg: float = 0.01, epsilon: float = 1e-8) -> float:
    """
    T√≠nh gi√° tr·ªã h√†m Lasso v·ªõi smooth approximation
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: L(w,b) = (1/2n) * ||Xw + b - y||¬≤ + Œª * Œ£‚àö(w¬≤+ Œµ) (khi bias != None)
    - Format m·ªõi: L(w) = (1/2n) * ||Xw - y||¬≤ + Œª * Œ£‚àö(w[:-1]¬≤+ Œµ) (khi X ƒë√£ bao g·ªìm c·ªôt bias)
    
    L∆∞u √Ω: Kh√¥ng regularize bias term trong c·∫£ 2 format
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
        lambda_reg: h·ªá s·ªë regularization
        epsilon: tham s·ªë smoothing
    
    Tr·∫£ v·ªÅ:
        float: gi√° tr·ªã h√†m Lasso
    """
    ols_loss = tinh_gia_tri_ham_OLS(X, y, w, bias)
    
    if bias is not None:
        # Format c≈©: ch·ªâ regularize weights, kh√¥ng regularize bias
        smooth_l1_penalty = lambda_reg * np.sum(np.sqrt(w ** 2 + epsilon))
    else:
        # Format m·ªõi: ch·ªâ regularize weights (kh√¥ng bao g·ªìm bias ·ªü cu·ªëi)
        smooth_l1_penalty = lambda_reg * np.sum(np.sqrt(w[:-1] ** 2 + epsilon))
        
    return ols_loss + smooth_l1_penalty


def tinh_gradient_Lasso_smooth(X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                              bias: float = None, lambda_reg: float = 0.01, epsilon: float = 1e-8) -> Tuple[np.ndarray, float]:
    """
    T√≠nh gradient c·ªßa h√†m Lasso v·ªõi smooth approximation theo weights
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: ‚àáL(w,b) = ((1/n) * X^T(Xw + b - y) + Œª * w / ‚àö(w¬≤ + Œµ), (1/n) * Œ£(Xw + b - y)) (khi bias != None)
    - Format m·ªõi: ‚àáL(w) = (1/n) * X^T(Xw - y) + Œª * [w[:-1] / ‚àö(w[:-1]¬≤ + Œµ); 0] (khi X ƒë√£ bao g·ªìm c·ªôt bias)
    
    L∆∞u √Ω: Kh√¥ng regularize bias term trong c·∫£ 2 format
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
        lambda_reg: h·ªá s·ªë regularization
        epsilon: tham s·ªë smoothing
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo weights (n_features,) ho·∫∑c (n_features + 1,)
        gradient_b: gradient theo bias (scalar, ho·∫∑c 0.0 cho format m·ªõi)
    """
    gradient_w, gradient_b = tinh_gradient_OLS(X, y, w, bias)
    
    if bias is not None:
        # Format c≈©: ch·ªâ regularize weights, kh√¥ng regularize bias
        smooth_l1_gradient = lambda_reg * w / np.sqrt(w ** 2 + epsilon)
        gradient_w = gradient_w + smooth_l1_gradient
        return gradient_w, gradient_b
    else:
        # Format m·ªõi: ch·ªâ regularize weights (kh√¥ng bao g·ªìm bias ·ªü cu·ªëi)
        smooth_l1_gradient = np.zeros_like(w)
        smooth_l1_gradient[:-1] = lambda_reg * w[:-1] / np.sqrt(w[:-1] ** 2 + epsilon)
        gradient_w = gradient_w + smooth_l1_gradient
        return gradient_w, 0.0


def tinh_hessian_Lasso_smooth(X: np.ndarray, w: np.ndarray, lambda_reg: float, epsilon: float = 1e-8) -> np.ndarray:
    """
    T√≠nh ma tr·∫≠n Hessian c·ªßa h√†m Lasso v·ªõi smooth approximation (kh√¥ng c√≥ bias)
    
    H = (1/n) * X^T * X + Œª * diag(Œµ / (w¬≤ + Œµ)^(3/2))
    
    Tham s·ªë:
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        w: vector weights (n_features,)
        lambda_reg: h·ªá s·ªë regularization
        epsilon: tham s·ªë smoothing
    
    Tr·∫£ v·ªÅ:
        hessian: ma tr·∫≠n Hessian (n_features, n_features)
    """
    ols_hessian = tinh_hessian_OLS(X)
    
    # Diagonal elements cho L1 smooth penalty
    denominator = (w ** 2 + epsilon) ** (3/2)
    l1_diagonal = lambda_reg * epsilon / denominator
    
    # Th√™m diagonal elements
    l1_hessian = np.diag(l1_diagonal)
    
    return ols_hessian + l1_hessian


# ==============================================================================
# 6. ƒê√ÅNH GI√Å M√î H√åNH
# ==============================================================================

def danh_gia_mo_hinh(weights: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, 
                      bias: float = 0.0) -> Dict[str, float]:
    """
    ƒê√°nh gi√° model tr√™n test set v·ªõi ƒë·∫ßy ƒë·ªß c√°c metrics
    
    
    Tham s·ªë:
        weights: tr·ªçng s·ªë ƒë√£ h·ªçc (n_features,)
        X_test: ma tr·∫≠n ƒë·∫∑c tr∆∞ng test (n_samples, n_features)
        y_test: gi√° tr·ªã th·∫≠t test (n_samples,)
        bias: bias term (m·∫∑c ƒë·ªãnh 0.0)
        is_log_transformed: c√≥ ph·∫£i target ƒë√£ ƒë∆∞·ª£c log transform kh√¥ng
    
    Tr·∫£ v·ªÅ:
        dict: dictionary ch·ª©a c√°c metrics ƒë√°nh gi√° tr√™n scale g·ªëc
    """
    # D·ª± ƒëo√°n tr√™n log scale (n·∫øu model ƒë∆∞·ª£c train tr√™n log)
    predictions_log = du_doan(X_test, weights, bias)
    
    # Convert c·∫£ predictions v√† test v·ªÅ original scale
    predictions_original = np.expm1(predictions_log)  # inverse of log1p
    y_test_original = np.expm1(y_test)                # inverse of log1p
    
    # Use original scale for evaluation
    predictions_eval = predictions_original
    y_test_eval = y_test_original
    
    # T√≠nh c√°c metrics c∆° b·∫£n tr√™n scale g·ªëc
    mse = tinh_mse(y_test_eval, predictions_eval)
    rmse = np.sqrt(mse)
    mae = tinh_mae(y_test_eval, predictions_eval)
    r2 = tinh_r2_score(y_test_eval, predictions_eval)
    
    # MAPE (Mean Absolute Percentage Error) - c·∫©n th·∫≠n v·ªõi chia cho 0
    with np.errstate(divide='ignore', invalid='ignore'):
        percentage_errors = np.abs((y_test_eval - predictions_eval) / y_test_eval)
        # Lo·∫°i b·ªè c√°c gi√° tr·ªã inf v√† nan
        valid_errors = percentage_errors[np.isfinite(percentage_errors)]
        mape = np.mean(valid_errors) * 100 if len(valid_errors) > 0 else float('inf')
    
    # Th√™m m·ªôt s·ªë metrics b·ªï sung
    # Max Error
    max_error = np.max(np.abs(y_test_eval - predictions_eval))
    
    # Explained variance score
    var_y = np.var(y_test_eval)
    var_residual = np.var(y_test_eval - predictions_eval)
    explained_variance = 1 - (var_residual / var_y) if var_y != 0 else 0
    
    # Th√™m metrics cho c·∫£ log scale (n·∫øu c√≥ transform)
    metrics = {
        'mse': float(mse),
        'rmse': float(rmse),
        'mae': float(mae),
        'r2': float(r2),
        'mape': float(mape),
        'max_error': float(max_error),
        'explained_variance': float(explained_variance)
    }
    
    # N·∫øu c√≥ log transform, th√™m metrics tr√™n log scale ƒë·ªÉ so s√°nh
    mse_log = tinh_mse(y_test, predictions_log)
    r2_log = tinh_r2_score(y_test, predictions_log)
    mae_log = tinh_mae(y_test, predictions_log)
    
    metrics.update({
        'mse_log_scale': float(mse_log),
        'r2_log_scale': float(r2_log),
        'mae_log_scale': float(mae_log)
    })
    
    return metrics

def danh_gia_mo_hinh_with_bias(weights: np.ndarray, bias: float, X_test: np.ndarray, y_test: np.ndarray) -> Dict[str, float]:
    """
    ƒê√°nh gi√° model tr√™n test set v·ªõi ƒë·∫ßy ƒë·ªß c√°c metrics (c√≥ bias term)
    
    Tham s·ªë:
        weights: tr·ªçng s·ªë ƒë√£ h·ªçc (n_features,)
        bias: bias term ƒë√£ h·ªçc (scalar)
        X_test: ma tr·∫≠n ƒë·∫∑c tr∆∞ng test (n_samples, n_features)
        y_test: gi√° tr·ªã th·∫≠t test (n_samples,)
    
    Tr·∫£ v·ªÅ:
        dict: dictionary ch·ª©a c√°c metrics ƒë√°nh gi√° tr√™n scale g·ªëc
    """
    # D·ª± ƒëo√°n tr√™n log scale (n·∫øu model ƒë∆∞·ª£c train tr√™n log)
    predictions_log = du_doan(X_test, weights, bias)
        
    # Convert c·∫£ predictions v√† test v·ªÅ original scale
    predictions_original = np.expm1(predictions_log)  # inverse of log1p
    y_test_original = np.expm1(y_test)                # inverse of log1p

    # Use original scale for evaluation
    predictions_eval = predictions_original
    y_test_eval = y_test_original
    
    # T√≠nh c√°c metrics c∆° b·∫£n tr√™n scale g·ªëc
    mse = tinh_mse(y_test_eval, predictions_eval)
    rmse = np.sqrt(mse)
    mae = tinh_mae(y_test_eval, predictions_eval)
    r2 = tinh_r2_score(y_test_eval, predictions_eval)
    
    # MAPE (Mean Absolute Percentage Error) - c·∫©n th·∫≠n v·ªõi chia cho 0
    with np.errstate(divide='ignore', invalid='ignore'):
        percentage_errors = np.abs((y_test_eval - predictions_eval) / y_test_eval)
        # Lo·∫°i b·ªè c√°c gi√° tr·ªã inf v√† nan
        valid_errors = percentage_errors[np.isfinite(percentage_errors)]
        mape = np.mean(valid_errors) * 100 if len(valid_errors) > 0 else float('inf')
    
    # Th√™m m·ªôt s·ªë metrics b·ªï sung
    # Max Error
    max_error = np.max(np.abs(y_test_eval - predictions_eval))
    
    # Explained variance score
    var_y = np.var(y_test_eval)
    var_residual = np.var(y_test_eval - predictions_eval)
    explained_variance = 1 - (var_residual / var_y) if var_y != 0 else 0
    
    # Th√™m metrics cho c·∫£ log scale (n·∫øu c√≥ transform)
    metrics = {
        'mse': float(mse),
        'rmse': float(rmse),
        'mae': float(mae),
        'r2': float(r2),
        'mape': float(mape),
        'max_error': float(max_error),
        'explained_variance': float(explained_variance)
    }
    
    # N·∫øu c√≥ log transform, th√™m metrics tr√™n log scale ƒë·ªÉ so s√°nh
    mse_log = tinh_mse(y_test, predictions_log)
    r2_log = tinh_r2_score(y_test, predictions_log)
    mae_log = tinh_mae(y_test, predictions_log)
    
    metrics.update({
        'mse_log_scale': float(mse_log),
        'r2_log_scale': float(r2_log),
        'mae_log_scale': float(mae_log)
    })
    
    return metrics


def in_ket_qua_danh_gia(metrics: Dict[str, float], training_time: float = None, 
                       algorithm_name: str = "Model"):
    """
    In k·∫øt qu·∫£ ƒë√°nh gi√° model m·ªôt c√°ch ƒë·∫πp m·∫Øt
    
    Tham s·ªë:
        metrics: dictionary ch·ª©a c√°c metrics t·ª´ evaluate_model
        training_time: th·ªùi gian training (t√πy ch·ªçn)
        algorithm_name: t√™n thu·∫≠t to√°n
    """
    print("\n" + "="*60)
    print(f"üìä {algorithm_name.upper()} - EVALUATION RESULTS")
    print("="*60)
    
    # Th√¥ng b√°o scale ƒë√°nh gi√°    
    print(f"\nüéØ REGRESSION METRICS:")
    print(f"   MSE:      {metrics['mse']:.8f}")
    print(f"   RMSE:     {metrics['rmse']:.6f}")
    print(f"   MAE:      {metrics['mae']:.6f}")
    print(f"   R¬≤ Score: {metrics['r2']:.6f}")
    
    if metrics['mape'] != float('inf'):
        print(f"   MAPE:     {metrics['mape']:.2f}%")
    else:
        print(f"   MAPE:     N/A (division by zero)")
        
    print(f"   Max Error: {metrics['max_error']:.6f}")
    print(f"   Explained Variance: {metrics['explained_variance']:.6f}")
    
    if training_time is not None:
        print(f"   Training Time: {training_time:.4f}s")
    
# ==============================================================================
# 7. TI·ªÜN √çCH DEBUG V√Ä IN TH√îNG TIN
# ==============================================================================

def in_thong_tin_ma_tran(matrix: np.ndarray, ten_ma_tran: str = "Matrix"):
    """
    In th√¥ng tin chi ti·∫øt v·ªÅ ma tr·∫≠n
    """
    print(f"\n=== {ten_ma_tran} ===")
    print(f"K√≠ch th∆∞·ªõc: {matrix.shape}")
    print(f"Condition number: {tinh_condition_number(matrix):.2e}")
    print(f"Positive definite: {kiem_tra_positive_definite(matrix)}")
    print(f"Eigenvalues min/max: {np.min(np.linalg.eigvals(matrix)):.2e} / {np.max(np.linalg.eigvals(matrix)):.2e}")


def in_thong_tin_gradient(gradient_w: np.ndarray, gradient_b: float):
    """
    In th√¥ng tin v·ªÅ gradient
    """
    print(f"\n=== Gradient Info ===")
    print(f"||‚àáw||: {np.linalg.norm(gradient_w):.2e}")
    print(f"|‚àáb|: {abs(gradient_b):.2e}")
    print(f"||‚àáf||: {np.sqrt(np.sum(gradient_w**2) + gradient_b**2):.2e}")


# ==============================================================================
# 9. B·ªî SUNG LOSS FUNCTIONS KH√ÅC
# ==============================================================================

def tinh_loss_elastic_net(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray,
                         he_so_tu_do: float, alpha: float, l1_ratio: float) -> float:
    """
    T√≠nh loss cho Elastic Net Regression
    
    Loss = MSE + alpha * (l1_ratio * L1 + (1-l1_ratio) * L2)
    """
    mse_loss = tinh_loss_ols(X, y, trong_so, he_so_tu_do)
    l1_penalty = alpha * l1_ratio * np.sum(np.abs(trong_so))
    l2_penalty = alpha * (1 - l1_ratio) * 0.5 * np.sum(trong_so ** 2)
    return mse_loss + l1_penalty + l2_penalty


def tinh_loss_huber(X: np.ndarray, y: np.ndarray, trong_so: np.ndarray,
                   he_so_tu_do: float, delta: float = 1.0) -> float:
    """
    T√≠nh Huber loss - robust loss function
    
    Huber(e) = 0.5 * e¬≤ if |e| ‚â§ Œ¥ else Œ¥ * (|e| - 0.5 * Œ¥)
    """
    predictions = du_doan(X, trong_so, he_so_tu_do)
    errors = predictions - y
    
    # √Åp d·ª•ng Huber loss
    abs_errors = np.abs(errors)
    quadratic = 0.5 * errors ** 2
    linear = delta * (abs_errors - 0.5 * delta)
    
    # Ch·ªçn quadratic ho·∫∑c linear d·ª±a tr√™n threshold
    huber_losses = np.where(abs_errors <= delta, quadratic, linear)
    return np.mean(huber_losses)


def tinh_gradient_ridge(X: np.ndarray, y: np.ndarray, w: np.ndarray, lam: float) -> np.ndarray:
    """
    T√≠nh gradient cho Ridge regression
    
    ‚àáf = X^T(Xw - y)/n + Œªw
    """
    n = X.shape[0]
    predictions = X @ w
    errors = predictions - y
    gradient = (X.T @ errors) / n + lam * w
    return gradient


def tinh_gradient_lasso_smooth(X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                              lam: float, epsilon: float = 1e-8) -> np.ndarray:
    """
    T√≠nh gradient cho Lasso regression (smooth approximation)
    
    ‚àáf = X^T(Xw - y)/n + Œª * w/‚àö(w¬≤ + Œµ)
    """
    n = X.shape[0]
    predictions = X @ w
    errors = predictions - y
    mse_gradient = (X.T @ errors) / n
    
    # Smooth L1 gradient
    l1_gradient = lam * w / np.sqrt(w ** 2 + epsilon)
    return mse_gradient + l1_gradient


def tinh_hessian_ridge(X: np.ndarray, lam: float) -> np.ndarray:
    """
    T√≠nh Hessian cho Ridge regression
    
    H = X^TX/n + ŒªI
    """
    n = X.shape[0]
    XTX = X.T @ X
    I = np.eye(X.shape[1])
    return XTX / n + lam * I


def tinh_gia_tri_ham_loss(ham_loss: str, X: np.ndarray, y: np.ndarray, w: np.ndarray, 
                         bias: float = None, regularization: float = 0.01, **kwargs) -> float:
    """
    H√†m th·ªëng nh·∫•t ƒë·ªÉ t√≠nh gi√° tr·ªã loss function
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: loss v·ªõi bias ri√™ng (khi bias != None)
    - Format m·ªõi: loss v·ªõi bias trong X (khi bias = None)
    
    Tham s·ªë:
        ham_loss: lo·∫°i loss function ('ols', 'ridge', 'lasso')
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
        regularization: h·ªá s·ªë regularization (m·∫∑c ƒë·ªãnh 0.01)
        **kwargs: c√°c tham s·ªë b·ªï sung (v√≠ d·ª• epsilon cho Lasso)
    
    Tr·∫£ v·ªÅ:
        float: gi√° tr·ªã loss function
    """
    ham_loss = ham_loss.lower()
    
    if ham_loss == 'ols':
        return tinh_gia_tri_ham_OLS(X, y, w, bias)
    elif ham_loss == 'ridge':
        return tinh_gia_tri_ham_Ridge(X, y, w, bias, regularization)
    elif ham_loss == 'lasso':
        epsilon = kwargs.get('epsilon', 1e-8)
        return tinh_gia_tri_ham_Lasso_smooth(X, y, w, bias, regularization, epsilon)
    else:
        raise ValueError(f"Kh√¥ng h·ªó tr·ª£ loss function: {ham_loss}. Ch·ªâ h·ªó tr·ª£: 'ols', 'ridge', 'lasso'")

def tinh_gradient_ham_loss(ham_loss: str, X: np.ndarray, y: np.ndarray, w: np.ndarray,
                          bias: float = None, regularization: float = 0.01, **kwargs) -> Tuple[np.ndarray, float]:
    """
    H√†m th·ªëng nh·∫•t ƒë·ªÉ t√≠nh gradient c·ªßa loss function
    
    H·ªó tr·ª£ c·∫£ 2 format:
    - Format c≈©: gradient v·ªõi bias ri√™ng (khi bias != None)
    - Format m·ªõi: gradient v·ªõi bias trong X (khi bias = None)
    
    Tham s·ªë:
        ham_loss: lo·∫°i loss function ('ols', 'ridge', 'lasso')
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features) ho·∫∑c (n_samples, n_features + 1) v·ªõi bias
        y: vector target (n_samples,)
        w: vector weights (n_features,) ho·∫∑c (n_features + 1,) v·ªõi bias
        bias: bias term (scalar, deprecated - s·ª≠ d·ª•ng None cho format m·ªõi)
        regularization: h·ªá s·ªë regularization (m·∫∑c ƒë·ªãnh 0.01)
        **kwargs: c√°c tham s·ªë b·ªï sung (v√≠ d·ª• epsilon cho Lasso)
    
    Tr·∫£ v·ªÅ:
        gradient_w: gradient theo weights (n_features,) ho·∫∑c (n_features + 1,)
        gradient_b: gradient theo bias (scalar, ho·∫∑c 0.0 cho format m·ªõi)
    """
    ham_loss = ham_loss.lower()
    
    if ham_loss == 'ols':
        return tinh_gradient_OLS(X, y, w, bias)
    elif ham_loss == 'ridge':
        return tinh_gradient_Ridge(X, y, w, bias, regularization)
    elif ham_loss == 'lasso':
        epsilon = kwargs.get('epsilon', 1e-8)
        return tinh_gradient_Lasso_smooth(X, y, w, bias, regularization, epsilon)
    else:
        raise ValueError(f"Kh√¥ng h·ªó tr·ª£ loss function: {ham_loss}. Ch·ªâ h·ªó tr·ª£: 'ols', 'ridge', 'lasso'")

def tinh_hessian_ham_loss(ham_loss: str, X: np.ndarray, w: np.ndarray = None,
                         regularization: float = 0.01, **kwargs) -> np.ndarray:
    """
    H√†m th·ªëng nh·∫•t ƒë·ªÉ t√≠nh Hessian matrix c·ªßa loss function
    
    Tham s·ªë:
        ham_loss: lo·∫°i loss function ('ols', 'ridge', 'lasso')
        X: ma tr·∫≠n ƒë·∫∑c tr∆∞ng (n_samples, n_features)
        w: vector weights (n_features,) - ch·ªâ c·∫ßn cho Lasso
        regularization: h·ªá s·ªë regularization (m·∫∑c ƒë·ªãnh 0.01)
        **kwargs: c√°c tham s·ªë b·ªï sung (v√≠ d·ª• epsilon cho Lasso)
    
    Tr·∫£ v·ªÅ:
        np.ndarray: Hessian matrix (n_features, n_features)
    """
    ham_loss = ham_loss.lower()
    
    if ham_loss == 'ols':
        return tinh_hessian_OLS(X)
    elif ham_loss == 'ridge':
        return tinh_hessian_Ridge(X, regularization)
    elif ham_loss == 'lasso':
        if w is None:
            raise ValueError("Vector weights w c·∫ßn thi·∫øt ƒë·ªÉ t√≠nh Hessian cho Lasso")
        epsilon = kwargs.get('epsilon', 1e-8)
        return tinh_hessian_Lasso_smooth(X, w, regularization, epsilon)
    else:
        raise ValueError(f"Kh√¥ng h·ªó tr·ª£ loss function: {ham_loss}. Ch·ªâ h·ªó tr·ª£: 'ols', 'ridge', 'lasso'")
