# Newton Methods - Kiáº¿n Thá»©c ToÃ¡n Há»c & Setups

## ğŸ“š **LÃ½ Thuyáº¿t ToÃ¡n Há»c**

### **Äá»‹nh NghÄ©a CÆ¡ Báº£n**
Newton Method sá»­ dá»¥ng thÃ´ng tin báº­c 2 (Hessian) Ä‘á»ƒ tÃ¬m minimum cá»§a function f(x).

**CÃ´ng thá»©c cáº­p nháº­t:**
```
x_{k+1} = x_k - H_k^{-1} âˆ‡f(x_k)
```

Trong Ä‘Ã³:
- `x_k`: Ä‘iá»ƒm hiá»‡n táº¡i
- `H_k = âˆ‡Â²f(x_k)`: Hessian matrix táº¡i x_k
- `âˆ‡f(x_k)`: gradient táº¡i x_k

### **Intuition: Taylor Expansion**
Newton method approximates f(x) báº±ng quadratic Taylor expansion:
```
f(x) â‰ˆ f(x_k) + âˆ‡f(x_k)^T(x - x_k) + (1/2)(x - x_k)^T H_k (x - x_k)
```

Minimum cá»§a quadratic nÃ y lÃ :
```
x* = x_k - H_k^{-1} âˆ‡f(x_k)
```

### **Cho Linear Regression**
Function: `f(w) = (1/2n) ||Xw - y||Â²`

**Gradient:**
```
âˆ‡f(w) = (1/n) X^T (Xw - y)
```

**Hessian (constant!):**
```
H = âˆ‡Â²f(w) = (1/n) X^T X
```

**Newton update:**
```
w_{k+1} = w_k - H^{-1} âˆ‡f(w_k)
```

### **Convergence Properties**
- **Quadratic convergence**: `||x_{k+1} - x*|| â‰¤ C ||x_k - x*||Â²`
- **Local convergence**: Cáº§n start gáº§n optimal point
- **One-step convergence** cho quadratic functions!

## ğŸ¯ **Setup vÃ  Ã NghÄ©a**

### **Standard Setup**
```python
regularization = 1e-8    # Minimal
max_iterations = 50      # Usually enough
tolerance = 1e-10        # Very strict
```

**ğŸ§  CÃ¡ch nhá»›:**
- "1e-8 nhÆ° thÃªm 1 giá»t nÆ°á»›c vÃ o biá»ƒn"
- "50 bÆ°á»›c vÃ¬ Newton ráº¥t nhanh"
- "1e-10 vÃ¬ cÃ³ thá»ƒ achieve high precision"

**âš–ï¸ Trade-offs:**
- âœ… Extremely fast convergence
- âœ… High precision
- âœ… Optimal for quadratic functions
- âŒ Expensive Hessian computation
- âŒ Memory intensive O(nÂ²)

### **Robust Setup**
```python
regularization = 1e-6    # Higher for stability
max_iterations = 100     # More iterations allowed
tolerance = 1e-8         # Slightly relaxed
```

**ğŸ§  CÃ¡ch nhá»›:**
- "1e-6 nhÆ° insurance policy"
- "100 iterations cho worst case"
- "Still very precise"

**âš–ï¸ Trade-offs:**
- âœ… More robust to ill-conditioning
- âœ… Handles edge cases better
- âŒ Slightly less pure Newton behavior

## ğŸ“Š **Regularization Deep Dive**

### **Why Regularization?**
Hessian `H = X^T X` cÃ³ thá»ƒ singular hoáº·c ill-conditioned:

**Problems:**
1. **Singular H**: Det(H) = 0, khÃ´ng invert Ä‘Æ°á»£c
2. **Ill-conditioned**: Condition number Îº(H) >> 1

**Solution: Regularization**
```
H_reg = H + Î»I
```

### **Regularization Values**

| Î» | Khi nÃ o dÃ¹ng | Effect |
|---|--------------|--------|
| **1e-12** | Perfect conditioning | Minimal impact |
| **1e-8** | Standard (good data) | Barely noticeable |
| **1e-6** | Moderate ill-conditioning | Slight smoothing |
| **1e-4** | Poor conditioning | Noticeable ridge effect |
| **1e-2** | Very ill-conditioned | Strong regularization |

### **Condition Number Analysis**
```python
Îº = Î»_max / Î»_min  # Condition number

if Îº < 1e6:    # Well-conditioned
    Î» = 1e-8
elif Îº < 1e12: # Moderately conditioned  
    Î» = 1e-6
else:          # Ill-conditioned
    Î» = 1e-4
```

## ğŸ§® **Mathematical Properties**

### **Convergence Rate**
**Quadratic convergence:**
```
||x_{k+1} - x*|| â‰¤ C ||x_k - x*||Â²
```

**In practice:**
- Iteration 1: Error = 0.1
- Iteration 2: Error = 0.01  
- Iteration 3: Error = 0.0001
- Iteration 4: Error = 0.00000001

### **Linear Algebra Costs**

| Operation | Cost | Memory |
|-----------|------|--------|
| **Hessian computation** | O(nÂ²m) | O(nÂ²) |
| **Matrix inversion** | O(nÂ³) | O(nÂ²) |
| **Matrix-vector product** | O(nÂ²) | O(n) |

**Total per iteration:** O(nÂ²m + nÂ³)

### **Comparison vá»›i Gradient Descent**

| Aspect | Newton | Gradient Descent |
|--------|--------|------------------|
| **Convergence rate** | Quadratic | Linear |
| **Iterations needed** | ~5-20 | ~100-1000 |
| **Per-iteration cost** | O(nÂ³) | O(nm) |
| **Memory** | O(nÂ²) | O(n) |
| **Robustness** | Local | Global |

## ğŸ¯ **Khi NÃ o DÃ¹ng Newton Method**

### **Ideal Conditions**
- âœ… **Small to medium n** (< 1000 features)
- âœ… **Quadratic/near-quadratic functions**
- âœ… **High precision required**
- âœ… **Good starting point**

### **Avoid When**
- âŒ **Large n** (> 10,000 features)
- âŒ **Ill-conditioned problems**
- âŒ **Non-convex functions**
- âŒ **Memory constraints**

### **Perfect Use Cases**
1. **Linear/Ridge Regression** (exactly quadratic)
2. **Logistic Regression** (well-conditioned)
3. **Small neural networks** (final layer tuning)
4. **Scientific computing** (high precision needed)

## ğŸ”§ **Implementation Details**

### **Hessian Computation**
```python
# For f(w) = (1/2n) ||Xw - y||Â²
H = (1/n) * X.T @ X

# Add regularization
H_reg = H + Î» * np.eye(n)

# Check condition number
Îº = np.linalg.cond(H_reg)
```

### **Safe Matrix Inversion**
```python
try:
    H_inv = np.linalg.inv(H_reg)
except np.linalg.LinAlgError:
    # Increase regularization
    H_reg = H + (Î» * 1000) * np.eye(n)
    H_inv = np.linalg.inv(H_reg)
```

### **Alternative: Solve Linear System**
Instead of computing H^{-1}, solve:
```python
# More numerically stable
newton_step = np.linalg.solve(H_reg, gradient)
weights_new = weights - newton_step
```

## ğŸ§  **Memory Aids & Intuition**

### **Newton vs Car Analogy**
```
Gradient Descent = Äi bá»™ vá»›i GPS
- Chá»‰ biáº¿t hÆ°á»›ng (gradient)
- Tá»« tá»«, step by step
- An toÃ n nhÆ°ng cháº­m

Newton Method = Äi xe cÃ³ báº£n Ä‘á»“ chi tiáº¿t
- Biáº¿t cáº£ hÆ°á»›ng vÃ  Ä‘á»™ cong Ä‘Æ°á»ng (Hessian)
- CÃ³ thá»ƒ optimal route
- Nhanh nhÆ°ng cáº§n sophisticated system
```

### **Regularization nhÆ° Insurance**
```
Î» = 1e-8   â†’  "Chá»‰ mua báº£o hiá»ƒm minimum"
Î» = 1e-6   â†’  "Báº£o hiá»ƒm standard"  
Î» = 1e-4   â†’  "Báº£o hiá»ƒm comprehensive"
```

### **Convergence Visualization**
```
Iteration 0: ğŸ”´ğŸ¯  (far from target)
Iteration 1: ğŸŸ¡ğŸ¯  (much closer)
Iteration 2: ğŸŸ¢ğŸ¯  (very close)
Iteration 3: âœ…ğŸ¯  (bullseye!)
```

## ğŸ” **Troubleshooting Guide**

### **Common Issues & Solutions**

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Singular Hessian** | LinAlgError | Increase regularization |
| **Slow convergence** | Many iterations | Check conditioning |
| **Memory error** | Out of RAM | Use Quasi-Newton instead |
| **Poor accuracy** | High final cost | Check data preprocessing |

### **Diagnostic Checks**
```python
# 1. Check condition number
Îº = np.linalg.cond(H)
print(f"Condition number: {Îº:.2e}")

# 2. Check eigenvalues
eigenvals = np.linalg.eigvals(H)
print(f"Min eigenvalue: {np.min(eigenvals):.2e}")
print(f"Max eigenvalue: {np.max(eigenvals):.2e}")

# 3. Check rank
rank = np.linalg.matrix_rank(H)
print(f"Rank: {rank}/{H.shape[0]}")
```

### **Performance Optimization**
```python
# 1. Use Cholesky for positive definite H
if is_positive_definite(H):
    L = np.linalg.cholesky(H)
    newton_step = solve_triangular(L, gradient)
    
# 2. Cache Hessian if constant
if problem_is_quadratic:
    H_inv = np.linalg.inv(H)  # Compute once
    
# 3. Use sparse matrices if applicable
if is_sparse(X):
    H = sparse_dot_product(X.T, X)
```

## ğŸ“ˆ **Advanced Variants**

### **Damped Newton Method**
```python
# Add line search
Î± = backtracking_line_search(...)
weights_new = weights - Î± * H_inv @ gradient
```

### **Trust Region Newton**
```python
# Constrain step size
newton_step = solve_trust_region(H, gradient, radius)
```

### **Limited Memory Newton**
For large problems, approximate Hessian with limited memory.

---

*"Newton Method: Use the full curvature information to take optimal steps toward the minimum"* ğŸ¯âš¡